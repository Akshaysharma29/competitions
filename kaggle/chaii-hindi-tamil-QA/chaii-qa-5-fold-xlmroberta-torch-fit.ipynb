{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4eee83",
   "metadata": {
    "papermill": {
     "duration": 0.022876,
     "end_time": "2021-09-06T09:50:25.144547",
     "exception": false,
     "start_time": "2021-09-06T09:50:25.121671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n",
    "    \n",
    "<h3><span \"style: color=#444\">Introduction</span></h3>\n",
    "\n",
    "The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n",
    "\n",
    "This is a three part kernel,\n",
    "\n",
    "- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n",
    "\n",
    "- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n",
    "\n",
    "- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n",
    "\n",
    "<h3><span \"style: color=#444\">Techniques</span></h3>\n",
    "\n",
    "The kernel has implementation for below techniques, click on the links to learn more -\n",
    "\n",
    " - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n",
    " \n",
    " - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
    " \n",
    " - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n",
    " \n",
    " - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
    " \n",
    " - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n",
    " \n",
    " - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n",
    " \n",
    " - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n",
    " \n",
    " - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n",
    " \n",
    " - etc.\n",
    " \n",
    "<h3><span \"style: color=#444\">References</span></h3>\n",
    "I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n",
    "\n",
    "- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n",
    "\n",
    "- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n",
    "\n",
    "- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n",
    "\n",
    "- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n",
    "\n",
    "- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac631c",
   "metadata": {
    "papermill": {
     "duration": 0.021342,
     "end_time": "2021-09-06T09:50:25.187770",
     "exception": false,
     "start_time": "2021-09-06T09:50:25.166428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3><span style=\"color=#444\">Note</span></h3>\n",
    "\n",
    "The below points are worth noting,\n",
    "\n",
    " - I haven't used FP16 because due to some reason this fails and model never starts training.\n",
    " - These are the original hyperparamters and setting that I have used for training my models.\n",
    " - I tried few pooling layers but none of them performed better than simple one.\n",
    " - Gradient clipping reduces model performance.\n",
    " - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n",
    " - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1dae1",
   "metadata": {
    "papermill": {
     "duration": 0.021488,
     "end_time": "2021-09-06T09:50:25.231442",
     "exception": false,
     "start_time": "2021-09-06T09:50:25.209954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install APEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1040146",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:25.282079Z",
     "iopub.status.busy": "2021-09-06T09:50:25.280990Z",
     "iopub.status.idle": "2021-09-06T09:50:25.283123Z",
     "shell.execute_reply": "2021-09-06T09:50:25.283584Z",
     "shell.execute_reply.started": "2021-09-06T09:44:11.736408Z"
    },
    "id": "aCY6yvR6ET3s",
    "outputId": "99831be8-d7e0-47d4-c561-f266d3ed1fd3",
    "papermill": {
     "duration": 0.030758,
     "end_time": "2021-09-06T09:50:25.283813",
     "exception": false,
     "start_time": "2021-09-06T09:50:25.253055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile setup.sh\n",
    "# export CUDA_HOME=/usr/local/cuda-10.1\n",
    "# git clone https://github.com/NVIDIA/apex\n",
    "# cd apex\n",
    "# pip install -v --disable-pip-version-check --no-cache-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6531c805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:25.330950Z",
     "iopub.status.busy": "2021-09-06T09:50:25.329780Z",
     "iopub.status.idle": "2021-09-06T09:50:25.332484Z",
     "shell.execute_reply": "2021-09-06T09:50:25.332096Z",
     "shell.execute_reply.started": "2021-09-06T09:44:11.746466Z"
    },
    "id": "-l2Jsav9ET3v",
    "papermill": {
     "duration": 0.027255,
     "end_time": "2021-09-06T09:50:25.332590",
     "exception": false,
     "start_time": "2021-09-06T09:50:25.305335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !sh setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190cef6e",
   "metadata": {
    "papermill": {
     "duration": 0.021081,
     "end_time": "2021-09-06T09:50:25.375118",
     "exception": false,
     "start_time": "2021-09-06T09:50:25.354037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99cb227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:25.513215Z",
     "iopub.status.busy": "2021-09-06T09:50:25.512412Z",
     "iopub.status.idle": "2021-09-06T09:50:32.540406Z",
     "shell.execute_reply": "2021-09-06T09:50:32.540804Z",
     "shell.execute_reply.started": "2021-09-06T09:44:11.805482Z"
    },
    "id": "E4l6PirHET3x",
    "outputId": "eeaea823-bdbc-4bef-a518-e51736877d6e",
    "papermill": {
     "duration": 7.144532,
     "end_time": "2021-09-06T09:50:32.540943",
     "exception": false,
     "start_time": "2021-09-06T09:50:25.396411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apex AMP Installed :: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader,\n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_INSTALLED = True\n",
    "except ImportError:\n",
    "    APEX_INSTALLED = False\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging,\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    ")\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def optimal_num_of_loader_workers():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    return optimal_value\n",
    "\n",
    "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d9906",
   "metadata": {
    "papermill": {
     "duration": 0.02122,
     "end_time": "2021-09-06T09:50:32.585168",
     "exception": false,
     "start_time": "2021-09-06T09:50:32.563948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77587b96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:32.633277Z",
     "iopub.status.busy": "2021-09-06T09:50:32.632622Z",
     "iopub.status.idle": "2021-09-06T09:50:32.634966Z",
     "shell.execute_reply": "2021-09-06T09:50:32.635385Z",
     "shell.execute_reply.started": "2021-09-06T09:44:18.625419Z"
    },
    "id": "LUx5XplNET3y",
    "papermill": {
     "duration": 0.029167,
     "end_time": "2021-09-06T09:50:32.635499",
     "exception": false,
     "start_time": "2021-09-06T09:50:32.606332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # model\n",
    "    model_type = 'xlm_roberta'\n",
    "    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n",
    "    config_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "    fp16 = True if APEX_INSTALLED else False\n",
    "    fp16_opt_level = \"O1\"\n",
    "    gradient_accumulation_steps = 8#2\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "    max_seq_length = 384\n",
    "    doc_stride = 128\n",
    "\n",
    "    # train\n",
    "    epochs = 2#1\n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 4#8\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_type = 'AdamW'\n",
    "    learning_rate = 1.5e-5\n",
    "    weight_decay = 1e-2\n",
    "    epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # scheduler\n",
    "    decay_name = 'linear-warmup'\n",
    "    warmup_ratio = 0.1\n",
    "\n",
    "    # logging\n",
    "    logging_steps = 10\n",
    "\n",
    "    # evaluate\n",
    "    output_dir = 'output'\n",
    "    seed = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14707284",
   "metadata": {
    "papermill": {
     "duration": 0.021498,
     "end_time": "2021-09-06T09:50:32.678352",
     "exception": false,
     "start_time": "2021-09-06T09:50:32.656854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85521bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:32.729857Z",
     "iopub.status.busy": "2021-09-06T09:50:32.729375Z",
     "iopub.status.idle": "2021-09-06T09:50:34.091620Z",
     "shell.execute_reply": "2021-09-06T09:50:34.090545Z",
     "shell.execute_reply.started": "2021-09-06T09:44:18.633694Z"
    },
    "id": "X_eRZQrzET3z",
    "papermill": {
     "duration": 1.391957,
     "end_time": "2021-09-06T09:50:34.091753",
     "exception": false,
     "start_time": "2021-09-06T09:50:32.699796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
    "test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
    "external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n",
    "external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n",
    "external_train = pd.concat([external_mlqa, external_xquad])\n",
    "\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "\n",
    "train = create_folds(train, num_splits=5)\n",
    "external_train[\"kfold\"] = -1\n",
    "external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
    "train = pd.concat([train, external_train]).reset_index(drop=True)#[:64]\n",
    "\n",
    "def convert_answers(row):\n",
    "    return {'answer_start': [row[0]], 'text': [row[1]]}\n",
    "\n",
    "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8698633",
   "metadata": {
    "papermill": {
     "duration": 0.021612,
     "end_time": "2021-09-06T09:50:34.136044",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.114432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Covert Examples to Features (Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92313833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:34.193407Z",
     "iopub.status.busy": "2021-09-06T09:50:34.191897Z",
     "iopub.status.idle": "2021-09-06T09:50:34.194259Z",
     "shell.execute_reply": "2021-09-06T09:50:34.194692Z",
     "shell.execute_reply.started": "2021-09-06T09:44:19.791859Z"
    },
    "id": "dxbZdct1ET3z",
    "papermill": {
     "duration": 0.036901,
     "end_time": "2021-09-06T09:50:34.194870",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.157969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(args, example, tokenizer):\n",
    "    example[\"question\"] = example[\"question\"].lstrip()\n",
    "    tokenized_example = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=args.max_seq_length,\n",
    "        stride=args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
    "\n",
    "    features = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        feature = {}\n",
    "\n",
    "        input_ids = tokenized_example[\"input_ids\"][i]\n",
    "        attention_mask = tokenized_example[\"attention_mask\"][i]\n",
    "\n",
    "        feature['input_ids'] = input_ids\n",
    "        feature['attention_mask'] = attention_mask\n",
    "        feature['offset_mapping'] = offsets\n",
    "\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_example.sequence_ids(i)\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = example[\"answers\"]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            feature[\"start_position\"] = cls_index\n",
    "            feature[\"end_position\"] = cls_index\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                feature[\"start_position\"] = cls_index\n",
    "                feature[\"end_position\"] = cls_index\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                feature[\"start_position\"] = token_start_index - 1\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                feature[\"end_position\"] = token_end_index + 1\n",
    "\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f812fa",
   "metadata": {
    "papermill": {
     "duration": 0.021662,
     "end_time": "2021-09-06T09:50:34.239298",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.217636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f856536a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:34.291844Z",
     "iopub.status.busy": "2021-09-06T09:50:34.291277Z",
     "iopub.status.idle": "2021-09-06T09:50:34.294349Z",
     "shell.execute_reply": "2021-09-06T09:50:34.293890Z",
     "shell.execute_reply.started": "2021-09-06T09:44:19.817230Z"
    },
    "id": "6TuzHdjmET30",
    "papermill": {
     "duration": 0.033438,
     "end_time": "2021-09-06T09:50:34.294465",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.261027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, features, mode='train'):\n",
    "        super(DatasetRetriever, self).__init__()\n",
    "        self.features = features\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, item):   \n",
    "        feature = self.features[item]\n",
    "        if self.mode == 'train':\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
    "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
    "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':feature['offset_mapping'],\n",
    "                'sequence_ids':feature['sequence_ids'],\n",
    "                'id':feature['example_id'],\n",
    "                'context': feature['context'],\n",
    "                'question': feature['question']\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4a1fe",
   "metadata": {
    "papermill": {
     "duration": 0.021724,
     "end_time": "2021-09-06T09:50:34.338425",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.316701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf83410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:34.392942Z",
     "iopub.status.busy": "2021-09-06T09:50:34.391529Z",
     "iopub.status.idle": "2021-09-06T09:50:34.393770Z",
     "shell.execute_reply": "2021-09-06T09:50:34.394236Z",
     "shell.execute_reply.started": "2021-09-06T09:44:19.840872Z"
    },
    "id": "9OxhKqxcET31",
    "papermill": {
     "duration": 0.033456,
     "end_time": "2021-09-06T09:50:34.394389",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.360933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, modelname_or_path, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self._init_weights(self.qa_outputs)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "        # token_type_ids=None\n",
    "    ):\n",
    "        outputs = self.xlm_roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        # sequence_output = self.dropout(sequence_output)\n",
    "        qa_logits = self.qa_outputs(sequence_output)\n",
    "        \n",
    "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "    \n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee4ff4",
   "metadata": {
    "papermill": {
     "duration": 0.021941,
     "end_time": "2021-09-06T09:50:34.438036",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.416095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "552af59f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:34.487791Z",
     "iopub.status.busy": "2021-09-06T09:50:34.487261Z",
     "iopub.status.idle": "2021-09-06T09:50:34.490627Z",
     "shell.execute_reply": "2021-09-06T09:50:34.491012Z",
     "shell.execute_reply.started": "2021-09-06T09:44:19.868168Z"
    },
    "id": "SxuNrJqqET32",
    "papermill": {
     "duration": 0.030544,
     "end_time": "2021-09-06T09:50:34.491208",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.460664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    start_preds, end_preds = preds\n",
    "    start_labels, end_labels = labels\n",
    "    \n",
    "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
    "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
    "    total_loss = (start_loss + end_loss) / 2\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c715e4",
   "metadata": {
    "papermill": {
     "duration": 0.021695,
     "end_time": "2021-09-06T09:50:34.534469",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.512774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Grouped Layerwise Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31769c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:34.593443Z",
     "iopub.status.busy": "2021-09-06T09:50:34.592725Z",
     "iopub.status.idle": "2021-09-06T09:50:34.596531Z",
     "shell.execute_reply": "2021-09-06T09:50:34.596107Z",
     "shell.execute_reply.started": "2021-09-06T09:44:19.886098Z"
    },
    "id": "vf6HVcu2ET34",
    "papermill": {
     "duration": 0.040137,
     "end_time": "2021-09-06T09:50:34.596652",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.556515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_grouped_parameters(args, model):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e1b84",
   "metadata": {
    "papermill": {
     "duration": 0.021626,
     "end_time": "2021-09-06T09:50:34.640377",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.618751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Metric Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db13a7e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:34.690884Z",
     "iopub.status.busy": "2021-09-06T09:50:34.690322Z",
     "iopub.status.idle": "2021-09-06T09:50:34.693430Z",
     "shell.execute_reply": "2021-09-06T09:50:34.693007Z",
     "shell.execute_reply.started": "2021-09-06T09:44:19.913663Z"
    },
    "id": "bkFB-iMcET34",
    "papermill": {
     "duration": 0.031054,
     "end_time": "2021-09-06T09:50:34.693536",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.662482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 0\n",
    "        self.min = 1e5\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if val > self.max:\n",
    "            self.max = val\n",
    "        if val < self.min:\n",
    "            self.min = val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad314f",
   "metadata": {
    "papermill": {
     "duration": 0.021707,
     "end_time": "2021-09-06T09:50:34.737521",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.715814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9fc3643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:34.795219Z",
     "iopub.status.busy": "2021-09-06T09:50:34.794479Z",
     "iopub.status.idle": "2021-09-06T09:50:34.797418Z",
     "shell.execute_reply": "2021-09-06T09:50:34.797012Z",
     "shell.execute_reply.started": "2021-09-06T09:44:19.930710Z"
    },
    "id": "spFRutV0ET34",
    "papermill": {
     "duration": 0.038021,
     "end_time": "2021-09-06T09:50:34.797534",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.759513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    model = Model(args.model_name_or_path, config=config)\n",
    "    return config, tokenizer, model\n",
    "\n",
    "def make_optimizer(args, model):\n",
    "    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    if args.optimizer_type == \"AdamW\":\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.epsilon,\n",
    "            correct_bias=True\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "def make_scheduler(\n",
    "    args, optimizer, \n",
    "    num_warmup_steps, \n",
    "    num_training_steps\n",
    "):\n",
    "    if args.decay_name == \"cosine-warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    return scheduler    \n",
    "\n",
    "def make_loader(\n",
    "    args, data, \n",
    "    tokenizer, fold\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "    \n",
    "    train_features, valid_features = [[] for _ in range(2)]\n",
    "    for i, row in train_set.iterrows():\n",
    "        train_features += prepare_train_features(args, row, tokenizer)\n",
    "    for i, row in valid_set.iterrows():\n",
    "        valid_features += prepare_train_features(args, row, tokenizer)\n",
    "\n",
    "    train_dataset = DatasetRetriever(train_features)\n",
    "    valid_dataset = DatasetRetriever(valid_features)\n",
    "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
    "    \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True,\n",
    "        drop_last=False \n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=args.eval_batch_size, \n",
    "        sampler=valid_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391c846",
   "metadata": {
    "papermill": {
     "duration": 0.022085,
     "end_time": "2021-09-06T09:50:34.841601",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.819516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "939aeec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:34.897621Z",
     "iopub.status.busy": "2021-09-06T09:50:34.896873Z",
     "iopub.status.idle": "2021-09-06T09:50:34.900391Z",
     "shell.execute_reply": "2021-09-06T09:50:34.899932Z",
     "shell.execute_reply.started": "2021-09-06T09:44:19.961062Z"
    },
    "id": "iFLvh1VQET35",
    "papermill": {
     "duration": 0.037122,
     "end_time": "2021-09-06T09:50:34.900504",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.863382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, tokenizer, \n",
    "        optimizer, scheduler\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train(\n",
    "        self, args, \n",
    "        train_dataloader, \n",
    "        epoch, result_dict\n",
    "    ):\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        self.model.train()\n",
    "        \n",
    "        fix_all_seeds(args.seed)\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "\n",
    "            outputs_start, outputs_end = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            \n",
    "            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            count += input_ids.size(0)\n",
    "            losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "            # if args.fp16:\n",
    "            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
    "            # else:\n",
    "            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
    "                _s = str(len(str(len(train_dataloader.sampler))))\n",
    "                ret = [\n",
    "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
    "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "\n",
    "        result_dict['train_loss'].append(losses.avg)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3fe4b",
   "metadata": {
    "papermill": {
     "duration": 0.021653,
     "end_time": "2021-09-06T09:50:34.943926",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.922273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5bc87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:34.997342Z",
     "iopub.status.busy": "2021-09-06T09:50:34.996526Z",
     "iopub.status.idle": "2021-09-06T09:50:34.998810Z",
     "shell.execute_reply": "2021-09-06T09:50:34.999192Z",
     "shell.execute_reply.started": "2021-09-06T09:44:19.990349Z"
    },
    "id": "1a8kG2UYET36",
    "papermill": {
     "duration": 0.0334,
     "end_time": "2021-09-06T09:50:34.999329",
     "exception": false,
     "start_time": "2021-09-06T09:50:34.965929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def save(self, result, output_dir):\n",
    "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "\n",
    "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
    "        losses = AverageMeter()\n",
    "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
    "            self.model = self.model.eval()\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "            \n",
    "            with torch.no_grad():            \n",
    "                outputs_start, outputs_end = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                \n",
    "                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "                \n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b64c66",
   "metadata": {
    "papermill": {
     "duration": 0.021656,
     "end_time": "2021-09-06T09:50:35.042288",
     "exception": false,
     "start_time": "2021-09-06T09:50:35.020632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "764f8525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:35.096309Z",
     "iopub.status.busy": "2021-09-06T09:50:35.094911Z",
     "iopub.status.idle": "2021-09-06T09:50:35.097181Z",
     "shell.execute_reply": "2021-09-06T09:50:35.097611Z",
     "shell.execute_reply.started": "2021-09-06T09:44:20.011335Z"
    },
    "id": "v-gUDyq2ET37",
    "papermill": {
     "duration": 0.033606,
     "end_time": "2021-09-06T09:50:35.097745",
     "exception": false,
     "start_time": "2021-09-06T09:50:35.064139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_training(args, data, fold):\n",
    "    fix_all_seeds(args.seed)\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    \n",
    "    # model\n",
    "    model_config, tokenizer, model = make_model(args)\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "    \n",
    "    # data loaders\n",
    "    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = make_optimizer(args, model)\n",
    "\n",
    "    # scheduler\n",
    "    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n",
    "    if args.warmup_ratio > 0:\n",
    "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
    "    else:\n",
    "        num_warmup_steps = 0\n",
    "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
    "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    # mixed precision training with NVIDIA Apex\n",
    "    if args.fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "    \n",
    "    result_dict = {\n",
    "        'epoch':[], \n",
    "        'train_loss': [], \n",
    "        'val_loss' : [], \n",
    "        'best_val_loss': np.inf\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        model, model_config, tokenizer, optimizer, scheduler, \n",
    "        train_dataloader, valid_dataloader, result_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c673b3c5",
   "metadata": {
    "papermill": {
     "duration": 0.021334,
     "end_time": "2021-09-06T09:50:35.140520",
     "exception": false,
     "start_time": "2021-09-06T09:50:35.119186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1fff865",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:35.196205Z",
     "iopub.status.busy": "2021-09-06T09:50:35.195464Z",
     "iopub.status.idle": "2021-09-06T09:50:35.198472Z",
     "shell.execute_reply": "2021-09-06T09:50:35.198071Z",
     "shell.execute_reply.started": "2021-09-06T09:44:20.032594Z"
    },
    "id": "39ei5Bm5ET37",
    "papermill": {
     "duration": 0.036459,
     "end_time": "2021-09-06T09:50:35.198587",
     "exception": false,
     "start_time": "2021-09-06T09:50:35.162128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(data, fold):\n",
    "    args = Config()\n",
    "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
    "        valid_dataloader, result_dict = init_training(args, data, fold)\n",
    "    \n",
    "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
    "    evaluator = Evaluator(model)\n",
    "\n",
    "    train_time_list = []\n",
    "    valid_time_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        result_dict['epoch'].append(epoch)\n",
    "\n",
    "        # Train\n",
    "        torch.cuda.synchronize()\n",
    "        tic1 = time.time()\n",
    "        result_dict = trainer.train(\n",
    "            args, train_dataloader, \n",
    "            epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic2 = time.time() \n",
    "        train_time_list.append(tic2 - tic1)\n",
    "        \n",
    "        # Evaluate\n",
    "        torch.cuda.synchronize()\n",
    "        tic3 = time.time()\n",
    "        result_dict = evaluator.evaluate(\n",
    "            valid_dataloader, epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic4 = time.time() \n",
    "        valid_time_list.append(tic4 - tic3)\n",
    "            \n",
    "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n",
    "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
    "            \n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "            model_config.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
    "            \n",
    "        print()\n",
    "\n",
    "    evaluator.save(result_dict, output_dir)\n",
    "    \n",
    "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
    "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    del trainer, evaluator\n",
    "    del model, model_config, tokenizer\n",
    "    del optimizer, scheduler\n",
    "    del train_dataloader, valid_dataloader, result_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d2934d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T09:50:35.248081Z",
     "iopub.status.busy": "2021-09-06T09:50:35.247392Z",
     "iopub.status.idle": "2021-09-06T17:41:43.092907Z",
     "shell.execute_reply": "2021-09-06T17:41:43.092369Z"
    },
    "id": "mPaGnnCnbhWl",
    "outputId": "1b25d103-7d4d-4779-ea74-34312b42ad4c",
    "papermill": {
     "duration": 28267.872139,
     "end_time": "2021-09-06T17:41:43.093062",
     "exception": false,
     "start_time": "2021-09-06T09:50:35.220923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbf7f51507f43bdadddb71078e39305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96a1305868449a8984939218043ac2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a4fbbbd9f34b89a68a4a9568dd72a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9f9842df6a4d6f83c2217f44eb09f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efab4e4c18324bd4901026885c4411ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 19985, Num examples Valid=3084\n",
      "Total Training Steps: 1250, Total Warmup Steps: 125\n",
      "Epoch: 00 [    4/19985 (  0%)], Train Loss: 0.73290\n",
      "Epoch: 00 [   44/19985 (  0%)], Train Loss: 0.72868\n",
      "Epoch: 00 [   84/19985 (  0%)], Train Loss: 0.72557\n",
      "Epoch: 00 [  124/19985 (  1%)], Train Loss: 0.72345\n",
      "Epoch: 00 [  164/19985 (  1%)], Train Loss: 0.72319\n",
      "Epoch: 00 [  204/19985 (  1%)], Train Loss: 0.72138\n",
      "Epoch: 00 [  244/19985 (  1%)], Train Loss: 0.71871\n",
      "Epoch: 00 [  284/19985 (  1%)], Train Loss: 0.71583\n",
      "Epoch: 00 [  324/19985 (  2%)], Train Loss: 0.71212\n",
      "Epoch: 00 [  364/19985 (  2%)], Train Loss: 0.70854\n",
      "Epoch: 00 [  404/19985 (  2%)], Train Loss: 0.70464\n",
      "Epoch: 00 [  444/19985 (  2%)], Train Loss: 0.69945\n",
      "Epoch: 00 [  484/19985 (  2%)], Train Loss: 0.69528\n",
      "Epoch: 00 [  524/19985 (  3%)], Train Loss: 0.68891\n",
      "Epoch: 00 [  564/19985 (  3%)], Train Loss: 0.68324\n",
      "Epoch: 00 [  604/19985 (  3%)], Train Loss: 0.67572\n",
      "Epoch: 00 [  644/19985 (  3%)], Train Loss: 0.66872\n",
      "Epoch: 00 [  684/19985 (  3%)], Train Loss: 0.66086\n",
      "Epoch: 00 [  724/19985 (  4%)], Train Loss: 0.65256\n",
      "Epoch: 00 [  764/19985 (  4%)], Train Loss: 0.64388\n",
      "Epoch: 00 [  804/19985 (  4%)], Train Loss: 0.63546\n",
      "Epoch: 00 [  844/19985 (  4%)], Train Loss: 0.62634\n",
      "Epoch: 00 [  884/19985 (  4%)], Train Loss: 0.61659\n",
      "Epoch: 00 [  924/19985 (  5%)], Train Loss: 0.60585\n",
      "Epoch: 00 [  964/19985 (  5%)], Train Loss: 0.59486\n",
      "Epoch: 00 [ 1004/19985 (  5%)], Train Loss: 0.58333\n",
      "Epoch: 00 [ 1044/19985 (  5%)], Train Loss: 0.57229\n",
      "Epoch: 00 [ 1084/19985 (  5%)], Train Loss: 0.56095\n",
      "Epoch: 00 [ 1124/19985 (  6%)], Train Loss: 0.55167\n",
      "Epoch: 00 [ 1164/19985 (  6%)], Train Loss: 0.54189\n",
      "Epoch: 00 [ 1204/19985 (  6%)], Train Loss: 0.53323\n",
      "Epoch: 00 [ 1244/19985 (  6%)], Train Loss: 0.52222\n",
      "Epoch: 00 [ 1284/19985 (  6%)], Train Loss: 0.51196\n",
      "Epoch: 00 [ 1324/19985 (  7%)], Train Loss: 0.49957\n",
      "Epoch: 00 [ 1364/19985 (  7%)], Train Loss: 0.49001\n",
      "Epoch: 00 [ 1404/19985 (  7%)], Train Loss: 0.48111\n",
      "Epoch: 00 [ 1444/19985 (  7%)], Train Loss: 0.47069\n",
      "Epoch: 00 [ 1484/19985 (  7%)], Train Loss: 0.46211\n",
      "Epoch: 00 [ 1524/19985 (  8%)], Train Loss: 0.45321\n",
      "Epoch: 00 [ 1564/19985 (  8%)], Train Loss: 0.44465\n",
      "Epoch: 00 [ 1604/19985 (  8%)], Train Loss: 0.43681\n",
      "Epoch: 00 [ 1644/19985 (  8%)], Train Loss: 0.42975\n",
      "Epoch: 00 [ 1684/19985 (  8%)], Train Loss: 0.42445\n",
      "Epoch: 00 [ 1724/19985 (  9%)], Train Loss: 0.41747\n",
      "Epoch: 00 [ 1764/19985 (  9%)], Train Loss: 0.41079\n",
      "Epoch: 00 [ 1804/19985 (  9%)], Train Loss: 0.40407\n",
      "Epoch: 00 [ 1844/19985 (  9%)], Train Loss: 0.39784\n",
      "Epoch: 00 [ 1884/19985 (  9%)], Train Loss: 0.39218\n",
      "Epoch: 00 [ 1924/19985 ( 10%)], Train Loss: 0.38660\n",
      "Epoch: 00 [ 1964/19985 ( 10%)], Train Loss: 0.38174\n",
      "Epoch: 00 [ 2004/19985 ( 10%)], Train Loss: 0.37695\n",
      "Epoch: 00 [ 2044/19985 ( 10%)], Train Loss: 0.37105\n",
      "Epoch: 00 [ 2084/19985 ( 10%)], Train Loss: 0.36636\n",
      "Epoch: 00 [ 2124/19985 ( 11%)], Train Loss: 0.36163\n",
      "Epoch: 00 [ 2164/19985 ( 11%)], Train Loss: 0.35691\n",
      "Epoch: 00 [ 2204/19985 ( 11%)], Train Loss: 0.35297\n",
      "Epoch: 00 [ 2244/19985 ( 11%)], Train Loss: 0.34847\n",
      "Epoch: 00 [ 2284/19985 ( 11%)], Train Loss: 0.34396\n",
      "Epoch: 00 [ 2324/19985 ( 12%)], Train Loss: 0.34040\n",
      "Epoch: 00 [ 2364/19985 ( 12%)], Train Loss: 0.33771\n",
      "Epoch: 00 [ 2404/19985 ( 12%)], Train Loss: 0.33338\n",
      "Epoch: 00 [ 2444/19985 ( 12%)], Train Loss: 0.32986\n",
      "Epoch: 00 [ 2484/19985 ( 12%)], Train Loss: 0.32658\n",
      "Epoch: 00 [ 2524/19985 ( 13%)], Train Loss: 0.32395\n",
      "Epoch: 00 [ 2564/19985 ( 13%)], Train Loss: 0.32093\n",
      "Epoch: 00 [ 2604/19985 ( 13%)], Train Loss: 0.31789\n",
      "Epoch: 00 [ 2644/19985 ( 13%)], Train Loss: 0.31469\n",
      "Epoch: 00 [ 2684/19985 ( 13%)], Train Loss: 0.31144\n",
      "Epoch: 00 [ 2724/19985 ( 14%)], Train Loss: 0.30819\n",
      "Epoch: 00 [ 2764/19985 ( 14%)], Train Loss: 0.30575\n",
      "Epoch: 00 [ 2804/19985 ( 14%)], Train Loss: 0.30270\n",
      "Epoch: 00 [ 2844/19985 ( 14%)], Train Loss: 0.30047\n",
      "Epoch: 00 [ 2884/19985 ( 14%)], Train Loss: 0.29869\n",
      "Epoch: 00 [ 2924/19985 ( 15%)], Train Loss: 0.29588\n",
      "Epoch: 00 [ 2964/19985 ( 15%)], Train Loss: 0.29355\n",
      "Epoch: 00 [ 3004/19985 ( 15%)], Train Loss: 0.29056\n",
      "Epoch: 00 [ 3044/19985 ( 15%)], Train Loss: 0.28804\n",
      "Epoch: 00 [ 3084/19985 ( 15%)], Train Loss: 0.28551\n",
      "Epoch: 00 [ 3124/19985 ( 16%)], Train Loss: 0.28324\n",
      "Epoch: 00 [ 3164/19985 ( 16%)], Train Loss: 0.28126\n",
      "Epoch: 00 [ 3204/19985 ( 16%)], Train Loss: 0.27858\n",
      "Epoch: 00 [ 3244/19985 ( 16%)], Train Loss: 0.27719\n",
      "Epoch: 00 [ 3284/19985 ( 16%)], Train Loss: 0.27500\n",
      "Epoch: 00 [ 3324/19985 ( 17%)], Train Loss: 0.27197\n",
      "Epoch: 00 [ 3364/19985 ( 17%)], Train Loss: 0.26957\n",
      "Epoch: 00 [ 3404/19985 ( 17%)], Train Loss: 0.26718\n",
      "Epoch: 00 [ 3444/19985 ( 17%)], Train Loss: 0.26557\n",
      "Epoch: 00 [ 3484/19985 ( 17%)], Train Loss: 0.26403\n",
      "Epoch: 00 [ 3524/19985 ( 18%)], Train Loss: 0.26255\n",
      "Epoch: 00 [ 3564/19985 ( 18%)], Train Loss: 0.26079\n",
      "Epoch: 00 [ 3604/19985 ( 18%)], Train Loss: 0.25936\n",
      "Epoch: 00 [ 3644/19985 ( 18%)], Train Loss: 0.25727\n",
      "Epoch: 00 [ 3684/19985 ( 18%)], Train Loss: 0.25549\n",
      "Epoch: 00 [ 3724/19985 ( 19%)], Train Loss: 0.25345\n",
      "Epoch: 00 [ 3764/19985 ( 19%)], Train Loss: 0.25192\n",
      "Epoch: 00 [ 3804/19985 ( 19%)], Train Loss: 0.24997\n",
      "Epoch: 00 [ 3844/19985 ( 19%)], Train Loss: 0.24917\n",
      "Epoch: 00 [ 3884/19985 ( 19%)], Train Loss: 0.24818\n",
      "Epoch: 00 [ 3924/19985 ( 20%)], Train Loss: 0.24662\n",
      "Epoch: 00 [ 3964/19985 ( 20%)], Train Loss: 0.24543\n",
      "Epoch: 00 [ 4004/19985 ( 20%)], Train Loss: 0.24374\n",
      "Epoch: 00 [ 4044/19985 ( 20%)], Train Loss: 0.24225\n",
      "Epoch: 00 [ 4084/19985 ( 20%)], Train Loss: 0.24033\n",
      "Epoch: 00 [ 4124/19985 ( 21%)], Train Loss: 0.23871\n",
      "Epoch: 00 [ 4164/19985 ( 21%)], Train Loss: 0.23726\n",
      "Epoch: 00 [ 4204/19985 ( 21%)], Train Loss: 0.23590\n",
      "Epoch: 00 [ 4244/19985 ( 21%)], Train Loss: 0.23516\n",
      "Epoch: 00 [ 4284/19985 ( 21%)], Train Loss: 0.23362\n",
      "Epoch: 00 [ 4324/19985 ( 22%)], Train Loss: 0.23250\n",
      "Epoch: 00 [ 4364/19985 ( 22%)], Train Loss: 0.23159\n",
      "Epoch: 00 [ 4404/19985 ( 22%)], Train Loss: 0.23015\n",
      "Epoch: 00 [ 4444/19985 ( 22%)], Train Loss: 0.22904\n",
      "Epoch: 00 [ 4484/19985 ( 22%)], Train Loss: 0.22791\n",
      "Epoch: 00 [ 4524/19985 ( 23%)], Train Loss: 0.22647\n",
      "Epoch: 00 [ 4564/19985 ( 23%)], Train Loss: 0.22539\n",
      "Epoch: 00 [ 4604/19985 ( 23%)], Train Loss: 0.22428\n",
      "Epoch: 00 [ 4644/19985 ( 23%)], Train Loss: 0.22281\n",
      "Epoch: 00 [ 4684/19985 ( 23%)], Train Loss: 0.22123\n",
      "Epoch: 00 [ 4724/19985 ( 24%)], Train Loss: 0.22056\n",
      "Epoch: 00 [ 4764/19985 ( 24%)], Train Loss: 0.21945\n",
      "Epoch: 00 [ 4804/19985 ( 24%)], Train Loss: 0.21819\n",
      "Epoch: 00 [ 4844/19985 ( 24%)], Train Loss: 0.21717\n",
      "Epoch: 00 [ 4884/19985 ( 24%)], Train Loss: 0.21609\n",
      "Epoch: 00 [ 4924/19985 ( 25%)], Train Loss: 0.21523\n",
      "Epoch: 00 [ 4964/19985 ( 25%)], Train Loss: 0.21432\n",
      "Epoch: 00 [ 5004/19985 ( 25%)], Train Loss: 0.21304\n",
      "Epoch: 00 [ 5044/19985 ( 25%)], Train Loss: 0.21224\n",
      "Epoch: 00 [ 5084/19985 ( 25%)], Train Loss: 0.21109\n",
      "Epoch: 00 [ 5124/19985 ( 26%)], Train Loss: 0.21012\n",
      "Epoch: 00 [ 5164/19985 ( 26%)], Train Loss: 0.20937\n",
      "Epoch: 00 [ 5204/19985 ( 26%)], Train Loss: 0.20830\n",
      "Epoch: 00 [ 5244/19985 ( 26%)], Train Loss: 0.20718\n",
      "Epoch: 00 [ 5284/19985 ( 26%)], Train Loss: 0.20644\n",
      "Epoch: 00 [ 5324/19985 ( 27%)], Train Loss: 0.20522\n",
      "Epoch: 00 [ 5364/19985 ( 27%)], Train Loss: 0.20427\n",
      "Epoch: 00 [ 5404/19985 ( 27%)], Train Loss: 0.20331\n",
      "Epoch: 00 [ 5444/19985 ( 27%)], Train Loss: 0.20258\n",
      "Epoch: 00 [ 5484/19985 ( 27%)], Train Loss: 0.20222\n",
      "Epoch: 00 [ 5524/19985 ( 28%)], Train Loss: 0.20154\n",
      "Epoch: 00 [ 5564/19985 ( 28%)], Train Loss: 0.20064\n",
      "Epoch: 00 [ 5604/19985 ( 28%)], Train Loss: 0.19991\n",
      "Epoch: 00 [ 5644/19985 ( 28%)], Train Loss: 0.19895\n",
      "Epoch: 00 [ 5684/19985 ( 28%)], Train Loss: 0.19802\n",
      "Epoch: 00 [ 5724/19985 ( 29%)], Train Loss: 0.19733\n",
      "Epoch: 00 [ 5764/19985 ( 29%)], Train Loss: 0.19621\n",
      "Epoch: 00 [ 5804/19985 ( 29%)], Train Loss: 0.19544\n",
      "Epoch: 00 [ 5844/19985 ( 29%)], Train Loss: 0.19454\n",
      "Epoch: 00 [ 5884/19985 ( 29%)], Train Loss: 0.19352\n",
      "Epoch: 00 [ 5924/19985 ( 30%)], Train Loss: 0.19273\n",
      "Epoch: 00 [ 5964/19985 ( 30%)], Train Loss: 0.19223\n",
      "Epoch: 00 [ 6004/19985 ( 30%)], Train Loss: 0.19154\n",
      "Epoch: 00 [ 6044/19985 ( 30%)], Train Loss: 0.19062\n",
      "Epoch: 00 [ 6084/19985 ( 30%)], Train Loss: 0.18991\n",
      "Epoch: 00 [ 6124/19985 ( 31%)], Train Loss: 0.18955\n",
      "Epoch: 00 [ 6164/19985 ( 31%)], Train Loss: 0.18918\n",
      "Epoch: 00 [ 6204/19985 ( 31%)], Train Loss: 0.18838\n",
      "Epoch: 00 [ 6244/19985 ( 31%)], Train Loss: 0.18773\n",
      "Epoch: 00 [ 6284/19985 ( 31%)], Train Loss: 0.18735\n",
      "Epoch: 00 [ 6324/19985 ( 32%)], Train Loss: 0.18671\n",
      "Epoch: 00 [ 6364/19985 ( 32%)], Train Loss: 0.18591\n",
      "Epoch: 00 [ 6404/19985 ( 32%)], Train Loss: 0.18546\n",
      "Epoch: 00 [ 6444/19985 ( 32%)], Train Loss: 0.18498\n",
      "Epoch: 00 [ 6484/19985 ( 32%)], Train Loss: 0.18435\n",
      "Epoch: 00 [ 6524/19985 ( 33%)], Train Loss: 0.18370\n",
      "Epoch: 00 [ 6564/19985 ( 33%)], Train Loss: 0.18331\n",
      "Epoch: 00 [ 6604/19985 ( 33%)], Train Loss: 0.18274\n",
      "Epoch: 00 [ 6644/19985 ( 33%)], Train Loss: 0.18212\n",
      "Epoch: 00 [ 6684/19985 ( 33%)], Train Loss: 0.18158\n",
      "Epoch: 00 [ 6724/19985 ( 34%)], Train Loss: 0.18101\n",
      "Epoch: 00 [ 6764/19985 ( 34%)], Train Loss: 0.18043\n",
      "Epoch: 00 [ 6804/19985 ( 34%)], Train Loss: 0.18004\n",
      "Epoch: 00 [ 6844/19985 ( 34%)], Train Loss: 0.17993\n",
      "Epoch: 00 [ 6884/19985 ( 34%)], Train Loss: 0.17943\n",
      "Epoch: 00 [ 6924/19985 ( 35%)], Train Loss: 0.17883\n",
      "Epoch: 00 [ 6964/19985 ( 35%)], Train Loss: 0.17846\n",
      "Epoch: 00 [ 7004/19985 ( 35%)], Train Loss: 0.17817\n",
      "Epoch: 00 [ 7044/19985 ( 35%)], Train Loss: 0.17760\n",
      "Epoch: 00 [ 7084/19985 ( 35%)], Train Loss: 0.17712\n",
      "Epoch: 00 [ 7124/19985 ( 36%)], Train Loss: 0.17676\n",
      "Epoch: 00 [ 7164/19985 ( 36%)], Train Loss: 0.17614\n",
      "Epoch: 00 [ 7204/19985 ( 36%)], Train Loss: 0.17570\n",
      "Epoch: 00 [ 7244/19985 ( 36%)], Train Loss: 0.17552\n",
      "Epoch: 00 [ 7284/19985 ( 36%)], Train Loss: 0.17499\n",
      "Epoch: 00 [ 7324/19985 ( 37%)], Train Loss: 0.17450\n",
      "Epoch: 00 [ 7364/19985 ( 37%)], Train Loss: 0.17401\n",
      "Epoch: 00 [ 7404/19985 ( 37%)], Train Loss: 0.17351\n",
      "Epoch: 00 [ 7444/19985 ( 37%)], Train Loss: 0.17287\n",
      "Epoch: 00 [ 7484/19985 ( 37%)], Train Loss: 0.17246\n",
      "Epoch: 00 [ 7524/19985 ( 38%)], Train Loss: 0.17220\n",
      "Epoch: 00 [ 7564/19985 ( 38%)], Train Loss: 0.17168\n",
      "Epoch: 00 [ 7604/19985 ( 38%)], Train Loss: 0.17129\n",
      "Epoch: 00 [ 7644/19985 ( 38%)], Train Loss: 0.17087\n",
      "Epoch: 00 [ 7684/19985 ( 38%)], Train Loss: 0.17070\n",
      "Epoch: 00 [ 7724/19985 ( 39%)], Train Loss: 0.17029\n",
      "Epoch: 00 [ 7764/19985 ( 39%)], Train Loss: 0.16961\n",
      "Epoch: 00 [ 7804/19985 ( 39%)], Train Loss: 0.16926\n",
      "Epoch: 00 [ 7844/19985 ( 39%)], Train Loss: 0.16897\n",
      "Epoch: 00 [ 7884/19985 ( 39%)], Train Loss: 0.16870\n",
      "Epoch: 00 [ 7924/19985 ( 40%)], Train Loss: 0.16844\n",
      "Epoch: 00 [ 7964/19985 ( 40%)], Train Loss: 0.16794\n",
      "Epoch: 00 [ 8004/19985 ( 40%)], Train Loss: 0.16769\n",
      "Epoch: 00 [ 8044/19985 ( 40%)], Train Loss: 0.16722\n",
      "Epoch: 00 [ 8084/19985 ( 40%)], Train Loss: 0.16672\n",
      "Epoch: 00 [ 8124/19985 ( 41%)], Train Loss: 0.16633\n",
      "Epoch: 00 [ 8164/19985 ( 41%)], Train Loss: 0.16591\n",
      "Epoch: 00 [ 8204/19985 ( 41%)], Train Loss: 0.16545\n",
      "Epoch: 00 [ 8244/19985 ( 41%)], Train Loss: 0.16530\n",
      "Epoch: 00 [ 8284/19985 ( 41%)], Train Loss: 0.16508\n",
      "Epoch: 00 [ 8324/19985 ( 42%)], Train Loss: 0.16471\n",
      "Epoch: 00 [ 8364/19985 ( 42%)], Train Loss: 0.16454\n",
      "Epoch: 00 [ 8404/19985 ( 42%)], Train Loss: 0.16423\n",
      "Epoch: 00 [ 8444/19985 ( 42%)], Train Loss: 0.16396\n",
      "Epoch: 00 [ 8484/19985 ( 42%)], Train Loss: 0.16351\n",
      "Epoch: 00 [ 8524/19985 ( 43%)], Train Loss: 0.16340\n",
      "Epoch: 00 [ 8564/19985 ( 43%)], Train Loss: 0.16312\n",
      "Epoch: 00 [ 8604/19985 ( 43%)], Train Loss: 0.16270\n",
      "Epoch: 00 [ 8644/19985 ( 43%)], Train Loss: 0.16224\n",
      "Epoch: 00 [ 8684/19985 ( 43%)], Train Loss: 0.16204\n",
      "Epoch: 00 [ 8724/19985 ( 44%)], Train Loss: 0.16162\n",
      "Epoch: 00 [ 8764/19985 ( 44%)], Train Loss: 0.16112\n",
      "Epoch: 00 [ 8804/19985 ( 44%)], Train Loss: 0.16092\n",
      "Epoch: 00 [ 8844/19985 ( 44%)], Train Loss: 0.16073\n",
      "Epoch: 00 [ 8884/19985 ( 44%)], Train Loss: 0.16051\n",
      "Epoch: 00 [ 8924/19985 ( 45%)], Train Loss: 0.15999\n",
      "Epoch: 00 [ 8964/19985 ( 45%)], Train Loss: 0.15969\n",
      "Epoch: 00 [ 9004/19985 ( 45%)], Train Loss: 0.15941\n",
      "Epoch: 00 [ 9044/19985 ( 45%)], Train Loss: 0.15908\n",
      "Epoch: 00 [ 9084/19985 ( 45%)], Train Loss: 0.15887\n",
      "Epoch: 00 [ 9124/19985 ( 46%)], Train Loss: 0.15872\n",
      "Epoch: 00 [ 9164/19985 ( 46%)], Train Loss: 0.15831\n",
      "Epoch: 00 [ 9204/19985 ( 46%)], Train Loss: 0.15810\n",
      "Epoch: 00 [ 9244/19985 ( 46%)], Train Loss: 0.15781\n",
      "Epoch: 00 [ 9284/19985 ( 46%)], Train Loss: 0.15758\n",
      "Epoch: 00 [ 9324/19985 ( 47%)], Train Loss: 0.15727\n",
      "Epoch: 00 [ 9364/19985 ( 47%)], Train Loss: 0.15708\n",
      "Epoch: 00 [ 9404/19985 ( 47%)], Train Loss: 0.15679\n",
      "Epoch: 00 [ 9444/19985 ( 47%)], Train Loss: 0.15658\n",
      "Epoch: 00 [ 9484/19985 ( 47%)], Train Loss: 0.15627\n",
      "Epoch: 00 [ 9524/19985 ( 48%)], Train Loss: 0.15610\n",
      "Epoch: 00 [ 9564/19985 ( 48%)], Train Loss: 0.15598\n",
      "Epoch: 00 [ 9604/19985 ( 48%)], Train Loss: 0.15577\n",
      "Epoch: 00 [ 9644/19985 ( 48%)], Train Loss: 0.15583\n",
      "Epoch: 00 [ 9684/19985 ( 48%)], Train Loss: 0.15555\n",
      "Epoch: 00 [ 9724/19985 ( 49%)], Train Loss: 0.15521\n",
      "Epoch: 00 [ 9764/19985 ( 49%)], Train Loss: 0.15484\n",
      "Epoch: 00 [ 9804/19985 ( 49%)], Train Loss: 0.15463\n",
      "Epoch: 00 [ 9844/19985 ( 49%)], Train Loss: 0.15438\n",
      "Epoch: 00 [ 9884/19985 ( 49%)], Train Loss: 0.15398\n",
      "Epoch: 00 [ 9924/19985 ( 50%)], Train Loss: 0.15370\n",
      "Epoch: 00 [ 9964/19985 ( 50%)], Train Loss: 0.15340\n",
      "Epoch: 00 [10004/19985 ( 50%)], Train Loss: 0.15315\n",
      "Epoch: 00 [10044/19985 ( 50%)], Train Loss: 0.15289\n",
      "Epoch: 00 [10084/19985 ( 50%)], Train Loss: 0.15262\n",
      "Epoch: 00 [10124/19985 ( 51%)], Train Loss: 0.15237\n",
      "Epoch: 00 [10164/19985 ( 51%)], Train Loss: 0.15212\n",
      "Epoch: 00 [10204/19985 ( 51%)], Train Loss: 0.15200\n",
      "Epoch: 00 [10244/19985 ( 51%)], Train Loss: 0.15165\n",
      "Epoch: 00 [10284/19985 ( 51%)], Train Loss: 0.15131\n",
      "Epoch: 00 [10324/19985 ( 52%)], Train Loss: 0.15096\n",
      "Epoch: 00 [10364/19985 ( 52%)], Train Loss: 0.15080\n",
      "Epoch: 00 [10404/19985 ( 52%)], Train Loss: 0.15056\n",
      "Epoch: 00 [10444/19985 ( 52%)], Train Loss: 0.15043\n",
      "Epoch: 00 [10484/19985 ( 52%)], Train Loss: 0.15016\n",
      "Epoch: 00 [10524/19985 ( 53%)], Train Loss: 0.14984\n",
      "Epoch: 00 [10564/19985 ( 53%)], Train Loss: 0.14962\n",
      "Epoch: 00 [10604/19985 ( 53%)], Train Loss: 0.14952\n",
      "Epoch: 00 [10644/19985 ( 53%)], Train Loss: 0.14932\n",
      "Epoch: 00 [10684/19985 ( 53%)], Train Loss: 0.14924\n",
      "Epoch: 00 [10724/19985 ( 54%)], Train Loss: 0.14898\n",
      "Epoch: 00 [10764/19985 ( 54%)], Train Loss: 0.14870\n",
      "Epoch: 00 [10804/19985 ( 54%)], Train Loss: 0.14847\n",
      "Epoch: 00 [10844/19985 ( 54%)], Train Loss: 0.14841\n",
      "Epoch: 00 [10884/19985 ( 54%)], Train Loss: 0.14822\n",
      "Epoch: 00 [10924/19985 ( 55%)], Train Loss: 0.14801\n",
      "Epoch: 00 [10964/19985 ( 55%)], Train Loss: 0.14778\n",
      "Epoch: 00 [11004/19985 ( 55%)], Train Loss: 0.14751\n",
      "Epoch: 00 [11044/19985 ( 55%)], Train Loss: 0.14740\n",
      "Epoch: 00 [11084/19985 ( 55%)], Train Loss: 0.14731\n",
      "Epoch: 00 [11124/19985 ( 56%)], Train Loss: 0.14709\n",
      "Epoch: 00 [11164/19985 ( 56%)], Train Loss: 0.14684\n",
      "Epoch: 00 [11204/19985 ( 56%)], Train Loss: 0.14680\n",
      "Epoch: 00 [11244/19985 ( 56%)], Train Loss: 0.14641\n",
      "Epoch: 00 [11284/19985 ( 56%)], Train Loss: 0.14619\n",
      "Epoch: 00 [11324/19985 ( 57%)], Train Loss: 0.14594\n",
      "Epoch: 00 [11364/19985 ( 57%)], Train Loss: 0.14560\n",
      "Epoch: 00 [11404/19985 ( 57%)], Train Loss: 0.14552\n",
      "Epoch: 00 [11444/19985 ( 57%)], Train Loss: 0.14539\n",
      "Epoch: 00 [11484/19985 ( 57%)], Train Loss: 0.14516\n",
      "Epoch: 00 [11524/19985 ( 58%)], Train Loss: 0.14522\n",
      "Epoch: 00 [11564/19985 ( 58%)], Train Loss: 0.14515\n",
      "Epoch: 00 [11604/19985 ( 58%)], Train Loss: 0.14494\n",
      "Epoch: 00 [11644/19985 ( 58%)], Train Loss: 0.14473\n",
      "Epoch: 00 [11684/19985 ( 58%)], Train Loss: 0.14451\n",
      "Epoch: 00 [11724/19985 ( 59%)], Train Loss: 0.14445\n",
      "Epoch: 00 [11764/19985 ( 59%)], Train Loss: 0.14415\n",
      "Epoch: 00 [11804/19985 ( 59%)], Train Loss: 0.14399\n",
      "Epoch: 00 [11844/19985 ( 59%)], Train Loss: 0.14369\n",
      "Epoch: 00 [11884/19985 ( 59%)], Train Loss: 0.14361\n",
      "Epoch: 00 [11924/19985 ( 60%)], Train Loss: 0.14352\n",
      "Epoch: 00 [11964/19985 ( 60%)], Train Loss: 0.14335\n",
      "Epoch: 00 [12004/19985 ( 60%)], Train Loss: 0.14320\n",
      "Epoch: 00 [12044/19985 ( 60%)], Train Loss: 0.14297\n",
      "Epoch: 00 [12084/19985 ( 60%)], Train Loss: 0.14278\n",
      "Epoch: 00 [12124/19985 ( 61%)], Train Loss: 0.14263\n",
      "Epoch: 00 [12164/19985 ( 61%)], Train Loss: 0.14253\n",
      "Epoch: 00 [12204/19985 ( 61%)], Train Loss: 0.14247\n",
      "Epoch: 00 [12244/19985 ( 61%)], Train Loss: 0.14234\n",
      "Epoch: 00 [12284/19985 ( 61%)], Train Loss: 0.14235\n",
      "Epoch: 00 [12324/19985 ( 62%)], Train Loss: 0.14206\n",
      "Epoch: 00 [12364/19985 ( 62%)], Train Loss: 0.14213\n",
      "Epoch: 00 [12404/19985 ( 62%)], Train Loss: 0.14194\n",
      "Epoch: 00 [12444/19985 ( 62%)], Train Loss: 0.14164\n",
      "Epoch: 00 [12484/19985 ( 62%)], Train Loss: 0.14149\n",
      "Epoch: 00 [12524/19985 ( 63%)], Train Loss: 0.14132\n",
      "Epoch: 00 [12564/19985 ( 63%)], Train Loss: 0.14114\n",
      "Epoch: 00 [12604/19985 ( 63%)], Train Loss: 0.14099\n",
      "Epoch: 00 [12644/19985 ( 63%)], Train Loss: 0.14082\n",
      "Epoch: 00 [12684/19985 ( 63%)], Train Loss: 0.14063\n",
      "Epoch: 00 [12724/19985 ( 64%)], Train Loss: 0.14053\n",
      "Epoch: 00 [12764/19985 ( 64%)], Train Loss: 0.14042\n",
      "Epoch: 00 [12804/19985 ( 64%)], Train Loss: 0.14026\n",
      "Epoch: 00 [12844/19985 ( 64%)], Train Loss: 0.14008\n",
      "Epoch: 00 [12884/19985 ( 64%)], Train Loss: 0.14001\n",
      "Epoch: 00 [12924/19985 ( 65%)], Train Loss: 0.13977\n",
      "Epoch: 00 [12964/19985 ( 65%)], Train Loss: 0.13965\n",
      "Epoch: 00 [13004/19985 ( 65%)], Train Loss: 0.13950\n",
      "Epoch: 00 [13044/19985 ( 65%)], Train Loss: 0.13928\n",
      "Epoch: 00 [13084/19985 ( 65%)], Train Loss: 0.13914\n",
      "Epoch: 00 [13124/19985 ( 66%)], Train Loss: 0.13893\n",
      "Epoch: 00 [13164/19985 ( 66%)], Train Loss: 0.13881\n",
      "Epoch: 00 [13204/19985 ( 66%)], Train Loss: 0.13861\n",
      "Epoch: 00 [13244/19985 ( 66%)], Train Loss: 0.13844\n",
      "Epoch: 00 [13284/19985 ( 66%)], Train Loss: 0.13838\n",
      "Epoch: 00 [13324/19985 ( 67%)], Train Loss: 0.13814\n",
      "Epoch: 00 [13364/19985 ( 67%)], Train Loss: 0.13804\n",
      "Epoch: 00 [13404/19985 ( 67%)], Train Loss: 0.13782\n",
      "Epoch: 00 [13444/19985 ( 67%)], Train Loss: 0.13766\n",
      "Epoch: 00 [13484/19985 ( 67%)], Train Loss: 0.13757\n",
      "Epoch: 00 [13524/19985 ( 68%)], Train Loss: 0.13739\n",
      "Epoch: 00 [13564/19985 ( 68%)], Train Loss: 0.13715\n",
      "Epoch: 00 [13604/19985 ( 68%)], Train Loss: 0.13711\n",
      "Epoch: 00 [13644/19985 ( 68%)], Train Loss: 0.13682\n",
      "Epoch: 00 [13684/19985 ( 68%)], Train Loss: 0.13666\n",
      "Epoch: 00 [13724/19985 ( 69%)], Train Loss: 0.13650\n",
      "Epoch: 00 [13764/19985 ( 69%)], Train Loss: 0.13635\n",
      "Epoch: 00 [13804/19985 ( 69%)], Train Loss: 0.13621\n",
      "Epoch: 00 [13844/19985 ( 69%)], Train Loss: 0.13620\n",
      "Epoch: 00 [13884/19985 ( 69%)], Train Loss: 0.13608\n",
      "Epoch: 00 [13924/19985 ( 70%)], Train Loss: 0.13596\n",
      "Epoch: 00 [13964/19985 ( 70%)], Train Loss: 0.13597\n",
      "Epoch: 00 [14004/19985 ( 70%)], Train Loss: 0.13586\n",
      "Epoch: 00 [14044/19985 ( 70%)], Train Loss: 0.13582\n",
      "Epoch: 00 [14084/19985 ( 70%)], Train Loss: 0.13565\n",
      "Epoch: 00 [14124/19985 ( 71%)], Train Loss: 0.13547\n",
      "Epoch: 00 [14164/19985 ( 71%)], Train Loss: 0.13525\n",
      "Epoch: 00 [14204/19985 ( 71%)], Train Loss: 0.13511\n",
      "Epoch: 00 [14244/19985 ( 71%)], Train Loss: 0.13496\n",
      "Epoch: 00 [14284/19985 ( 71%)], Train Loss: 0.13493\n",
      "Epoch: 00 [14324/19985 ( 72%)], Train Loss: 0.13465\n",
      "Epoch: 00 [14364/19985 ( 72%)], Train Loss: 0.13452\n",
      "Epoch: 00 [14404/19985 ( 72%)], Train Loss: 0.13443\n",
      "Epoch: 00 [14444/19985 ( 72%)], Train Loss: 0.13424\n",
      "Epoch: 00 [14484/19985 ( 72%)], Train Loss: 0.13415\n",
      "Epoch: 00 [14524/19985 ( 73%)], Train Loss: 0.13392\n",
      "Epoch: 00 [14564/19985 ( 73%)], Train Loss: 0.13376\n",
      "Epoch: 00 [14604/19985 ( 73%)], Train Loss: 0.13371\n",
      "Epoch: 00 [14644/19985 ( 73%)], Train Loss: 0.13356\n",
      "Epoch: 00 [14684/19985 ( 73%)], Train Loss: 0.13339\n",
      "Epoch: 00 [14724/19985 ( 74%)], Train Loss: 0.13317\n",
      "Epoch: 00 [14764/19985 ( 74%)], Train Loss: 0.13303\n",
      "Epoch: 00 [14804/19985 ( 74%)], Train Loss: 0.13284\n",
      "Epoch: 00 [14844/19985 ( 74%)], Train Loss: 0.13276\n",
      "Epoch: 00 [14884/19985 ( 74%)], Train Loss: 0.13255\n",
      "Epoch: 00 [14924/19985 ( 75%)], Train Loss: 0.13253\n",
      "Epoch: 00 [14964/19985 ( 75%)], Train Loss: 0.13244\n",
      "Epoch: 00 [15004/19985 ( 75%)], Train Loss: 0.13242\n",
      "Epoch: 00 [15044/19985 ( 75%)], Train Loss: 0.13221\n",
      "Epoch: 00 [15084/19985 ( 75%)], Train Loss: 0.13217\n",
      "Epoch: 00 [15124/19985 ( 76%)], Train Loss: 0.13206\n",
      "Epoch: 00 [15164/19985 ( 76%)], Train Loss: 0.13201\n",
      "Epoch: 00 [15204/19985 ( 76%)], Train Loss: 0.13187\n",
      "Epoch: 00 [15244/19985 ( 76%)], Train Loss: 0.13178\n",
      "Epoch: 00 [15284/19985 ( 76%)], Train Loss: 0.13172\n",
      "Epoch: 00 [15324/19985 ( 77%)], Train Loss: 0.13158\n",
      "Epoch: 00 [15364/19985 ( 77%)], Train Loss: 0.13140\n",
      "Epoch: 00 [15404/19985 ( 77%)], Train Loss: 0.13130\n",
      "Epoch: 00 [15444/19985 ( 77%)], Train Loss: 0.13119\n",
      "Epoch: 00 [15484/19985 ( 77%)], Train Loss: 0.13107\n",
      "Epoch: 00 [15524/19985 ( 78%)], Train Loss: 0.13091\n",
      "Epoch: 00 [15564/19985 ( 78%)], Train Loss: 0.13077\n",
      "Epoch: 00 [15604/19985 ( 78%)], Train Loss: 0.13064\n",
      "Epoch: 00 [15644/19985 ( 78%)], Train Loss: 0.13049\n",
      "Epoch: 00 [15684/19985 ( 78%)], Train Loss: 0.13037\n",
      "Epoch: 00 [15724/19985 ( 79%)], Train Loss: 0.13033\n",
      "Epoch: 00 [15764/19985 ( 79%)], Train Loss: 0.13012\n",
      "Epoch: 00 [15804/19985 ( 79%)], Train Loss: 0.12993\n",
      "Epoch: 00 [15844/19985 ( 79%)], Train Loss: 0.12980\n",
      "Epoch: 00 [15884/19985 ( 79%)], Train Loss: 0.12968\n",
      "Epoch: 00 [15924/19985 ( 80%)], Train Loss: 0.12955\n",
      "Epoch: 00 [15964/19985 ( 80%)], Train Loss: 0.12943\n",
      "Epoch: 00 [16004/19985 ( 80%)], Train Loss: 0.12935\n",
      "Epoch: 00 [16044/19985 ( 80%)], Train Loss: 0.12922\n",
      "Epoch: 00 [16084/19985 ( 80%)], Train Loss: 0.12901\n",
      "Epoch: 00 [16124/19985 ( 81%)], Train Loss: 0.12889\n",
      "Epoch: 00 [16164/19985 ( 81%)], Train Loss: 0.12876\n",
      "Epoch: 00 [16204/19985 ( 81%)], Train Loss: 0.12857\n",
      "Epoch: 00 [16244/19985 ( 81%)], Train Loss: 0.12842\n",
      "Epoch: 00 [16284/19985 ( 81%)], Train Loss: 0.12835\n",
      "Epoch: 00 [16324/19985 ( 82%)], Train Loss: 0.12825\n",
      "Epoch: 00 [16364/19985 ( 82%)], Train Loss: 0.12805\n",
      "Epoch: 00 [16404/19985 ( 82%)], Train Loss: 0.12790\n",
      "Epoch: 00 [16444/19985 ( 82%)], Train Loss: 0.12781\n",
      "Epoch: 00 [16484/19985 ( 82%)], Train Loss: 0.12767\n",
      "Epoch: 00 [16524/19985 ( 83%)], Train Loss: 0.12750\n",
      "Epoch: 00 [16564/19985 ( 83%)], Train Loss: 0.12737\n",
      "Epoch: 00 [16604/19985 ( 83%)], Train Loss: 0.12722\n",
      "Epoch: 00 [16644/19985 ( 83%)], Train Loss: 0.12702\n",
      "Epoch: 00 [16684/19985 ( 83%)], Train Loss: 0.12687\n",
      "Epoch: 00 [16724/19985 ( 84%)], Train Loss: 0.12681\n",
      "Epoch: 00 [16764/19985 ( 84%)], Train Loss: 0.12673\n",
      "Epoch: 00 [16804/19985 ( 84%)], Train Loss: 0.12666\n",
      "Epoch: 00 [16844/19985 ( 84%)], Train Loss: 0.12655\n",
      "Epoch: 00 [16884/19985 ( 84%)], Train Loss: 0.12643\n",
      "Epoch: 00 [16924/19985 ( 85%)], Train Loss: 0.12641\n",
      "Epoch: 00 [16964/19985 ( 85%)], Train Loss: 0.12633\n",
      "Epoch: 00 [17004/19985 ( 85%)], Train Loss: 0.12620\n",
      "Epoch: 00 [17044/19985 ( 85%)], Train Loss: 0.12608\n",
      "Epoch: 00 [17084/19985 ( 85%)], Train Loss: 0.12600\n",
      "Epoch: 00 [17124/19985 ( 86%)], Train Loss: 0.12588\n",
      "Epoch: 00 [17164/19985 ( 86%)], Train Loss: 0.12567\n",
      "Epoch: 00 [17204/19985 ( 86%)], Train Loss: 0.12556\n",
      "Epoch: 00 [17244/19985 ( 86%)], Train Loss: 0.12547\n",
      "Epoch: 00 [17284/19985 ( 86%)], Train Loss: 0.12537\n",
      "Epoch: 00 [17324/19985 ( 87%)], Train Loss: 0.12521\n",
      "Epoch: 00 [17364/19985 ( 87%)], Train Loss: 0.12499\n",
      "Epoch: 00 [17404/19985 ( 87%)], Train Loss: 0.12489\n",
      "Epoch: 00 [17444/19985 ( 87%)], Train Loss: 0.12495\n",
      "Epoch: 00 [17484/19985 ( 87%)], Train Loss: 0.12486\n",
      "Epoch: 00 [17524/19985 ( 88%)], Train Loss: 0.12473\n",
      "Epoch: 00 [17564/19985 ( 88%)], Train Loss: 0.12461\n",
      "Epoch: 00 [17604/19985 ( 88%)], Train Loss: 0.12450\n",
      "Epoch: 00 [17644/19985 ( 88%)], Train Loss: 0.12445\n",
      "Epoch: 00 [17684/19985 ( 88%)], Train Loss: 0.12444\n",
      "Epoch: 00 [17724/19985 ( 89%)], Train Loss: 0.12444\n",
      "Epoch: 00 [17764/19985 ( 89%)], Train Loss: 0.12433\n",
      "Epoch: 00 [17804/19985 ( 89%)], Train Loss: 0.12413\n",
      "Epoch: 00 [17844/19985 ( 89%)], Train Loss: 0.12405\n",
      "Epoch: 00 [17884/19985 ( 89%)], Train Loss: 0.12394\n",
      "Epoch: 00 [17924/19985 ( 90%)], Train Loss: 0.12384\n",
      "Epoch: 00 [17964/19985 ( 90%)], Train Loss: 0.12371\n",
      "Epoch: 00 [18004/19985 ( 90%)], Train Loss: 0.12360\n",
      "Epoch: 00 [18044/19985 ( 90%)], Train Loss: 0.12349\n",
      "Epoch: 00 [18084/19985 ( 90%)], Train Loss: 0.12332\n",
      "Epoch: 00 [18124/19985 ( 91%)], Train Loss: 0.12323\n",
      "Epoch: 00 [18164/19985 ( 91%)], Train Loss: 0.12309\n",
      "Epoch: 00 [18204/19985 ( 91%)], Train Loss: 0.12300\n",
      "Epoch: 00 [18244/19985 ( 91%)], Train Loss: 0.12294\n",
      "Epoch: 00 [18284/19985 ( 91%)], Train Loss: 0.12291\n",
      "Epoch: 00 [18324/19985 ( 92%)], Train Loss: 0.12279\n",
      "Epoch: 00 [18364/19985 ( 92%)], Train Loss: 0.12274\n",
      "Epoch: 00 [18404/19985 ( 92%)], Train Loss: 0.12264\n",
      "Epoch: 00 [18444/19985 ( 92%)], Train Loss: 0.12252\n",
      "Epoch: 00 [18484/19985 ( 92%)], Train Loss: 0.12242\n",
      "Epoch: 00 [18524/19985 ( 93%)], Train Loss: 0.12229\n",
      "Epoch: 00 [18564/19985 ( 93%)], Train Loss: 0.12228\n",
      "Epoch: 00 [18604/19985 ( 93%)], Train Loss: 0.12225\n",
      "Epoch: 00 [18644/19985 ( 93%)], Train Loss: 0.12212\n",
      "Epoch: 00 [18684/19985 ( 93%)], Train Loss: 0.12205\n",
      "Epoch: 00 [18724/19985 ( 94%)], Train Loss: 0.12200\n",
      "Epoch: 00 [18764/19985 ( 94%)], Train Loss: 0.12190\n",
      "Epoch: 00 [18804/19985 ( 94%)], Train Loss: 0.12179\n",
      "Epoch: 00 [18844/19985 ( 94%)], Train Loss: 0.12170\n",
      "Epoch: 00 [18884/19985 ( 94%)], Train Loss: 0.12163\n",
      "Epoch: 00 [18924/19985 ( 95%)], Train Loss: 0.12158\n",
      "Epoch: 00 [18964/19985 ( 95%)], Train Loss: 0.12153\n",
      "Epoch: 00 [19004/19985 ( 95%)], Train Loss: 0.12153\n",
      "Epoch: 00 [19044/19985 ( 95%)], Train Loss: 0.12144\n",
      "Epoch: 00 [19084/19985 ( 95%)], Train Loss: 0.12147\n",
      "Epoch: 00 [19124/19985 ( 96%)], Train Loss: 0.12136\n",
      "Epoch: 00 [19164/19985 ( 96%)], Train Loss: 0.12122\n",
      "Epoch: 00 [19204/19985 ( 96%)], Train Loss: 0.12119\n",
      "Epoch: 00 [19244/19985 ( 96%)], Train Loss: 0.12105\n",
      "Epoch: 00 [19284/19985 ( 96%)], Train Loss: 0.12107\n",
      "Epoch: 00 [19324/19985 ( 97%)], Train Loss: 0.12096\n",
      "Epoch: 00 [19364/19985 ( 97%)], Train Loss: 0.12094\n",
      "Epoch: 00 [19404/19985 ( 97%)], Train Loss: 0.12089\n",
      "Epoch: 00 [19444/19985 ( 97%)], Train Loss: 0.12080\n",
      "Epoch: 00 [19484/19985 ( 97%)], Train Loss: 0.12072\n",
      "Epoch: 00 [19524/19985 ( 98%)], Train Loss: 0.12064\n",
      "Epoch: 00 [19564/19985 ( 98%)], Train Loss: 0.12048\n",
      "Epoch: 00 [19604/19985 ( 98%)], Train Loss: 0.12037\n",
      "Epoch: 00 [19644/19985 ( 98%)], Train Loss: 0.12030\n",
      "Epoch: 00 [19684/19985 ( 98%)], Train Loss: 0.12032\n",
      "Epoch: 00 [19724/19985 ( 99%)], Train Loss: 0.12034\n",
      "Epoch: 00 [19764/19985 ( 99%)], Train Loss: 0.12023\n",
      "Epoch: 00 [19804/19985 ( 99%)], Train Loss: 0.12016\n",
      "Epoch: 00 [19844/19985 ( 99%)], Train Loss: 0.12008\n",
      "Epoch: 00 [19884/19985 ( 99%)], Train Loss: 0.12005\n",
      "Epoch: 00 [19924/19985 (100%)], Train Loss: 0.11993\n",
      "Epoch: 00 [19964/19985 (100%)], Train Loss: 0.11983\n",
      "Epoch: 00 [19985/19985 (100%)], Train Loss: 0.11975\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.24426\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.24426\n",
      "Saving model checkpoint to output/checkpoint-fold-0.\n",
      "\n",
      "Epoch: 01 [    4/19985 (  0%)], Train Loss: 0.00031\n",
      "Epoch: 01 [   44/19985 (  0%)], Train Loss: 0.10890\n",
      "Epoch: 01 [   84/19985 (  0%)], Train Loss: 0.09475\n",
      "Epoch: 01 [  124/19985 (  1%)], Train Loss: 0.08128\n",
      "Epoch: 01 [  164/19985 (  1%)], Train Loss: 0.07936\n",
      "Epoch: 01 [  204/19985 (  1%)], Train Loss: 0.07547\n",
      "Epoch: 01 [  244/19985 (  1%)], Train Loss: 0.07243\n",
      "Epoch: 01 [  284/19985 (  1%)], Train Loss: 0.07497\n",
      "Epoch: 01 [  324/19985 (  2%)], Train Loss: 0.07773\n",
      "Epoch: 01 [  364/19985 (  2%)], Train Loss: 0.08023\n",
      "Epoch: 01 [  404/19985 (  2%)], Train Loss: 0.08097\n",
      "Epoch: 01 [  444/19985 (  2%)], Train Loss: 0.07835\n",
      "Epoch: 01 [  484/19985 (  2%)], Train Loss: 0.08278\n",
      "Epoch: 01 [  524/19985 (  3%)], Train Loss: 0.08030\n",
      "Epoch: 01 [  564/19985 (  3%)], Train Loss: 0.08271\n",
      "Epoch: 01 [  604/19985 (  3%)], Train Loss: 0.08071\n",
      "Epoch: 01 [  644/19985 (  3%)], Train Loss: 0.07989\n",
      "Epoch: 01 [  684/19985 (  3%)], Train Loss: 0.07886\n",
      "Epoch: 01 [  724/19985 (  4%)], Train Loss: 0.07869\n",
      "Epoch: 01 [  764/19985 (  4%)], Train Loss: 0.07780\n",
      "Epoch: 01 [  804/19985 (  4%)], Train Loss: 0.07845\n",
      "Epoch: 01 [  844/19985 (  4%)], Train Loss: 0.07811\n",
      "Epoch: 01 [  884/19985 (  4%)], Train Loss: 0.07881\n",
      "Epoch: 01 [  924/19985 (  5%)], Train Loss: 0.07802\n",
      "Epoch: 01 [  964/19985 (  5%)], Train Loss: 0.07716\n",
      "Epoch: 01 [ 1004/19985 (  5%)], Train Loss: 0.07669\n",
      "Epoch: 01 [ 1044/19985 (  5%)], Train Loss: 0.07708\n",
      "Epoch: 01 [ 1084/19985 (  5%)], Train Loss: 0.07737\n",
      "Epoch: 01 [ 1124/19985 (  6%)], Train Loss: 0.07793\n",
      "Epoch: 01 [ 1164/19985 (  6%)], Train Loss: 0.07828\n",
      "Epoch: 01 [ 1204/19985 (  6%)], Train Loss: 0.07932\n",
      "Epoch: 01 [ 1244/19985 (  6%)], Train Loss: 0.07877\n",
      "Epoch: 01 [ 1284/19985 (  6%)], Train Loss: 0.07843\n",
      "Epoch: 01 [ 1324/19985 (  7%)], Train Loss: 0.07709\n",
      "Epoch: 01 [ 1364/19985 (  7%)], Train Loss: 0.07767\n",
      "Epoch: 01 [ 1404/19985 (  7%)], Train Loss: 0.07747\n",
      "Epoch: 01 [ 1444/19985 (  7%)], Train Loss: 0.07629\n",
      "Epoch: 01 [ 1484/19985 (  7%)], Train Loss: 0.07624\n",
      "Epoch: 01 [ 1524/19985 (  8%)], Train Loss: 0.07550\n",
      "Epoch: 01 [ 1564/19985 (  8%)], Train Loss: 0.07458\n",
      "Epoch: 01 [ 1604/19985 (  8%)], Train Loss: 0.07463\n",
      "Epoch: 01 [ 1644/19985 (  8%)], Train Loss: 0.07436\n",
      "Epoch: 01 [ 1684/19985 (  8%)], Train Loss: 0.07496\n",
      "Epoch: 01 [ 1724/19985 (  9%)], Train Loss: 0.07457\n",
      "Epoch: 01 [ 1764/19985 (  9%)], Train Loss: 0.07442\n",
      "Epoch: 01 [ 1804/19985 (  9%)], Train Loss: 0.07407\n",
      "Epoch: 01 [ 1844/19985 (  9%)], Train Loss: 0.07337\n",
      "Epoch: 01 [ 1884/19985 (  9%)], Train Loss: 0.07296\n",
      "Epoch: 01 [ 1924/19985 ( 10%)], Train Loss: 0.07288\n",
      "Epoch: 01 [ 1964/19985 ( 10%)], Train Loss: 0.07282\n",
      "Epoch: 01 [ 2004/19985 ( 10%)], Train Loss: 0.07262\n",
      "Epoch: 01 [ 2044/19985 ( 10%)], Train Loss: 0.07208\n",
      "Epoch: 01 [ 2084/19985 ( 10%)], Train Loss: 0.07180\n",
      "Epoch: 01 [ 2124/19985 ( 11%)], Train Loss: 0.07156\n",
      "Epoch: 01 [ 2164/19985 ( 11%)], Train Loss: 0.07164\n",
      "Epoch: 01 [ 2204/19985 ( 11%)], Train Loss: 0.07150\n",
      "Epoch: 01 [ 2244/19985 ( 11%)], Train Loss: 0.07081\n",
      "Epoch: 01 [ 2284/19985 ( 11%)], Train Loss: 0.07016\n",
      "Epoch: 01 [ 2324/19985 ( 12%)], Train Loss: 0.07003\n",
      "Epoch: 01 [ 2364/19985 ( 12%)], Train Loss: 0.07024\n",
      "Epoch: 01 [ 2404/19985 ( 12%)], Train Loss: 0.06959\n",
      "Epoch: 01 [ 2444/19985 ( 12%)], Train Loss: 0.06941\n",
      "Epoch: 01 [ 2484/19985 ( 12%)], Train Loss: 0.06911\n",
      "Epoch: 01 [ 2524/19985 ( 13%)], Train Loss: 0.06891\n",
      "Epoch: 01 [ 2564/19985 ( 13%)], Train Loss: 0.06892\n",
      "Epoch: 01 [ 2604/19985 ( 13%)], Train Loss: 0.06860\n",
      "Epoch: 01 [ 2644/19985 ( 13%)], Train Loss: 0.06837\n",
      "Epoch: 01 [ 2684/19985 ( 13%)], Train Loss: 0.06786\n",
      "Epoch: 01 [ 2724/19985 ( 14%)], Train Loss: 0.06732\n",
      "Epoch: 01 [ 2764/19985 ( 14%)], Train Loss: 0.06702\n",
      "Epoch: 01 [ 2804/19985 ( 14%)], Train Loss: 0.06661\n",
      "Epoch: 01 [ 2844/19985 ( 14%)], Train Loss: 0.06645\n",
      "Epoch: 01 [ 2884/19985 ( 14%)], Train Loss: 0.06659\n",
      "Epoch: 01 [ 2924/19985 ( 15%)], Train Loss: 0.06611\n",
      "Epoch: 01 [ 2964/19985 ( 15%)], Train Loss: 0.06579\n",
      "Epoch: 01 [ 3004/19985 ( 15%)], Train Loss: 0.06553\n",
      "Epoch: 01 [ 3044/19985 ( 15%)], Train Loss: 0.06527\n",
      "Epoch: 01 [ 3084/19985 ( 15%)], Train Loss: 0.06497\n",
      "Epoch: 01 [ 3124/19985 ( 16%)], Train Loss: 0.06460\n",
      "Epoch: 01 [ 3164/19985 ( 16%)], Train Loss: 0.06450\n",
      "Epoch: 01 [ 3204/19985 ( 16%)], Train Loss: 0.06403\n",
      "Epoch: 01 [ 3244/19985 ( 16%)], Train Loss: 0.06425\n",
      "Epoch: 01 [ 3284/19985 ( 16%)], Train Loss: 0.06403\n",
      "Epoch: 01 [ 3324/19985 ( 17%)], Train Loss: 0.06336\n",
      "Epoch: 01 [ 3364/19985 ( 17%)], Train Loss: 0.06308\n",
      "Epoch: 01 [ 3404/19985 ( 17%)], Train Loss: 0.06258\n",
      "Epoch: 01 [ 3444/19985 ( 17%)], Train Loss: 0.06239\n",
      "Epoch: 01 [ 3484/19985 ( 17%)], Train Loss: 0.06238\n",
      "Epoch: 01 [ 3524/19985 ( 18%)], Train Loss: 0.06224\n",
      "Epoch: 01 [ 3564/19985 ( 18%)], Train Loss: 0.06190\n",
      "Epoch: 01 [ 3604/19985 ( 18%)], Train Loss: 0.06183\n",
      "Epoch: 01 [ 3644/19985 ( 18%)], Train Loss: 0.06125\n",
      "Epoch: 01 [ 3684/19985 ( 18%)], Train Loss: 0.06076\n",
      "Epoch: 01 [ 3724/19985 ( 19%)], Train Loss: 0.06046\n",
      "Epoch: 01 [ 3764/19985 ( 19%)], Train Loss: 0.06017\n",
      "Epoch: 01 [ 3804/19985 ( 19%)], Train Loss: 0.05978\n",
      "Epoch: 01 [ 3844/19985 ( 19%)], Train Loss: 0.05979\n",
      "Epoch: 01 [ 3884/19985 ( 19%)], Train Loss: 0.05991\n",
      "Epoch: 01 [ 3924/19985 ( 20%)], Train Loss: 0.05957\n",
      "Epoch: 01 [ 3964/19985 ( 20%)], Train Loss: 0.05932\n",
      "Epoch: 01 [ 4004/19985 ( 20%)], Train Loss: 0.05908\n",
      "Epoch: 01 [ 4044/19985 ( 20%)], Train Loss: 0.05877\n",
      "Epoch: 01 [ 4084/19985 ( 20%)], Train Loss: 0.05840\n",
      "Epoch: 01 [ 4124/19985 ( 21%)], Train Loss: 0.05815\n",
      "Epoch: 01 [ 4164/19985 ( 21%)], Train Loss: 0.05787\n",
      "Epoch: 01 [ 4204/19985 ( 21%)], Train Loss: 0.05763\n",
      "Epoch: 01 [ 4244/19985 ( 21%)], Train Loss: 0.05791\n",
      "Epoch: 01 [ 4284/19985 ( 21%)], Train Loss: 0.05758\n",
      "Epoch: 01 [ 4324/19985 ( 22%)], Train Loss: 0.05742\n",
      "Epoch: 01 [ 4364/19985 ( 22%)], Train Loss: 0.05726\n",
      "Epoch: 01 [ 4404/19985 ( 22%)], Train Loss: 0.05702\n",
      "Epoch: 01 [ 4444/19985 ( 22%)], Train Loss: 0.05678\n",
      "Epoch: 01 [ 4484/19985 ( 22%)], Train Loss: 0.05681\n",
      "Epoch: 01 [ 4524/19985 ( 23%)], Train Loss: 0.05639\n",
      "Epoch: 01 [ 4564/19985 ( 23%)], Train Loss: 0.05624\n",
      "Epoch: 01 [ 4604/19985 ( 23%)], Train Loss: 0.05613\n",
      "Epoch: 01 [ 4644/19985 ( 23%)], Train Loss: 0.05586\n",
      "Epoch: 01 [ 4684/19985 ( 23%)], Train Loss: 0.05544\n",
      "Epoch: 01 [ 4724/19985 ( 24%)], Train Loss: 0.05539\n",
      "Epoch: 01 [ 4764/19985 ( 24%)], Train Loss: 0.05523\n",
      "Epoch: 01 [ 4804/19985 ( 24%)], Train Loss: 0.05498\n",
      "Epoch: 01 [ 4844/19985 ( 24%)], Train Loss: 0.05476\n",
      "Epoch: 01 [ 4884/19985 ( 24%)], Train Loss: 0.05463\n",
      "Epoch: 01 [ 4924/19985 ( 25%)], Train Loss: 0.05451\n",
      "Epoch: 01 [ 4964/19985 ( 25%)], Train Loss: 0.05442\n",
      "Epoch: 01 [ 5004/19985 ( 25%)], Train Loss: 0.05409\n",
      "Epoch: 01 [ 5044/19985 ( 25%)], Train Loss: 0.05399\n",
      "Epoch: 01 [ 5084/19985 ( 25%)], Train Loss: 0.05373\n",
      "Epoch: 01 [ 5124/19985 ( 26%)], Train Loss: 0.05366\n",
      "Epoch: 01 [ 5164/19985 ( 26%)], Train Loss: 0.05367\n",
      "Epoch: 01 [ 5204/19985 ( 26%)], Train Loss: 0.05344\n",
      "Epoch: 01 [ 5244/19985 ( 26%)], Train Loss: 0.05319\n",
      "Epoch: 01 [ 5284/19985 ( 26%)], Train Loss: 0.05309\n",
      "Epoch: 01 [ 5324/19985 ( 27%)], Train Loss: 0.05278\n",
      "Epoch: 01 [ 5364/19985 ( 27%)], Train Loss: 0.05253\n",
      "Epoch: 01 [ 5404/19985 ( 27%)], Train Loss: 0.05228\n",
      "Epoch: 01 [ 5444/19985 ( 27%)], Train Loss: 0.05225\n",
      "Epoch: 01 [ 5484/19985 ( 27%)], Train Loss: 0.05225\n",
      "Epoch: 01 [ 5524/19985 ( 28%)], Train Loss: 0.05218\n",
      "Epoch: 01 [ 5564/19985 ( 28%)], Train Loss: 0.05192\n",
      "Epoch: 01 [ 5604/19985 ( 28%)], Train Loss: 0.05191\n",
      "Epoch: 01 [ 5644/19985 ( 28%)], Train Loss: 0.05172\n",
      "Epoch: 01 [ 5684/19985 ( 28%)], Train Loss: 0.05146\n",
      "Epoch: 01 [ 5724/19985 ( 29%)], Train Loss: 0.05152\n",
      "Epoch: 01 [ 5764/19985 ( 29%)], Train Loss: 0.05124\n",
      "Epoch: 01 [ 5804/19985 ( 29%)], Train Loss: 0.05102\n",
      "Epoch: 01 [ 5844/19985 ( 29%)], Train Loss: 0.05088\n",
      "Epoch: 01 [ 5884/19985 ( 29%)], Train Loss: 0.05068\n",
      "Epoch: 01 [ 5924/19985 ( 30%)], Train Loss: 0.05055\n",
      "Epoch: 01 [ 5964/19985 ( 30%)], Train Loss: 0.05048\n",
      "Epoch: 01 [ 6004/19985 ( 30%)], Train Loss: 0.05026\n",
      "Epoch: 01 [ 6044/19985 ( 30%)], Train Loss: 0.05001\n",
      "Epoch: 01 [ 6084/19985 ( 30%)], Train Loss: 0.04984\n",
      "Epoch: 01 [ 6124/19985 ( 31%)], Train Loss: 0.04975\n",
      "Epoch: 01 [ 6164/19985 ( 31%)], Train Loss: 0.04966\n",
      "Epoch: 01 [ 6204/19985 ( 31%)], Train Loss: 0.04947\n",
      "Epoch: 01 [ 6244/19985 ( 31%)], Train Loss: 0.04933\n",
      "Epoch: 01 [ 6284/19985 ( 31%)], Train Loss: 0.04922\n",
      "Epoch: 01 [ 6324/19985 ( 32%)], Train Loss: 0.04915\n",
      "Epoch: 01 [ 6364/19985 ( 32%)], Train Loss: 0.04893\n",
      "Epoch: 01 [ 6404/19985 ( 32%)], Train Loss: 0.04899\n",
      "Epoch: 01 [ 6444/19985 ( 32%)], Train Loss: 0.04896\n",
      "Epoch: 01 [ 6484/19985 ( 32%)], Train Loss: 0.04881\n",
      "Epoch: 01 [ 6524/19985 ( 33%)], Train Loss: 0.04866\n",
      "Epoch: 01 [ 6564/19985 ( 33%)], Train Loss: 0.04866\n",
      "Epoch: 01 [ 6604/19985 ( 33%)], Train Loss: 0.04847\n",
      "Epoch: 01 [ 6644/19985 ( 33%)], Train Loss: 0.04833\n",
      "Epoch: 01 [ 6684/19985 ( 33%)], Train Loss: 0.04816\n",
      "Epoch: 01 [ 6724/19985 ( 34%)], Train Loss: 0.04800\n",
      "Epoch: 01 [ 6764/19985 ( 34%)], Train Loss: 0.04785\n",
      "Epoch: 01 [ 6804/19985 ( 34%)], Train Loss: 0.04785\n",
      "Epoch: 01 [ 6844/19985 ( 34%)], Train Loss: 0.04800\n",
      "Epoch: 01 [ 6884/19985 ( 34%)], Train Loss: 0.04791\n",
      "Epoch: 01 [ 6924/19985 ( 35%)], Train Loss: 0.04778\n",
      "Epoch: 01 [ 6964/19985 ( 35%)], Train Loss: 0.04775\n",
      "Epoch: 01 [ 7004/19985 ( 35%)], Train Loss: 0.04778\n",
      "Epoch: 01 [ 7044/19985 ( 35%)], Train Loss: 0.04763\n",
      "Epoch: 01 [ 7084/19985 ( 35%)], Train Loss: 0.04760\n",
      "Epoch: 01 [ 7124/19985 ( 36%)], Train Loss: 0.04756\n",
      "Epoch: 01 [ 7164/19985 ( 36%)], Train Loss: 0.04738\n",
      "Epoch: 01 [ 7204/19985 ( 36%)], Train Loss: 0.04731\n",
      "Epoch: 01 [ 7244/19985 ( 36%)], Train Loss: 0.04739\n",
      "Epoch: 01 [ 7284/19985 ( 36%)], Train Loss: 0.04726\n",
      "Epoch: 01 [ 7324/19985 ( 37%)], Train Loss: 0.04727\n",
      "Epoch: 01 [ 7364/19985 ( 37%)], Train Loss: 0.04716\n",
      "Epoch: 01 [ 7404/19985 ( 37%)], Train Loss: 0.04698\n",
      "Epoch: 01 [ 7444/19985 ( 37%)], Train Loss: 0.04682\n",
      "Epoch: 01 [ 7484/19985 ( 37%)], Train Loss: 0.04676\n",
      "Epoch: 01 [ 7524/19985 ( 38%)], Train Loss: 0.04683\n",
      "Epoch: 01 [ 7564/19985 ( 38%)], Train Loss: 0.04672\n",
      "Epoch: 01 [ 7604/19985 ( 38%)], Train Loss: 0.04663\n",
      "Epoch: 01 [ 7644/19985 ( 38%)], Train Loss: 0.04662\n",
      "Epoch: 01 [ 7684/19985 ( 38%)], Train Loss: 0.04661\n",
      "Epoch: 01 [ 7724/19985 ( 39%)], Train Loss: 0.04651\n",
      "Epoch: 01 [ 7764/19985 ( 39%)], Train Loss: 0.04637\n",
      "Epoch: 01 [ 7804/19985 ( 39%)], Train Loss: 0.04636\n",
      "Epoch: 01 [ 7844/19985 ( 39%)], Train Loss: 0.04639\n",
      "Epoch: 01 [ 7884/19985 ( 39%)], Train Loss: 0.04650\n",
      "Epoch: 01 [ 7924/19985 ( 40%)], Train Loss: 0.04647\n",
      "Epoch: 01 [ 7964/19985 ( 40%)], Train Loss: 0.04642\n",
      "Epoch: 01 [ 8004/19985 ( 40%)], Train Loss: 0.04633\n",
      "Epoch: 01 [ 8044/19985 ( 40%)], Train Loss: 0.04618\n",
      "Epoch: 01 [ 8084/19985 ( 40%)], Train Loss: 0.04604\n",
      "Epoch: 01 [ 8124/19985 ( 41%)], Train Loss: 0.04597\n",
      "Epoch: 01 [ 8164/19985 ( 41%)], Train Loss: 0.04584\n",
      "Epoch: 01 [ 8204/19985 ( 41%)], Train Loss: 0.04569\n",
      "Epoch: 01 [ 8244/19985 ( 41%)], Train Loss: 0.04564\n",
      "Epoch: 01 [ 8284/19985 ( 41%)], Train Loss: 0.04569\n",
      "Epoch: 01 [ 8324/19985 ( 42%)], Train Loss: 0.04564\n",
      "Epoch: 01 [ 8364/19985 ( 42%)], Train Loss: 0.04571\n",
      "Epoch: 01 [ 8404/19985 ( 42%)], Train Loss: 0.04571\n",
      "Epoch: 01 [ 8444/19985 ( 42%)], Train Loss: 0.04566\n",
      "Epoch: 01 [ 8484/19985 ( 42%)], Train Loss: 0.04555\n",
      "Epoch: 01 [ 8524/19985 ( 43%)], Train Loss: 0.04567\n",
      "Epoch: 01 [ 8564/19985 ( 43%)], Train Loss: 0.04562\n",
      "Epoch: 01 [ 8604/19985 ( 43%)], Train Loss: 0.04551\n",
      "Epoch: 01 [ 8644/19985 ( 43%)], Train Loss: 0.04542\n",
      "Epoch: 01 [ 8684/19985 ( 43%)], Train Loss: 0.04539\n",
      "Epoch: 01 [ 8724/19985 ( 44%)], Train Loss: 0.04527\n",
      "Epoch: 01 [ 8764/19985 ( 44%)], Train Loss: 0.04513\n",
      "Epoch: 01 [ 8804/19985 ( 44%)], Train Loss: 0.04511\n",
      "Epoch: 01 [ 8844/19985 ( 44%)], Train Loss: 0.04526\n",
      "Epoch: 01 [ 8884/19985 ( 44%)], Train Loss: 0.04532\n",
      "Epoch: 01 [ 8924/19985 ( 45%)], Train Loss: 0.04517\n",
      "Epoch: 01 [ 8964/19985 ( 45%)], Train Loss: 0.04508\n",
      "Epoch: 01 [ 9004/19985 ( 45%)], Train Loss: 0.04503\n",
      "Epoch: 01 [ 9044/19985 ( 45%)], Train Loss: 0.04495\n",
      "Epoch: 01 [ 9084/19985 ( 45%)], Train Loss: 0.04493\n",
      "Epoch: 01 [ 9124/19985 ( 46%)], Train Loss: 0.04497\n",
      "Epoch: 01 [ 9164/19985 ( 46%)], Train Loss: 0.04484\n",
      "Epoch: 01 [ 9204/19985 ( 46%)], Train Loss: 0.04480\n",
      "Epoch: 01 [ 9244/19985 ( 46%)], Train Loss: 0.04468\n",
      "Epoch: 01 [ 9284/19985 ( 46%)], Train Loss: 0.04466\n",
      "Epoch: 01 [ 9324/19985 ( 47%)], Train Loss: 0.04454\n",
      "Epoch: 01 [ 9364/19985 ( 47%)], Train Loss: 0.04454\n",
      "Epoch: 01 [ 9404/19985 ( 47%)], Train Loss: 0.04446\n",
      "Epoch: 01 [ 9444/19985 ( 47%)], Train Loss: 0.04447\n",
      "Epoch: 01 [ 9484/19985 ( 47%)], Train Loss: 0.04443\n",
      "Epoch: 01 [ 9524/19985 ( 48%)], Train Loss: 0.04443\n",
      "Epoch: 01 [ 9564/19985 ( 48%)], Train Loss: 0.04447\n",
      "Epoch: 01 [ 9604/19985 ( 48%)], Train Loss: 0.04442\n",
      "Epoch: 01 [ 9644/19985 ( 48%)], Train Loss: 0.04446\n",
      "Epoch: 01 [ 9684/19985 ( 48%)], Train Loss: 0.04439\n",
      "Epoch: 01 [ 9724/19985 ( 49%)], Train Loss: 0.04431\n",
      "Epoch: 01 [ 9764/19985 ( 49%)], Train Loss: 0.04420\n",
      "Epoch: 01 [ 9804/19985 ( 49%)], Train Loss: 0.04418\n",
      "Epoch: 01 [ 9844/19985 ( 49%)], Train Loss: 0.04417\n",
      "Epoch: 01 [ 9884/19985 ( 49%)], Train Loss: 0.04403\n",
      "Epoch: 01 [ 9924/19985 ( 50%)], Train Loss: 0.04398\n",
      "Epoch: 01 [ 9964/19985 ( 50%)], Train Loss: 0.04391\n",
      "Epoch: 01 [10004/19985 ( 50%)], Train Loss: 0.04386\n",
      "Epoch: 01 [10044/19985 ( 50%)], Train Loss: 0.04374\n",
      "Epoch: 01 [10084/19985 ( 50%)], Train Loss: 0.04365\n",
      "Epoch: 01 [10124/19985 ( 51%)], Train Loss: 0.04359\n",
      "Epoch: 01 [10164/19985 ( 51%)], Train Loss: 0.04348\n",
      "Epoch: 01 [10204/19985 ( 51%)], Train Loss: 0.04346\n",
      "Epoch: 01 [10244/19985 ( 51%)], Train Loss: 0.04339\n",
      "Epoch: 01 [10284/19985 ( 51%)], Train Loss: 0.04330\n",
      "Epoch: 01 [10324/19985 ( 52%)], Train Loss: 0.04322\n",
      "Epoch: 01 [10364/19985 ( 52%)], Train Loss: 0.04315\n",
      "Epoch: 01 [10404/19985 ( 52%)], Train Loss: 0.04308\n",
      "Epoch: 01 [10444/19985 ( 52%)], Train Loss: 0.04309\n",
      "Epoch: 01 [10484/19985 ( 52%)], Train Loss: 0.04303\n",
      "Epoch: 01 [10524/19985 ( 53%)], Train Loss: 0.04290\n",
      "Epoch: 01 [10564/19985 ( 53%)], Train Loss: 0.04286\n",
      "Epoch: 01 [10604/19985 ( 53%)], Train Loss: 0.04287\n",
      "Epoch: 01 [10644/19985 ( 53%)], Train Loss: 0.04281\n",
      "Epoch: 01 [10684/19985 ( 53%)], Train Loss: 0.04286\n",
      "Epoch: 01 [10724/19985 ( 54%)], Train Loss: 0.04279\n",
      "Epoch: 01 [10764/19985 ( 54%)], Train Loss: 0.04271\n",
      "Epoch: 01 [10804/19985 ( 54%)], Train Loss: 0.04266\n",
      "Epoch: 01 [10844/19985 ( 54%)], Train Loss: 0.04268\n",
      "Epoch: 01 [10884/19985 ( 54%)], Train Loss: 0.04262\n",
      "Epoch: 01 [10924/19985 ( 55%)], Train Loss: 0.04256\n",
      "Epoch: 01 [10964/19985 ( 55%)], Train Loss: 0.04252\n",
      "Epoch: 01 [11004/19985 ( 55%)], Train Loss: 0.04241\n",
      "Epoch: 01 [11044/19985 ( 55%)], Train Loss: 0.04239\n",
      "Epoch: 01 [11084/19985 ( 55%)], Train Loss: 0.04234\n",
      "Epoch: 01 [11124/19985 ( 56%)], Train Loss: 0.04226\n",
      "Epoch: 01 [11164/19985 ( 56%)], Train Loss: 0.04221\n",
      "Epoch: 01 [11204/19985 ( 56%)], Train Loss: 0.04227\n",
      "Epoch: 01 [11244/19985 ( 56%)], Train Loss: 0.04220\n",
      "Epoch: 01 [11284/19985 ( 56%)], Train Loss: 0.04216\n",
      "Epoch: 01 [11324/19985 ( 57%)], Train Loss: 0.04208\n",
      "Epoch: 01 [11364/19985 ( 57%)], Train Loss: 0.04199\n",
      "Epoch: 01 [11404/19985 ( 57%)], Train Loss: 0.04206\n",
      "Epoch: 01 [11444/19985 ( 57%)], Train Loss: 0.04204\n",
      "Epoch: 01 [11484/19985 ( 57%)], Train Loss: 0.04198\n",
      "Epoch: 01 [11524/19985 ( 58%)], Train Loss: 0.04197\n",
      "Epoch: 01 [11564/19985 ( 58%)], Train Loss: 0.04198\n",
      "Epoch: 01 [11604/19985 ( 58%)], Train Loss: 0.04197\n",
      "Epoch: 01 [11644/19985 ( 58%)], Train Loss: 0.04193\n",
      "Epoch: 01 [11684/19985 ( 58%)], Train Loss: 0.04187\n",
      "Epoch: 01 [11724/19985 ( 59%)], Train Loss: 0.04184\n",
      "Epoch: 01 [11764/19985 ( 59%)], Train Loss: 0.04179\n",
      "Epoch: 01 [11804/19985 ( 59%)], Train Loss: 0.04177\n",
      "Epoch: 01 [11844/19985 ( 59%)], Train Loss: 0.04173\n",
      "Epoch: 01 [11884/19985 ( 59%)], Train Loss: 0.04175\n",
      "Epoch: 01 [11924/19985 ( 60%)], Train Loss: 0.04174\n",
      "Epoch: 01 [11964/19985 ( 60%)], Train Loss: 0.04172\n",
      "Epoch: 01 [12004/19985 ( 60%)], Train Loss: 0.04173\n",
      "Epoch: 01 [12044/19985 ( 60%)], Train Loss: 0.04168\n",
      "Epoch: 01 [12084/19985 ( 60%)], Train Loss: 0.04165\n",
      "Epoch: 01 [12124/19985 ( 61%)], Train Loss: 0.04160\n",
      "Epoch: 01 [12164/19985 ( 61%)], Train Loss: 0.04164\n",
      "Epoch: 01 [12204/19985 ( 61%)], Train Loss: 0.04169\n",
      "Epoch: 01 [12244/19985 ( 61%)], Train Loss: 0.04163\n",
      "Epoch: 01 [12284/19985 ( 61%)], Train Loss: 0.04176\n",
      "Epoch: 01 [12324/19985 ( 62%)], Train Loss: 0.04170\n",
      "Epoch: 01 [12364/19985 ( 62%)], Train Loss: 0.04185\n",
      "Epoch: 01 [12404/19985 ( 62%)], Train Loss: 0.04180\n",
      "Epoch: 01 [12444/19985 ( 62%)], Train Loss: 0.04172\n",
      "Epoch: 01 [12484/19985 ( 62%)], Train Loss: 0.04173\n",
      "Epoch: 01 [12524/19985 ( 63%)], Train Loss: 0.04174\n",
      "Epoch: 01 [12564/19985 ( 63%)], Train Loss: 0.04172\n",
      "Epoch: 01 [12604/19985 ( 63%)], Train Loss: 0.04169\n",
      "Epoch: 01 [12644/19985 ( 63%)], Train Loss: 0.04165\n",
      "Epoch: 01 [12684/19985 ( 63%)], Train Loss: 0.04163\n",
      "Epoch: 01 [12724/19985 ( 64%)], Train Loss: 0.04155\n",
      "Epoch: 01 [12764/19985 ( 64%)], Train Loss: 0.04159\n",
      "Epoch: 01 [12804/19985 ( 64%)], Train Loss: 0.04159\n",
      "Epoch: 01 [12844/19985 ( 64%)], Train Loss: 0.04155\n",
      "Epoch: 01 [12884/19985 ( 64%)], Train Loss: 0.04153\n",
      "Epoch: 01 [12924/19985 ( 65%)], Train Loss: 0.04148\n",
      "Epoch: 01 [12964/19985 ( 65%)], Train Loss: 0.04144\n",
      "Epoch: 01 [13004/19985 ( 65%)], Train Loss: 0.04142\n",
      "Epoch: 01 [13044/19985 ( 65%)], Train Loss: 0.04135\n",
      "Epoch: 01 [13084/19985 ( 65%)], Train Loss: 0.04131\n",
      "Epoch: 01 [13124/19985 ( 66%)], Train Loss: 0.04125\n",
      "Epoch: 01 [13164/19985 ( 66%)], Train Loss: 0.04119\n",
      "Epoch: 01 [13204/19985 ( 66%)], Train Loss: 0.04118\n",
      "Epoch: 01 [13244/19985 ( 66%)], Train Loss: 0.04110\n",
      "Epoch: 01 [13284/19985 ( 66%)], Train Loss: 0.04112\n",
      "Epoch: 01 [13324/19985 ( 67%)], Train Loss: 0.04104\n",
      "Epoch: 01 [13364/19985 ( 67%)], Train Loss: 0.04102\n",
      "Epoch: 01 [13404/19985 ( 67%)], Train Loss: 0.04095\n",
      "Epoch: 01 [13444/19985 ( 67%)], Train Loss: 0.04089\n",
      "Epoch: 01 [13484/19985 ( 67%)], Train Loss: 0.04091\n",
      "Epoch: 01 [13524/19985 ( 68%)], Train Loss: 0.04086\n",
      "Epoch: 01 [13564/19985 ( 68%)], Train Loss: 0.04079\n",
      "Epoch: 01 [13604/19985 ( 68%)], Train Loss: 0.04077\n",
      "Epoch: 01 [13644/19985 ( 68%)], Train Loss: 0.04068\n",
      "Epoch: 01 [13684/19985 ( 68%)], Train Loss: 0.04063\n",
      "Epoch: 01 [13724/19985 ( 69%)], Train Loss: 0.04058\n",
      "Epoch: 01 [13764/19985 ( 69%)], Train Loss: 0.04055\n",
      "Epoch: 01 [13804/19985 ( 69%)], Train Loss: 0.04052\n",
      "Epoch: 01 [13844/19985 ( 69%)], Train Loss: 0.04054\n",
      "Epoch: 01 [13884/19985 ( 69%)], Train Loss: 0.04048\n",
      "Epoch: 01 [13924/19985 ( 70%)], Train Loss: 0.04046\n",
      "Epoch: 01 [13964/19985 ( 70%)], Train Loss: 0.04050\n",
      "Epoch: 01 [14004/19985 ( 70%)], Train Loss: 0.04047\n",
      "Epoch: 01 [14044/19985 ( 70%)], Train Loss: 0.04046\n",
      "Epoch: 01 [14084/19985 ( 70%)], Train Loss: 0.04041\n",
      "Epoch: 01 [14124/19985 ( 71%)], Train Loss: 0.04037\n",
      "Epoch: 01 [14164/19985 ( 71%)], Train Loss: 0.04032\n",
      "Epoch: 01 [14204/19985 ( 71%)], Train Loss: 0.04030\n",
      "Epoch: 01 [14244/19985 ( 71%)], Train Loss: 0.04029\n",
      "Epoch: 01 [14284/19985 ( 71%)], Train Loss: 0.04032\n",
      "Epoch: 01 [14324/19985 ( 72%)], Train Loss: 0.04024\n",
      "Epoch: 01 [14364/19985 ( 72%)], Train Loss: 0.04024\n",
      "Epoch: 01 [14404/19985 ( 72%)], Train Loss: 0.04026\n",
      "Epoch: 01 [14444/19985 ( 72%)], Train Loss: 0.04022\n",
      "Epoch: 01 [14484/19985 ( 72%)], Train Loss: 0.04033\n",
      "Epoch: 01 [14524/19985 ( 73%)], Train Loss: 0.04027\n",
      "Epoch: 01 [14564/19985 ( 73%)], Train Loss: 0.04021\n",
      "Epoch: 01 [14604/19985 ( 73%)], Train Loss: 0.04023\n",
      "Epoch: 01 [14644/19985 ( 73%)], Train Loss: 0.04017\n",
      "Epoch: 01 [14684/19985 ( 73%)], Train Loss: 0.04010\n",
      "Epoch: 01 [14724/19985 ( 74%)], Train Loss: 0.04006\n",
      "Epoch: 01 [14764/19985 ( 74%)], Train Loss: 0.04004\n",
      "Epoch: 01 [14804/19985 ( 74%)], Train Loss: 0.03998\n",
      "Epoch: 01 [14844/19985 ( 74%)], Train Loss: 0.03997\n",
      "Epoch: 01 [14884/19985 ( 74%)], Train Loss: 0.03989\n",
      "Epoch: 01 [14924/19985 ( 75%)], Train Loss: 0.03992\n",
      "Epoch: 01 [14964/19985 ( 75%)], Train Loss: 0.03991\n",
      "Epoch: 01 [15004/19985 ( 75%)], Train Loss: 0.03994\n",
      "Epoch: 01 [15044/19985 ( 75%)], Train Loss: 0.03989\n",
      "Epoch: 01 [15084/19985 ( 75%)], Train Loss: 0.03990\n",
      "Epoch: 01 [15124/19985 ( 76%)], Train Loss: 0.03988\n",
      "Epoch: 01 [15164/19985 ( 76%)], Train Loss: 0.03994\n",
      "Epoch: 01 [15204/19985 ( 76%)], Train Loss: 0.03993\n",
      "Epoch: 01 [15244/19985 ( 76%)], Train Loss: 0.03991\n",
      "Epoch: 01 [15284/19985 ( 76%)], Train Loss: 0.03994\n",
      "Epoch: 01 [15324/19985 ( 77%)], Train Loss: 0.03990\n",
      "Epoch: 01 [15364/19985 ( 77%)], Train Loss: 0.03984\n",
      "Epoch: 01 [15404/19985 ( 77%)], Train Loss: 0.03980\n",
      "Epoch: 01 [15444/19985 ( 77%)], Train Loss: 0.03982\n",
      "Epoch: 01 [15484/19985 ( 77%)], Train Loss: 0.03978\n",
      "Epoch: 01 [15524/19985 ( 78%)], Train Loss: 0.03977\n",
      "Epoch: 01 [15564/19985 ( 78%)], Train Loss: 0.03974\n",
      "Epoch: 01 [15604/19985 ( 78%)], Train Loss: 0.03971\n",
      "Epoch: 01 [15644/19985 ( 78%)], Train Loss: 0.03970\n",
      "Epoch: 01 [15684/19985 ( 78%)], Train Loss: 0.03965\n",
      "Epoch: 01 [15724/19985 ( 79%)], Train Loss: 0.03967\n",
      "Epoch: 01 [15764/19985 ( 79%)], Train Loss: 0.03961\n",
      "Epoch: 01 [15804/19985 ( 79%)], Train Loss: 0.03956\n",
      "Epoch: 01 [15844/19985 ( 79%)], Train Loss: 0.03949\n",
      "Epoch: 01 [15884/19985 ( 79%)], Train Loss: 0.03948\n",
      "Epoch: 01 [15924/19985 ( 80%)], Train Loss: 0.03944\n",
      "Epoch: 01 [15964/19985 ( 80%)], Train Loss: 0.03944\n",
      "Epoch: 01 [16004/19985 ( 80%)], Train Loss: 0.03942\n",
      "Epoch: 01 [16044/19985 ( 80%)], Train Loss: 0.03938\n",
      "Epoch: 01 [16084/19985 ( 80%)], Train Loss: 0.03932\n",
      "Epoch: 01 [16124/19985 ( 81%)], Train Loss: 0.03933\n",
      "Epoch: 01 [16164/19985 ( 81%)], Train Loss: 0.03926\n",
      "Epoch: 01 [16204/19985 ( 81%)], Train Loss: 0.03922\n",
      "Epoch: 01 [16244/19985 ( 81%)], Train Loss: 0.03920\n",
      "Epoch: 01 [16284/19985 ( 81%)], Train Loss: 0.03921\n",
      "Epoch: 01 [16324/19985 ( 82%)], Train Loss: 0.03918\n",
      "Epoch: 01 [16364/19985 ( 82%)], Train Loss: 0.03913\n",
      "Epoch: 01 [16404/19985 ( 82%)], Train Loss: 0.03907\n",
      "Epoch: 01 [16444/19985 ( 82%)], Train Loss: 0.03906\n",
      "Epoch: 01 [16484/19985 ( 82%)], Train Loss: 0.03902\n",
      "Epoch: 01 [16524/19985 ( 83%)], Train Loss: 0.03896\n",
      "Epoch: 01 [16564/19985 ( 83%)], Train Loss: 0.03894\n",
      "Epoch: 01 [16604/19985 ( 83%)], Train Loss: 0.03889\n",
      "Epoch: 01 [16644/19985 ( 83%)], Train Loss: 0.03884\n",
      "Epoch: 01 [16684/19985 ( 83%)], Train Loss: 0.03881\n",
      "Epoch: 01 [16724/19985 ( 84%)], Train Loss: 0.03886\n",
      "Epoch: 01 [16764/19985 ( 84%)], Train Loss: 0.03890\n",
      "Epoch: 01 [16804/19985 ( 84%)], Train Loss: 0.03890\n",
      "Epoch: 01 [16844/19985 ( 84%)], Train Loss: 0.03886\n",
      "Epoch: 01 [16884/19985 ( 84%)], Train Loss: 0.03884\n",
      "Epoch: 01 [16924/19985 ( 85%)], Train Loss: 0.03884\n",
      "Epoch: 01 [16964/19985 ( 85%)], Train Loss: 0.03883\n",
      "Epoch: 01 [17004/19985 ( 85%)], Train Loss: 0.03879\n",
      "Epoch: 01 [17044/19985 ( 85%)], Train Loss: 0.03875\n",
      "Epoch: 01 [17084/19985 ( 85%)], Train Loss: 0.03876\n",
      "Epoch: 01 [17124/19985 ( 86%)], Train Loss: 0.03873\n",
      "Epoch: 01 [17164/19985 ( 86%)], Train Loss: 0.03865\n",
      "Epoch: 01 [17204/19985 ( 86%)], Train Loss: 0.03861\n",
      "Epoch: 01 [17244/19985 ( 86%)], Train Loss: 0.03861\n",
      "Epoch: 01 [17284/19985 ( 86%)], Train Loss: 0.03858\n",
      "Epoch: 01 [17324/19985 ( 87%)], Train Loss: 0.03852\n",
      "Epoch: 01 [17364/19985 ( 87%)], Train Loss: 0.03847\n",
      "Epoch: 01 [17404/19985 ( 87%)], Train Loss: 0.03845\n",
      "Epoch: 01 [17444/19985 ( 87%)], Train Loss: 0.03847\n",
      "Epoch: 01 [17484/19985 ( 87%)], Train Loss: 0.03845\n",
      "Epoch: 01 [17524/19985 ( 88%)], Train Loss: 0.03842\n",
      "Epoch: 01 [17564/19985 ( 88%)], Train Loss: 0.03838\n",
      "Epoch: 01 [17604/19985 ( 88%)], Train Loss: 0.03835\n",
      "Epoch: 01 [17644/19985 ( 88%)], Train Loss: 0.03838\n",
      "Epoch: 01 [17684/19985 ( 88%)], Train Loss: 0.03848\n",
      "Epoch: 01 [17724/19985 ( 89%)], Train Loss: 0.03854\n",
      "Epoch: 01 [17764/19985 ( 89%)], Train Loss: 0.03850\n",
      "Epoch: 01 [17804/19985 ( 89%)], Train Loss: 0.03843\n",
      "Epoch: 01 [17844/19985 ( 89%)], Train Loss: 0.03841\n",
      "Epoch: 01 [17884/19985 ( 89%)], Train Loss: 0.03841\n",
      "Epoch: 01 [17924/19985 ( 90%)], Train Loss: 0.03843\n",
      "Epoch: 01 [17964/19985 ( 90%)], Train Loss: 0.03840\n",
      "Epoch: 01 [18004/19985 ( 90%)], Train Loss: 0.03841\n",
      "Epoch: 01 [18044/19985 ( 90%)], Train Loss: 0.03839\n",
      "Epoch: 01 [18084/19985 ( 90%)], Train Loss: 0.03835\n",
      "Epoch: 01 [18124/19985 ( 91%)], Train Loss: 0.03832\n",
      "Epoch: 01 [18164/19985 ( 91%)], Train Loss: 0.03828\n",
      "Epoch: 01 [18204/19985 ( 91%)], Train Loss: 0.03830\n",
      "Epoch: 01 [18244/19985 ( 91%)], Train Loss: 0.03828\n",
      "Epoch: 01 [18284/19985 ( 91%)], Train Loss: 0.03830\n",
      "Epoch: 01 [18324/19985 ( 92%)], Train Loss: 0.03826\n",
      "Epoch: 01 [18364/19985 ( 92%)], Train Loss: 0.03827\n",
      "Epoch: 01 [18404/19985 ( 92%)], Train Loss: 0.03822\n",
      "Epoch: 01 [18444/19985 ( 92%)], Train Loss: 0.03823\n",
      "Epoch: 01 [18484/19985 ( 92%)], Train Loss: 0.03822\n",
      "Epoch: 01 [18524/19985 ( 93%)], Train Loss: 0.03821\n",
      "Epoch: 01 [18564/19985 ( 93%)], Train Loss: 0.03825\n",
      "Epoch: 01 [18604/19985 ( 93%)], Train Loss: 0.03824\n",
      "Epoch: 01 [18644/19985 ( 93%)], Train Loss: 0.03819\n",
      "Epoch: 01 [18684/19985 ( 93%)], Train Loss: 0.03818\n",
      "Epoch: 01 [18724/19985 ( 94%)], Train Loss: 0.03815\n",
      "Epoch: 01 [18764/19985 ( 94%)], Train Loss: 0.03813\n",
      "Epoch: 01 [18804/19985 ( 94%)], Train Loss: 0.03808\n",
      "Epoch: 01 [18844/19985 ( 94%)], Train Loss: 0.03805\n",
      "Epoch: 01 [18884/19985 ( 94%)], Train Loss: 0.03806\n",
      "Epoch: 01 [18924/19985 ( 95%)], Train Loss: 0.03807\n",
      "Epoch: 01 [18964/19985 ( 95%)], Train Loss: 0.03805\n",
      "Epoch: 01 [19004/19985 ( 95%)], Train Loss: 0.03806\n",
      "Epoch: 01 [19044/19985 ( 95%)], Train Loss: 0.03804\n",
      "Epoch: 01 [19084/19985 ( 95%)], Train Loss: 0.03809\n",
      "Epoch: 01 [19124/19985 ( 96%)], Train Loss: 0.03808\n",
      "Epoch: 01 [19164/19985 ( 96%)], Train Loss: 0.03803\n",
      "Epoch: 01 [19204/19985 ( 96%)], Train Loss: 0.03806\n",
      "Epoch: 01 [19244/19985 ( 96%)], Train Loss: 0.03802\n",
      "Epoch: 01 [19284/19985 ( 96%)], Train Loss: 0.03802\n",
      "Epoch: 01 [19324/19985 ( 97%)], Train Loss: 0.03802\n",
      "Epoch: 01 [19364/19985 ( 97%)], Train Loss: 0.03805\n",
      "Epoch: 01 [19404/19985 ( 97%)], Train Loss: 0.03807\n",
      "Epoch: 01 [19444/19985 ( 97%)], Train Loss: 0.03805\n",
      "Epoch: 01 [19484/19985 ( 97%)], Train Loss: 0.03804\n",
      "Epoch: 01 [19524/19985 ( 98%)], Train Loss: 0.03805\n",
      "Epoch: 01 [19564/19985 ( 98%)], Train Loss: 0.03800\n",
      "Epoch: 01 [19604/19985 ( 98%)], Train Loss: 0.03798\n",
      "Epoch: 01 [19644/19985 ( 98%)], Train Loss: 0.03796\n",
      "Epoch: 01 [19684/19985 ( 98%)], Train Loss: 0.03801\n",
      "Epoch: 01 [19724/19985 ( 99%)], Train Loss: 0.03802\n",
      "Epoch: 01 [19764/19985 ( 99%)], Train Loss: 0.03801\n",
      "Epoch: 01 [19804/19985 ( 99%)], Train Loss: 0.03801\n",
      "Epoch: 01 [19844/19985 ( 99%)], Train Loss: 0.03796\n",
      "Epoch: 01 [19884/19985 ( 99%)], Train Loss: 0.03795\n",
      "Epoch: 01 [19924/19985 (100%)], Train Loss: 0.03793\n",
      "Epoch: 01 [19964/19985 (100%)], Train Loss: 0.03791\n",
      "Epoch: 01 [19985/19985 (100%)], Train Loss: 0.03789\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.28529\n",
      "\n",
      "Total Training Time: 5259.965389251709secs, Average Training Time per Epoch: 2629.9826946258545secs.\n",
      "Total Validation Time: 269.79840660095215secs, Average Validation Time per Epoch: 134.89920330047607secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 1\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20298, Num examples Valid=2771\n",
      "Total Training Steps: 1270, Total Warmup Steps: 127\n",
      "Epoch: 00 [    4/20298 (  0%)], Train Loss: 0.71434\n",
      "Epoch: 00 [   44/20298 (  0%)], Train Loss: 0.72961\n",
      "Epoch: 00 [   84/20298 (  0%)], Train Loss: 0.72936\n",
      "Epoch: 00 [  124/20298 (  1%)], Train Loss: 0.72555\n",
      "Epoch: 00 [  164/20298 (  1%)], Train Loss: 0.72303\n",
      "Epoch: 00 [  204/20298 (  1%)], Train Loss: 0.72254\n",
      "Epoch: 00 [  244/20298 (  1%)], Train Loss: 0.71940\n",
      "Epoch: 00 [  284/20298 (  1%)], Train Loss: 0.71560\n",
      "Epoch: 00 [  324/20298 (  2%)], Train Loss: 0.71171\n",
      "Epoch: 00 [  364/20298 (  2%)], Train Loss: 0.70811\n",
      "Epoch: 00 [  404/20298 (  2%)], Train Loss: 0.70467\n",
      "Epoch: 00 [  444/20298 (  2%)], Train Loss: 0.70026\n",
      "Epoch: 00 [  484/20298 (  2%)], Train Loss: 0.69588\n",
      "Epoch: 00 [  524/20298 (  3%)], Train Loss: 0.68975\n",
      "Epoch: 00 [  564/20298 (  3%)], Train Loss: 0.68256\n",
      "Epoch: 00 [  604/20298 (  3%)], Train Loss: 0.67516\n",
      "Epoch: 00 [  644/20298 (  3%)], Train Loss: 0.66728\n",
      "Epoch: 00 [  684/20298 (  3%)], Train Loss: 0.65945\n",
      "Epoch: 00 [  724/20298 (  4%)], Train Loss: 0.65277\n",
      "Epoch: 00 [  764/20298 (  4%)], Train Loss: 0.64547\n",
      "Epoch: 00 [  804/20298 (  4%)], Train Loss: 0.63607\n",
      "Epoch: 00 [  844/20298 (  4%)], Train Loss: 0.62767\n",
      "Epoch: 00 [  884/20298 (  4%)], Train Loss: 0.61826\n",
      "Epoch: 00 [  924/20298 (  5%)], Train Loss: 0.60780\n",
      "Epoch: 00 [  964/20298 (  5%)], Train Loss: 0.59587\n",
      "Epoch: 00 [ 1004/20298 (  5%)], Train Loss: 0.58555\n",
      "Epoch: 00 [ 1044/20298 (  5%)], Train Loss: 0.57356\n",
      "Epoch: 00 [ 1084/20298 (  5%)], Train Loss: 0.56235\n",
      "Epoch: 00 [ 1124/20298 (  6%)], Train Loss: 0.55148\n",
      "Epoch: 00 [ 1164/20298 (  6%)], Train Loss: 0.53994\n",
      "Epoch: 00 [ 1204/20298 (  6%)], Train Loss: 0.52881\n",
      "Epoch: 00 [ 1244/20298 (  6%)], Train Loss: 0.51848\n",
      "Epoch: 00 [ 1284/20298 (  6%)], Train Loss: 0.50850\n",
      "Epoch: 00 [ 1324/20298 (  7%)], Train Loss: 0.49849\n",
      "Epoch: 00 [ 1364/20298 (  7%)], Train Loss: 0.48885\n",
      "Epoch: 00 [ 1404/20298 (  7%)], Train Loss: 0.48049\n",
      "Epoch: 00 [ 1444/20298 (  7%)], Train Loss: 0.47200\n",
      "Epoch: 00 [ 1484/20298 (  7%)], Train Loss: 0.46216\n",
      "Epoch: 00 [ 1524/20298 (  8%)], Train Loss: 0.45430\n",
      "Epoch: 00 [ 1564/20298 (  8%)], Train Loss: 0.44679\n",
      "Epoch: 00 [ 1604/20298 (  8%)], Train Loss: 0.43833\n",
      "Epoch: 00 [ 1644/20298 (  8%)], Train Loss: 0.43081\n",
      "Epoch: 00 [ 1684/20298 (  8%)], Train Loss: 0.42364\n",
      "Epoch: 00 [ 1724/20298 (  8%)], Train Loss: 0.41698\n",
      "Epoch: 00 [ 1764/20298 (  9%)], Train Loss: 0.40990\n",
      "Epoch: 00 [ 1804/20298 (  9%)], Train Loss: 0.40262\n",
      "Epoch: 00 [ 1844/20298 (  9%)], Train Loss: 0.39689\n",
      "Epoch: 00 [ 1884/20298 (  9%)], Train Loss: 0.39172\n",
      "Epoch: 00 [ 1924/20298 (  9%)], Train Loss: 0.38700\n",
      "Epoch: 00 [ 1964/20298 ( 10%)], Train Loss: 0.38109\n",
      "Epoch: 00 [ 2004/20298 ( 10%)], Train Loss: 0.37582\n",
      "Epoch: 00 [ 2044/20298 ( 10%)], Train Loss: 0.37037\n",
      "Epoch: 00 [ 2084/20298 ( 10%)], Train Loss: 0.36556\n",
      "Epoch: 00 [ 2124/20298 ( 10%)], Train Loss: 0.36152\n",
      "Epoch: 00 [ 2164/20298 ( 11%)], Train Loss: 0.35688\n",
      "Epoch: 00 [ 2204/20298 ( 11%)], Train Loss: 0.35165\n",
      "Epoch: 00 [ 2244/20298 ( 11%)], Train Loss: 0.34715\n",
      "Epoch: 00 [ 2284/20298 ( 11%)], Train Loss: 0.34270\n",
      "Epoch: 00 [ 2324/20298 ( 11%)], Train Loss: 0.33938\n",
      "Epoch: 00 [ 2364/20298 ( 12%)], Train Loss: 0.33522\n",
      "Epoch: 00 [ 2404/20298 ( 12%)], Train Loss: 0.33089\n",
      "Epoch: 00 [ 2444/20298 ( 12%)], Train Loss: 0.32762\n",
      "Epoch: 00 [ 2484/20298 ( 12%)], Train Loss: 0.32361\n",
      "Epoch: 00 [ 2524/20298 ( 12%)], Train Loss: 0.32062\n",
      "Epoch: 00 [ 2564/20298 ( 13%)], Train Loss: 0.31726\n",
      "Epoch: 00 [ 2604/20298 ( 13%)], Train Loss: 0.31418\n",
      "Epoch: 00 [ 2644/20298 ( 13%)], Train Loss: 0.31078\n",
      "Epoch: 00 [ 2684/20298 ( 13%)], Train Loss: 0.30736\n",
      "Epoch: 00 [ 2724/20298 ( 13%)], Train Loss: 0.30464\n",
      "Epoch: 00 [ 2764/20298 ( 14%)], Train Loss: 0.30135\n",
      "Epoch: 00 [ 2804/20298 ( 14%)], Train Loss: 0.29969\n",
      "Epoch: 00 [ 2844/20298 ( 14%)], Train Loss: 0.29693\n",
      "Epoch: 00 [ 2884/20298 ( 14%)], Train Loss: 0.29426\n",
      "Epoch: 00 [ 2924/20298 ( 14%)], Train Loss: 0.29136\n",
      "Epoch: 00 [ 2964/20298 ( 15%)], Train Loss: 0.28838\n",
      "Epoch: 00 [ 3004/20298 ( 15%)], Train Loss: 0.28553\n",
      "Epoch: 00 [ 3044/20298 ( 15%)], Train Loss: 0.28346\n",
      "Epoch: 00 [ 3084/20298 ( 15%)], Train Loss: 0.28093\n",
      "Epoch: 00 [ 3124/20298 ( 15%)], Train Loss: 0.27803\n",
      "Epoch: 00 [ 3164/20298 ( 16%)], Train Loss: 0.27561\n",
      "Epoch: 00 [ 3204/20298 ( 16%)], Train Loss: 0.27311\n",
      "Epoch: 00 [ 3244/20298 ( 16%)], Train Loss: 0.27080\n",
      "Epoch: 00 [ 3284/20298 ( 16%)], Train Loss: 0.26830\n",
      "Epoch: 00 [ 3324/20298 ( 16%)], Train Loss: 0.26635\n",
      "Epoch: 00 [ 3364/20298 ( 17%)], Train Loss: 0.26419\n",
      "Epoch: 00 [ 3404/20298 ( 17%)], Train Loss: 0.26356\n",
      "Epoch: 00 [ 3444/20298 ( 17%)], Train Loss: 0.26151\n",
      "Epoch: 00 [ 3484/20298 ( 17%)], Train Loss: 0.25973\n",
      "Epoch: 00 [ 3524/20298 ( 17%)], Train Loss: 0.25806\n",
      "Epoch: 00 [ 3564/20298 ( 18%)], Train Loss: 0.25635\n",
      "Epoch: 00 [ 3604/20298 ( 18%)], Train Loss: 0.25488\n",
      "Epoch: 00 [ 3644/20298 ( 18%)], Train Loss: 0.25331\n",
      "Epoch: 00 [ 3684/20298 ( 18%)], Train Loss: 0.25158\n",
      "Epoch: 00 [ 3724/20298 ( 18%)], Train Loss: 0.24994\n",
      "Epoch: 00 [ 3764/20298 ( 19%)], Train Loss: 0.24850\n",
      "Epoch: 00 [ 3804/20298 ( 19%)], Train Loss: 0.24660\n",
      "Epoch: 00 [ 3844/20298 ( 19%)], Train Loss: 0.24477\n",
      "Epoch: 00 [ 3884/20298 ( 19%)], Train Loss: 0.24364\n",
      "Epoch: 00 [ 3924/20298 ( 19%)], Train Loss: 0.24207\n",
      "Epoch: 00 [ 3964/20298 ( 20%)], Train Loss: 0.24055\n",
      "Epoch: 00 [ 4004/20298 ( 20%)], Train Loss: 0.23953\n",
      "Epoch: 00 [ 4044/20298 ( 20%)], Train Loss: 0.23844\n",
      "Epoch: 00 [ 4084/20298 ( 20%)], Train Loss: 0.23677\n",
      "Epoch: 00 [ 4124/20298 ( 20%)], Train Loss: 0.23524\n",
      "Epoch: 00 [ 4164/20298 ( 21%)], Train Loss: 0.23402\n",
      "Epoch: 00 [ 4204/20298 ( 21%)], Train Loss: 0.23311\n",
      "Epoch: 00 [ 4244/20298 ( 21%)], Train Loss: 0.23212\n",
      "Epoch: 00 [ 4284/20298 ( 21%)], Train Loss: 0.23095\n",
      "Epoch: 00 [ 4324/20298 ( 21%)], Train Loss: 0.22957\n",
      "Epoch: 00 [ 4364/20298 ( 21%)], Train Loss: 0.22895\n",
      "Epoch: 00 [ 4404/20298 ( 22%)], Train Loss: 0.22776\n",
      "Epoch: 00 [ 4444/20298 ( 22%)], Train Loss: 0.22679\n",
      "Epoch: 00 [ 4484/20298 ( 22%)], Train Loss: 0.22584\n",
      "Epoch: 00 [ 4524/20298 ( 22%)], Train Loss: 0.22477\n",
      "Epoch: 00 [ 4564/20298 ( 22%)], Train Loss: 0.22363\n",
      "Epoch: 00 [ 4604/20298 ( 23%)], Train Loss: 0.22265\n",
      "Epoch: 00 [ 4644/20298 ( 23%)], Train Loss: 0.22126\n",
      "Epoch: 00 [ 4684/20298 ( 23%)], Train Loss: 0.22016\n",
      "Epoch: 00 [ 4724/20298 ( 23%)], Train Loss: 0.21892\n",
      "Epoch: 00 [ 4764/20298 ( 23%)], Train Loss: 0.21764\n",
      "Epoch: 00 [ 4804/20298 ( 24%)], Train Loss: 0.21643\n",
      "Epoch: 00 [ 4844/20298 ( 24%)], Train Loss: 0.21591\n",
      "Epoch: 00 [ 4884/20298 ( 24%)], Train Loss: 0.21499\n",
      "Epoch: 00 [ 4924/20298 ( 24%)], Train Loss: 0.21390\n",
      "Epoch: 00 [ 4964/20298 ( 24%)], Train Loss: 0.21268\n",
      "Epoch: 00 [ 5004/20298 ( 25%)], Train Loss: 0.21159\n",
      "Epoch: 00 [ 5044/20298 ( 25%)], Train Loss: 0.21071\n",
      "Epoch: 00 [ 5084/20298 ( 25%)], Train Loss: 0.20978\n",
      "Epoch: 00 [ 5124/20298 ( 25%)], Train Loss: 0.20915\n",
      "Epoch: 00 [ 5164/20298 ( 25%)], Train Loss: 0.20875\n",
      "Epoch: 00 [ 5204/20298 ( 26%)], Train Loss: 0.20786\n",
      "Epoch: 00 [ 5244/20298 ( 26%)], Train Loss: 0.20687\n",
      "Epoch: 00 [ 5284/20298 ( 26%)], Train Loss: 0.20576\n",
      "Epoch: 00 [ 5324/20298 ( 26%)], Train Loss: 0.20515\n",
      "Epoch: 00 [ 5364/20298 ( 26%)], Train Loss: 0.20420\n",
      "Epoch: 00 [ 5404/20298 ( 27%)], Train Loss: 0.20327\n",
      "Epoch: 00 [ 5444/20298 ( 27%)], Train Loss: 0.20244\n",
      "Epoch: 00 [ 5484/20298 ( 27%)], Train Loss: 0.20138\n",
      "Epoch: 00 [ 5524/20298 ( 27%)], Train Loss: 0.20062\n",
      "Epoch: 00 [ 5564/20298 ( 27%)], Train Loss: 0.19987\n",
      "Epoch: 00 [ 5604/20298 ( 28%)], Train Loss: 0.19928\n",
      "Epoch: 00 [ 5644/20298 ( 28%)], Train Loss: 0.19889\n",
      "Epoch: 00 [ 5684/20298 ( 28%)], Train Loss: 0.19796\n",
      "Epoch: 00 [ 5724/20298 ( 28%)], Train Loss: 0.19730\n",
      "Epoch: 00 [ 5764/20298 ( 28%)], Train Loss: 0.19659\n",
      "Epoch: 00 [ 5804/20298 ( 29%)], Train Loss: 0.19557\n",
      "Epoch: 00 [ 5844/20298 ( 29%)], Train Loss: 0.19511\n",
      "Epoch: 00 [ 5884/20298 ( 29%)], Train Loss: 0.19458\n",
      "Epoch: 00 [ 5924/20298 ( 29%)], Train Loss: 0.19398\n",
      "Epoch: 00 [ 5964/20298 ( 29%)], Train Loss: 0.19322\n",
      "Epoch: 00 [ 6004/20298 ( 30%)], Train Loss: 0.19271\n",
      "Epoch: 00 [ 6044/20298 ( 30%)], Train Loss: 0.19199\n",
      "Epoch: 00 [ 6084/20298 ( 30%)], Train Loss: 0.19122\n",
      "Epoch: 00 [ 6124/20298 ( 30%)], Train Loss: 0.19070\n",
      "Epoch: 00 [ 6164/20298 ( 30%)], Train Loss: 0.19009\n",
      "Epoch: 00 [ 6204/20298 ( 31%)], Train Loss: 0.19005\n",
      "Epoch: 00 [ 6244/20298 ( 31%)], Train Loss: 0.18931\n",
      "Epoch: 00 [ 6284/20298 ( 31%)], Train Loss: 0.18879\n",
      "Epoch: 00 [ 6324/20298 ( 31%)], Train Loss: 0.18801\n",
      "Epoch: 00 [ 6364/20298 ( 31%)], Train Loss: 0.18756\n",
      "Epoch: 00 [ 6404/20298 ( 32%)], Train Loss: 0.18696\n",
      "Epoch: 00 [ 6444/20298 ( 32%)], Train Loss: 0.18638\n",
      "Epoch: 00 [ 6484/20298 ( 32%)], Train Loss: 0.18566\n",
      "Epoch: 00 [ 6524/20298 ( 32%)], Train Loss: 0.18509\n",
      "Epoch: 00 [ 6564/20298 ( 32%)], Train Loss: 0.18461\n",
      "Epoch: 00 [ 6604/20298 ( 33%)], Train Loss: 0.18407\n",
      "Epoch: 00 [ 6644/20298 ( 33%)], Train Loss: 0.18380\n",
      "Epoch: 00 [ 6684/20298 ( 33%)], Train Loss: 0.18310\n",
      "Epoch: 00 [ 6724/20298 ( 33%)], Train Loss: 0.18253\n",
      "Epoch: 00 [ 6764/20298 ( 33%)], Train Loss: 0.18206\n",
      "Epoch: 00 [ 6804/20298 ( 34%)], Train Loss: 0.18149\n",
      "Epoch: 00 [ 6844/20298 ( 34%)], Train Loss: 0.18095\n",
      "Epoch: 00 [ 6884/20298 ( 34%)], Train Loss: 0.18044\n",
      "Epoch: 00 [ 6924/20298 ( 34%)], Train Loss: 0.17990\n",
      "Epoch: 00 [ 6964/20298 ( 34%)], Train Loss: 0.17952\n",
      "Epoch: 00 [ 7004/20298 ( 35%)], Train Loss: 0.17908\n",
      "Epoch: 00 [ 7044/20298 ( 35%)], Train Loss: 0.17837\n",
      "Epoch: 00 [ 7084/20298 ( 35%)], Train Loss: 0.17786\n",
      "Epoch: 00 [ 7124/20298 ( 35%)], Train Loss: 0.17723\n",
      "Epoch: 00 [ 7164/20298 ( 35%)], Train Loss: 0.17673\n",
      "Epoch: 00 [ 7204/20298 ( 35%)], Train Loss: 0.17610\n",
      "Epoch: 00 [ 7244/20298 ( 36%)], Train Loss: 0.17563\n",
      "Epoch: 00 [ 7284/20298 ( 36%)], Train Loss: 0.17509\n",
      "Epoch: 00 [ 7324/20298 ( 36%)], Train Loss: 0.17464\n",
      "Epoch: 00 [ 7364/20298 ( 36%)], Train Loss: 0.17433\n",
      "Epoch: 00 [ 7404/20298 ( 36%)], Train Loss: 0.17400\n",
      "Epoch: 00 [ 7444/20298 ( 37%)], Train Loss: 0.17370\n",
      "Epoch: 00 [ 7484/20298 ( 37%)], Train Loss: 0.17304\n",
      "Epoch: 00 [ 7524/20298 ( 37%)], Train Loss: 0.17261\n",
      "Epoch: 00 [ 7564/20298 ( 37%)], Train Loss: 0.17234\n",
      "Epoch: 00 [ 7604/20298 ( 37%)], Train Loss: 0.17178\n",
      "Epoch: 00 [ 7644/20298 ( 38%)], Train Loss: 0.17127\n",
      "Epoch: 00 [ 7684/20298 ( 38%)], Train Loss: 0.17079\n",
      "Epoch: 00 [ 7724/20298 ( 38%)], Train Loss: 0.17033\n",
      "Epoch: 00 [ 7764/20298 ( 38%)], Train Loss: 0.17013\n",
      "Epoch: 00 [ 7804/20298 ( 38%)], Train Loss: 0.16964\n",
      "Epoch: 00 [ 7844/20298 ( 39%)], Train Loss: 0.16936\n",
      "Epoch: 00 [ 7884/20298 ( 39%)], Train Loss: 0.16891\n",
      "Epoch: 00 [ 7924/20298 ( 39%)], Train Loss: 0.16845\n",
      "Epoch: 00 [ 7964/20298 ( 39%)], Train Loss: 0.16807\n",
      "Epoch: 00 [ 8004/20298 ( 39%)], Train Loss: 0.16757\n",
      "Epoch: 00 [ 8044/20298 ( 40%)], Train Loss: 0.16706\n",
      "Epoch: 00 [ 8084/20298 ( 40%)], Train Loss: 0.16676\n",
      "Epoch: 00 [ 8124/20298 ( 40%)], Train Loss: 0.16656\n",
      "Epoch: 00 [ 8164/20298 ( 40%)], Train Loss: 0.16609\n",
      "Epoch: 00 [ 8204/20298 ( 40%)], Train Loss: 0.16573\n",
      "Epoch: 00 [ 8244/20298 ( 41%)], Train Loss: 0.16521\n",
      "Epoch: 00 [ 8284/20298 ( 41%)], Train Loss: 0.16480\n",
      "Epoch: 00 [ 8324/20298 ( 41%)], Train Loss: 0.16458\n",
      "Epoch: 00 [ 8364/20298 ( 41%)], Train Loss: 0.16413\n",
      "Epoch: 00 [ 8404/20298 ( 41%)], Train Loss: 0.16383\n",
      "Epoch: 00 [ 8444/20298 ( 42%)], Train Loss: 0.16345\n",
      "Epoch: 00 [ 8484/20298 ( 42%)], Train Loss: 0.16306\n",
      "Epoch: 00 [ 8524/20298 ( 42%)], Train Loss: 0.16274\n",
      "Epoch: 00 [ 8564/20298 ( 42%)], Train Loss: 0.16230\n",
      "Epoch: 00 [ 8604/20298 ( 42%)], Train Loss: 0.16197\n",
      "Epoch: 00 [ 8644/20298 ( 43%)], Train Loss: 0.16155\n",
      "Epoch: 00 [ 8684/20298 ( 43%)], Train Loss: 0.16126\n",
      "Epoch: 00 [ 8724/20298 ( 43%)], Train Loss: 0.16104\n",
      "Epoch: 00 [ 8764/20298 ( 43%)], Train Loss: 0.16091\n",
      "Epoch: 00 [ 8804/20298 ( 43%)], Train Loss: 0.16071\n",
      "Epoch: 00 [ 8844/20298 ( 44%)], Train Loss: 0.16038\n",
      "Epoch: 00 [ 8884/20298 ( 44%)], Train Loss: 0.15999\n",
      "Epoch: 00 [ 8924/20298 ( 44%)], Train Loss: 0.15959\n",
      "Epoch: 00 [ 8964/20298 ( 44%)], Train Loss: 0.15930\n",
      "Epoch: 00 [ 9004/20298 ( 44%)], Train Loss: 0.15903\n",
      "Epoch: 00 [ 9044/20298 ( 45%)], Train Loss: 0.15882\n",
      "Epoch: 00 [ 9084/20298 ( 45%)], Train Loss: 0.15858\n",
      "Epoch: 00 [ 9124/20298 ( 45%)], Train Loss: 0.15833\n",
      "Epoch: 00 [ 9164/20298 ( 45%)], Train Loss: 0.15814\n",
      "Epoch: 00 [ 9204/20298 ( 45%)], Train Loss: 0.15767\n",
      "Epoch: 00 [ 9244/20298 ( 46%)], Train Loss: 0.15725\n",
      "Epoch: 00 [ 9284/20298 ( 46%)], Train Loss: 0.15695\n",
      "Epoch: 00 [ 9324/20298 ( 46%)], Train Loss: 0.15663\n",
      "Epoch: 00 [ 9364/20298 ( 46%)], Train Loss: 0.15637\n",
      "Epoch: 00 [ 9404/20298 ( 46%)], Train Loss: 0.15592\n",
      "Epoch: 00 [ 9444/20298 ( 47%)], Train Loss: 0.15557\n",
      "Epoch: 00 [ 9484/20298 ( 47%)], Train Loss: 0.15534\n",
      "Epoch: 00 [ 9524/20298 ( 47%)], Train Loss: 0.15528\n",
      "Epoch: 00 [ 9564/20298 ( 47%)], Train Loss: 0.15497\n",
      "Epoch: 00 [ 9604/20298 ( 47%)], Train Loss: 0.15479\n",
      "Epoch: 00 [ 9644/20298 ( 48%)], Train Loss: 0.15472\n",
      "Epoch: 00 [ 9684/20298 ( 48%)], Train Loss: 0.15448\n",
      "Epoch: 00 [ 9724/20298 ( 48%)], Train Loss: 0.15418\n",
      "Epoch: 00 [ 9764/20298 ( 48%)], Train Loss: 0.15392\n",
      "Epoch: 00 [ 9804/20298 ( 48%)], Train Loss: 0.15372\n",
      "Epoch: 00 [ 9844/20298 ( 48%)], Train Loss: 0.15347\n",
      "Epoch: 00 [ 9884/20298 ( 49%)], Train Loss: 0.15323\n",
      "Epoch: 00 [ 9924/20298 ( 49%)], Train Loss: 0.15293\n",
      "Epoch: 00 [ 9964/20298 ( 49%)], Train Loss: 0.15276\n",
      "Epoch: 00 [10004/20298 ( 49%)], Train Loss: 0.15260\n",
      "Epoch: 00 [10044/20298 ( 49%)], Train Loss: 0.15209\n",
      "Epoch: 00 [10084/20298 ( 50%)], Train Loss: 0.15174\n",
      "Epoch: 00 [10124/20298 ( 50%)], Train Loss: 0.15136\n",
      "Epoch: 00 [10164/20298 ( 50%)], Train Loss: 0.15109\n",
      "Epoch: 00 [10204/20298 ( 50%)], Train Loss: 0.15090\n",
      "Epoch: 00 [10244/20298 ( 50%)], Train Loss: 0.15076\n",
      "Epoch: 00 [10284/20298 ( 51%)], Train Loss: 0.15050\n",
      "Epoch: 00 [10324/20298 ( 51%)], Train Loss: 0.15038\n",
      "Epoch: 00 [10364/20298 ( 51%)], Train Loss: 0.15008\n",
      "Epoch: 00 [10404/20298 ( 51%)], Train Loss: 0.14983\n",
      "Epoch: 00 [10444/20298 ( 51%)], Train Loss: 0.14960\n",
      "Epoch: 00 [10484/20298 ( 52%)], Train Loss: 0.14926\n",
      "Epoch: 00 [10524/20298 ( 52%)], Train Loss: 0.14905\n",
      "Epoch: 00 [10564/20298 ( 52%)], Train Loss: 0.14885\n",
      "Epoch: 00 [10604/20298 ( 52%)], Train Loss: 0.14859\n",
      "Epoch: 00 [10644/20298 ( 52%)], Train Loss: 0.14826\n",
      "Epoch: 00 [10684/20298 ( 53%)], Train Loss: 0.14801\n",
      "Epoch: 00 [10724/20298 ( 53%)], Train Loss: 0.14781\n",
      "Epoch: 00 [10764/20298 ( 53%)], Train Loss: 0.14759\n",
      "Epoch: 00 [10804/20298 ( 53%)], Train Loss: 0.14753\n",
      "Epoch: 00 [10844/20298 ( 53%)], Train Loss: 0.14721\n",
      "Epoch: 00 [10884/20298 ( 54%)], Train Loss: 0.14676\n",
      "Epoch: 00 [10924/20298 ( 54%)], Train Loss: 0.14661\n",
      "Epoch: 00 [10964/20298 ( 54%)], Train Loss: 0.14645\n",
      "Epoch: 00 [11004/20298 ( 54%)], Train Loss: 0.14656\n",
      "Epoch: 00 [11044/20298 ( 54%)], Train Loss: 0.14627\n",
      "Epoch: 00 [11084/20298 ( 55%)], Train Loss: 0.14603\n",
      "Epoch: 00 [11124/20298 ( 55%)], Train Loss: 0.14574\n",
      "Epoch: 00 [11164/20298 ( 55%)], Train Loss: 0.14556\n",
      "Epoch: 00 [11204/20298 ( 55%)], Train Loss: 0.14546\n",
      "Epoch: 00 [11244/20298 ( 55%)], Train Loss: 0.14536\n",
      "Epoch: 00 [11284/20298 ( 56%)], Train Loss: 0.14519\n",
      "Epoch: 00 [11324/20298 ( 56%)], Train Loss: 0.14489\n",
      "Epoch: 00 [11364/20298 ( 56%)], Train Loss: 0.14472\n",
      "Epoch: 00 [11404/20298 ( 56%)], Train Loss: 0.14458\n",
      "Epoch: 00 [11444/20298 ( 56%)], Train Loss: 0.14447\n",
      "Epoch: 00 [11484/20298 ( 57%)], Train Loss: 0.14412\n",
      "Epoch: 00 [11524/20298 ( 57%)], Train Loss: 0.14390\n",
      "Epoch: 00 [11564/20298 ( 57%)], Train Loss: 0.14362\n",
      "Epoch: 00 [11604/20298 ( 57%)], Train Loss: 0.14333\n",
      "Epoch: 00 [11644/20298 ( 57%)], Train Loss: 0.14310\n",
      "Epoch: 00 [11684/20298 ( 58%)], Train Loss: 0.14295\n",
      "Epoch: 00 [11724/20298 ( 58%)], Train Loss: 0.14274\n",
      "Epoch: 00 [11764/20298 ( 58%)], Train Loss: 0.14253\n",
      "Epoch: 00 [11804/20298 ( 58%)], Train Loss: 0.14212\n",
      "Epoch: 00 [11844/20298 ( 58%)], Train Loss: 0.14206\n",
      "Epoch: 00 [11884/20298 ( 59%)], Train Loss: 0.14189\n",
      "Epoch: 00 [11924/20298 ( 59%)], Train Loss: 0.14184\n",
      "Epoch: 00 [11964/20298 ( 59%)], Train Loss: 0.14176\n",
      "Epoch: 00 [12004/20298 ( 59%)], Train Loss: 0.14155\n",
      "Epoch: 00 [12044/20298 ( 59%)], Train Loss: 0.14152\n",
      "Epoch: 00 [12084/20298 ( 60%)], Train Loss: 0.14136\n",
      "Epoch: 00 [12124/20298 ( 60%)], Train Loss: 0.14114\n",
      "Epoch: 00 [12164/20298 ( 60%)], Train Loss: 0.14093\n",
      "Epoch: 00 [12204/20298 ( 60%)], Train Loss: 0.14095\n",
      "Epoch: 00 [12244/20298 ( 60%)], Train Loss: 0.14067\n",
      "Epoch: 00 [12284/20298 ( 61%)], Train Loss: 0.14043\n",
      "Epoch: 00 [12324/20298 ( 61%)], Train Loss: 0.14019\n",
      "Epoch: 00 [12364/20298 ( 61%)], Train Loss: 0.14007\n",
      "Epoch: 00 [12404/20298 ( 61%)], Train Loss: 0.13997\n",
      "Epoch: 00 [12444/20298 ( 61%)], Train Loss: 0.13982\n",
      "Epoch: 00 [12484/20298 ( 62%)], Train Loss: 0.13984\n",
      "Epoch: 00 [12524/20298 ( 62%)], Train Loss: 0.13957\n",
      "Epoch: 00 [12564/20298 ( 62%)], Train Loss: 0.13927\n",
      "Epoch: 00 [12604/20298 ( 62%)], Train Loss: 0.13923\n",
      "Epoch: 00 [12644/20298 ( 62%)], Train Loss: 0.13915\n",
      "Epoch: 00 [12684/20298 ( 62%)], Train Loss: 0.13918\n",
      "Epoch: 00 [12724/20298 ( 63%)], Train Loss: 0.13900\n",
      "Epoch: 00 [12764/20298 ( 63%)], Train Loss: 0.13879\n",
      "Epoch: 00 [12804/20298 ( 63%)], Train Loss: 0.13853\n",
      "Epoch: 00 [12844/20298 ( 63%)], Train Loss: 0.13831\n",
      "Epoch: 00 [12884/20298 ( 63%)], Train Loss: 0.13817\n",
      "Epoch: 00 [12924/20298 ( 64%)], Train Loss: 0.13803\n",
      "Epoch: 00 [12964/20298 ( 64%)], Train Loss: 0.13788\n",
      "Epoch: 00 [13004/20298 ( 64%)], Train Loss: 0.13774\n",
      "Epoch: 00 [13044/20298 ( 64%)], Train Loss: 0.13754\n",
      "Epoch: 00 [13084/20298 ( 64%)], Train Loss: 0.13735\n",
      "Epoch: 00 [13124/20298 ( 65%)], Train Loss: 0.13710\n",
      "Epoch: 00 [13164/20298 ( 65%)], Train Loss: 0.13709\n",
      "Epoch: 00 [13204/20298 ( 65%)], Train Loss: 0.13710\n",
      "Epoch: 00 [13244/20298 ( 65%)], Train Loss: 0.13707\n",
      "Epoch: 00 [13284/20298 ( 65%)], Train Loss: 0.13689\n",
      "Epoch: 00 [13324/20298 ( 66%)], Train Loss: 0.13698\n",
      "Epoch: 00 [13364/20298 ( 66%)], Train Loss: 0.13691\n",
      "Epoch: 00 [13404/20298 ( 66%)], Train Loss: 0.13669\n",
      "Epoch: 00 [13444/20298 ( 66%)], Train Loss: 0.13665\n",
      "Epoch: 00 [13484/20298 ( 66%)], Train Loss: 0.13645\n",
      "Epoch: 00 [13524/20298 ( 67%)], Train Loss: 0.13635\n",
      "Epoch: 00 [13564/20298 ( 67%)], Train Loss: 0.13608\n",
      "Epoch: 00 [13604/20298 ( 67%)], Train Loss: 0.13594\n",
      "Epoch: 00 [13644/20298 ( 67%)], Train Loss: 0.13577\n",
      "Epoch: 00 [13684/20298 ( 67%)], Train Loss: 0.13557\n",
      "Epoch: 00 [13724/20298 ( 68%)], Train Loss: 0.13548\n",
      "Epoch: 00 [13764/20298 ( 68%)], Train Loss: 0.13537\n",
      "Epoch: 00 [13804/20298 ( 68%)], Train Loss: 0.13532\n",
      "Epoch: 00 [13844/20298 ( 68%)], Train Loss: 0.13524\n",
      "Epoch: 00 [13884/20298 ( 68%)], Train Loss: 0.13505\n",
      "Epoch: 00 [13924/20298 ( 69%)], Train Loss: 0.13484\n",
      "Epoch: 00 [13964/20298 ( 69%)], Train Loss: 0.13472\n",
      "Epoch: 00 [14004/20298 ( 69%)], Train Loss: 0.13442\n",
      "Epoch: 00 [14044/20298 ( 69%)], Train Loss: 0.13417\n",
      "Epoch: 00 [14084/20298 ( 69%)], Train Loss: 0.13414\n",
      "Epoch: 00 [14124/20298 ( 70%)], Train Loss: 0.13407\n",
      "Epoch: 00 [14164/20298 ( 70%)], Train Loss: 0.13391\n",
      "Epoch: 00 [14204/20298 ( 70%)], Train Loss: 0.13367\n",
      "Epoch: 00 [14244/20298 ( 70%)], Train Loss: 0.13361\n",
      "Epoch: 00 [14284/20298 ( 70%)], Train Loss: 0.13338\n",
      "Epoch: 00 [14324/20298 ( 71%)], Train Loss: 0.13329\n",
      "Epoch: 00 [14364/20298 ( 71%)], Train Loss: 0.13323\n",
      "Epoch: 00 [14404/20298 ( 71%)], Train Loss: 0.13297\n",
      "Epoch: 00 [14444/20298 ( 71%)], Train Loss: 0.13277\n",
      "Epoch: 00 [14484/20298 ( 71%)], Train Loss: 0.13268\n",
      "Epoch: 00 [14524/20298 ( 72%)], Train Loss: 0.13250\n",
      "Epoch: 00 [14564/20298 ( 72%)], Train Loss: 0.13228\n",
      "Epoch: 00 [14604/20298 ( 72%)], Train Loss: 0.13219\n",
      "Epoch: 00 [14644/20298 ( 72%)], Train Loss: 0.13206\n",
      "Epoch: 00 [14684/20298 ( 72%)], Train Loss: 0.13196\n",
      "Epoch: 00 [14724/20298 ( 73%)], Train Loss: 0.13180\n",
      "Epoch: 00 [14764/20298 ( 73%)], Train Loss: 0.13166\n",
      "Epoch: 00 [14804/20298 ( 73%)], Train Loss: 0.13167\n",
      "Epoch: 00 [14844/20298 ( 73%)], Train Loss: 0.13147\n",
      "Epoch: 00 [14884/20298 ( 73%)], Train Loss: 0.13141\n",
      "Epoch: 00 [14924/20298 ( 74%)], Train Loss: 0.13139\n",
      "Epoch: 00 [14964/20298 ( 74%)], Train Loss: 0.13125\n",
      "Epoch: 00 [15004/20298 ( 74%)], Train Loss: 0.13108\n",
      "Epoch: 00 [15044/20298 ( 74%)], Train Loss: 0.13095\n",
      "Epoch: 00 [15084/20298 ( 74%)], Train Loss: 0.13087\n",
      "Epoch: 00 [15124/20298 ( 75%)], Train Loss: 0.13073\n",
      "Epoch: 00 [15164/20298 ( 75%)], Train Loss: 0.13060\n",
      "Epoch: 00 [15204/20298 ( 75%)], Train Loss: 0.13047\n",
      "Epoch: 00 [15244/20298 ( 75%)], Train Loss: 0.13033\n",
      "Epoch: 00 [15284/20298 ( 75%)], Train Loss: 0.13018\n",
      "Epoch: 00 [15324/20298 ( 75%)], Train Loss: 0.13015\n",
      "Epoch: 00 [15364/20298 ( 76%)], Train Loss: 0.13003\n",
      "Epoch: 00 [15404/20298 ( 76%)], Train Loss: 0.12990\n",
      "Epoch: 00 [15444/20298 ( 76%)], Train Loss: 0.12969\n",
      "Epoch: 00 [15484/20298 ( 76%)], Train Loss: 0.12942\n",
      "Epoch: 00 [15524/20298 ( 76%)], Train Loss: 0.12929\n",
      "Epoch: 00 [15564/20298 ( 77%)], Train Loss: 0.12928\n",
      "Epoch: 00 [15604/20298 ( 77%)], Train Loss: 0.12907\n",
      "Epoch: 00 [15644/20298 ( 77%)], Train Loss: 0.12883\n",
      "Epoch: 00 [15684/20298 ( 77%)], Train Loss: 0.12878\n",
      "Epoch: 00 [15724/20298 ( 77%)], Train Loss: 0.12868\n",
      "Epoch: 00 [15764/20298 ( 78%)], Train Loss: 0.12852\n",
      "Epoch: 00 [15804/20298 ( 78%)], Train Loss: 0.12844\n",
      "Epoch: 00 [15844/20298 ( 78%)], Train Loss: 0.12836\n",
      "Epoch: 00 [15884/20298 ( 78%)], Train Loss: 0.12834\n",
      "Epoch: 00 [15924/20298 ( 78%)], Train Loss: 0.12823\n",
      "Epoch: 00 [15964/20298 ( 79%)], Train Loss: 0.12809\n",
      "Epoch: 00 [16004/20298 ( 79%)], Train Loss: 0.12797\n",
      "Epoch: 00 [16044/20298 ( 79%)], Train Loss: 0.12800\n",
      "Epoch: 00 [16084/20298 ( 79%)], Train Loss: 0.12784\n",
      "Epoch: 00 [16124/20298 ( 79%)], Train Loss: 0.12774\n",
      "Epoch: 00 [16164/20298 ( 80%)], Train Loss: 0.12764\n",
      "Epoch: 00 [16204/20298 ( 80%)], Train Loss: 0.12753\n",
      "Epoch: 00 [16244/20298 ( 80%)], Train Loss: 0.12745\n",
      "Epoch: 00 [16284/20298 ( 80%)], Train Loss: 0.12735\n",
      "Epoch: 00 [16324/20298 ( 80%)], Train Loss: 0.12722\n",
      "Epoch: 00 [16364/20298 ( 81%)], Train Loss: 0.12711\n",
      "Epoch: 00 [16404/20298 ( 81%)], Train Loss: 0.12690\n",
      "Epoch: 00 [16444/20298 ( 81%)], Train Loss: 0.12682\n",
      "Epoch: 00 [16484/20298 ( 81%)], Train Loss: 0.12661\n",
      "Epoch: 00 [16524/20298 ( 81%)], Train Loss: 0.12645\n",
      "Epoch: 00 [16564/20298 ( 82%)], Train Loss: 0.12635\n",
      "Epoch: 00 [16604/20298 ( 82%)], Train Loss: 0.12621\n",
      "Epoch: 00 [16644/20298 ( 82%)], Train Loss: 0.12604\n",
      "Epoch: 00 [16684/20298 ( 82%)], Train Loss: 0.12597\n",
      "Epoch: 00 [16724/20298 ( 82%)], Train Loss: 0.12595\n",
      "Epoch: 00 [16764/20298 ( 83%)], Train Loss: 0.12582\n",
      "Epoch: 00 [16804/20298 ( 83%)], Train Loss: 0.12570\n",
      "Epoch: 00 [16844/20298 ( 83%)], Train Loss: 0.12569\n",
      "Epoch: 00 [16884/20298 ( 83%)], Train Loss: 0.12554\n",
      "Epoch: 00 [16924/20298 ( 83%)], Train Loss: 0.12539\n",
      "Epoch: 00 [16964/20298 ( 84%)], Train Loss: 0.12536\n",
      "Epoch: 00 [17004/20298 ( 84%)], Train Loss: 0.12525\n",
      "Epoch: 00 [17044/20298 ( 84%)], Train Loss: 0.12526\n",
      "Epoch: 00 [17084/20298 ( 84%)], Train Loss: 0.12516\n",
      "Epoch: 00 [17124/20298 ( 84%)], Train Loss: 0.12512\n",
      "Epoch: 00 [17164/20298 ( 85%)], Train Loss: 0.12503\n",
      "Epoch: 00 [17204/20298 ( 85%)], Train Loss: 0.12488\n",
      "Epoch: 00 [17244/20298 ( 85%)], Train Loss: 0.12477\n",
      "Epoch: 00 [17284/20298 ( 85%)], Train Loss: 0.12471\n",
      "Epoch: 00 [17324/20298 ( 85%)], Train Loss: 0.12462\n",
      "Epoch: 00 [17364/20298 ( 86%)], Train Loss: 0.12452\n",
      "Epoch: 00 [17404/20298 ( 86%)], Train Loss: 0.12434\n",
      "Epoch: 00 [17444/20298 ( 86%)], Train Loss: 0.12422\n",
      "Epoch: 00 [17484/20298 ( 86%)], Train Loss: 0.12420\n",
      "Epoch: 00 [17524/20298 ( 86%)], Train Loss: 0.12411\n",
      "Epoch: 00 [17564/20298 ( 87%)], Train Loss: 0.12397\n",
      "Epoch: 00 [17604/20298 ( 87%)], Train Loss: 0.12391\n",
      "Epoch: 00 [17644/20298 ( 87%)], Train Loss: 0.12387\n",
      "Epoch: 00 [17684/20298 ( 87%)], Train Loss: 0.12368\n",
      "Epoch: 00 [17724/20298 ( 87%)], Train Loss: 0.12348\n",
      "Epoch: 00 [17764/20298 ( 88%)], Train Loss: 0.12331\n",
      "Epoch: 00 [17804/20298 ( 88%)], Train Loss: 0.12323\n",
      "Epoch: 00 [17844/20298 ( 88%)], Train Loss: 0.12311\n",
      "Epoch: 00 [17884/20298 ( 88%)], Train Loss: 0.12296\n",
      "Epoch: 00 [17924/20298 ( 88%)], Train Loss: 0.12279\n",
      "Epoch: 00 [17964/20298 ( 89%)], Train Loss: 0.12277\n",
      "Epoch: 00 [18004/20298 ( 89%)], Train Loss: 0.12264\n",
      "Epoch: 00 [18044/20298 ( 89%)], Train Loss: 0.12246\n",
      "Epoch: 00 [18084/20298 ( 89%)], Train Loss: 0.12234\n",
      "Epoch: 00 [18124/20298 ( 89%)], Train Loss: 0.12220\n",
      "Epoch: 00 [18164/20298 ( 89%)], Train Loss: 0.12218\n",
      "Epoch: 00 [18204/20298 ( 90%)], Train Loss: 0.12217\n",
      "Epoch: 00 [18244/20298 ( 90%)], Train Loss: 0.12205\n",
      "Epoch: 00 [18284/20298 ( 90%)], Train Loss: 0.12204\n",
      "Epoch: 00 [18324/20298 ( 90%)], Train Loss: 0.12203\n",
      "Epoch: 00 [18364/20298 ( 90%)], Train Loss: 0.12199\n",
      "Epoch: 00 [18404/20298 ( 91%)], Train Loss: 0.12184\n",
      "Epoch: 00 [18444/20298 ( 91%)], Train Loss: 0.12176\n",
      "Epoch: 00 [18484/20298 ( 91%)], Train Loss: 0.12169\n",
      "Epoch: 00 [18524/20298 ( 91%)], Train Loss: 0.12157\n",
      "Epoch: 00 [18564/20298 ( 91%)], Train Loss: 0.12147\n",
      "Epoch: 00 [18604/20298 ( 92%)], Train Loss: 0.12142\n",
      "Epoch: 00 [18644/20298 ( 92%)], Train Loss: 0.12137\n",
      "Epoch: 00 [18684/20298 ( 92%)], Train Loss: 0.12134\n",
      "Epoch: 00 [18724/20298 ( 92%)], Train Loss: 0.12126\n",
      "Epoch: 00 [18764/20298 ( 92%)], Train Loss: 0.12123\n",
      "Epoch: 00 [18804/20298 ( 93%)], Train Loss: 0.12113\n",
      "Epoch: 00 [18844/20298 ( 93%)], Train Loss: 0.12106\n",
      "Epoch: 00 [18884/20298 ( 93%)], Train Loss: 0.12099\n",
      "Epoch: 00 [18924/20298 ( 93%)], Train Loss: 0.12087\n",
      "Epoch: 00 [18964/20298 ( 93%)], Train Loss: 0.12088\n",
      "Epoch: 00 [19004/20298 ( 94%)], Train Loss: 0.12080\n",
      "Epoch: 00 [19044/20298 ( 94%)], Train Loss: 0.12075\n",
      "Epoch: 00 [19084/20298 ( 94%)], Train Loss: 0.12073\n",
      "Epoch: 00 [19124/20298 ( 94%)], Train Loss: 0.12061\n",
      "Epoch: 00 [19164/20298 ( 94%)], Train Loss: 0.12056\n",
      "Epoch: 00 [19204/20298 ( 95%)], Train Loss: 0.12052\n",
      "Epoch: 00 [19244/20298 ( 95%)], Train Loss: 0.12038\n",
      "Epoch: 00 [19284/20298 ( 95%)], Train Loss: 0.12035\n",
      "Epoch: 00 [19324/20298 ( 95%)], Train Loss: 0.12025\n",
      "Epoch: 00 [19364/20298 ( 95%)], Train Loss: 0.12010\n",
      "Epoch: 00 [19404/20298 ( 96%)], Train Loss: 0.12012\n",
      "Epoch: 00 [19444/20298 ( 96%)], Train Loss: 0.12005\n",
      "Epoch: 00 [19484/20298 ( 96%)], Train Loss: 0.11998\n",
      "Epoch: 00 [19524/20298 ( 96%)], Train Loss: 0.11995\n",
      "Epoch: 00 [19564/20298 ( 96%)], Train Loss: 0.11987\n",
      "Epoch: 00 [19604/20298 ( 97%)], Train Loss: 0.11976\n",
      "Epoch: 00 [19644/20298 ( 97%)], Train Loss: 0.11973\n",
      "Epoch: 00 [19684/20298 ( 97%)], Train Loss: 0.11963\n",
      "Epoch: 00 [19724/20298 ( 97%)], Train Loss: 0.11954\n",
      "Epoch: 00 [19764/20298 ( 97%)], Train Loss: 0.11941\n",
      "Epoch: 00 [19804/20298 ( 98%)], Train Loss: 0.11934\n",
      "Epoch: 00 [19844/20298 ( 98%)], Train Loss: 0.11924\n",
      "Epoch: 00 [19884/20298 ( 98%)], Train Loss: 0.11922\n",
      "Epoch: 00 [19924/20298 ( 98%)], Train Loss: 0.11909\n",
      "Epoch: 00 [19964/20298 ( 98%)], Train Loss: 0.11902\n",
      "Epoch: 00 [20004/20298 ( 99%)], Train Loss: 0.11886\n",
      "Epoch: 00 [20044/20298 ( 99%)], Train Loss: 0.11889\n",
      "Epoch: 00 [20084/20298 ( 99%)], Train Loss: 0.11887\n",
      "Epoch: 00 [20124/20298 ( 99%)], Train Loss: 0.11872\n",
      "Epoch: 00 [20164/20298 ( 99%)], Train Loss: 0.11879\n",
      "Epoch: 00 [20204/20298 (100%)], Train Loss: 0.11867\n",
      "Epoch: 00 [20244/20298 (100%)], Train Loss: 0.11855\n",
      "Epoch: 00 [20284/20298 (100%)], Train Loss: 0.11848\n",
      "Epoch: 00 [20298/20298 (100%)], Train Loss: 0.11843\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.25122\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.25122\n",
      "Saving model checkpoint to output/checkpoint-fold-1.\n",
      "\n",
      "Epoch: 01 [    4/20298 (  0%)], Train Loss: 0.10670\n",
      "Epoch: 01 [   44/20298 (  0%)], Train Loss: 0.10144\n",
      "Epoch: 01 [   84/20298 (  0%)], Train Loss: 0.10484\n",
      "Epoch: 01 [  124/20298 (  1%)], Train Loss: 0.09956\n",
      "Epoch: 01 [  164/20298 (  1%)], Train Loss: 0.08370\n",
      "Epoch: 01 [  204/20298 (  1%)], Train Loss: 0.08682\n",
      "Epoch: 01 [  244/20298 (  1%)], Train Loss: 0.08764\n",
      "Epoch: 01 [  284/20298 (  1%)], Train Loss: 0.08492\n",
      "Epoch: 01 [  324/20298 (  2%)], Train Loss: 0.08189\n",
      "Epoch: 01 [  364/20298 (  2%)], Train Loss: 0.08546\n",
      "Epoch: 01 [  404/20298 (  2%)], Train Loss: 0.08573\n",
      "Epoch: 01 [  444/20298 (  2%)], Train Loss: 0.08649\n",
      "Epoch: 01 [  484/20298 (  2%)], Train Loss: 0.08730\n",
      "Epoch: 01 [  524/20298 (  3%)], Train Loss: 0.08642\n",
      "Epoch: 01 [  564/20298 (  3%)], Train Loss: 0.08434\n",
      "Epoch: 01 [  604/20298 (  3%)], Train Loss: 0.08213\n",
      "Epoch: 01 [  644/20298 (  3%)], Train Loss: 0.08069\n",
      "Epoch: 01 [  684/20298 (  3%)], Train Loss: 0.07811\n",
      "Epoch: 01 [  724/20298 (  4%)], Train Loss: 0.08117\n",
      "Epoch: 01 [  764/20298 (  4%)], Train Loss: 0.08166\n",
      "Epoch: 01 [  804/20298 (  4%)], Train Loss: 0.08049\n",
      "Epoch: 01 [  844/20298 (  4%)], Train Loss: 0.08074\n",
      "Epoch: 01 [  884/20298 (  4%)], Train Loss: 0.08190\n",
      "Epoch: 01 [  924/20298 (  5%)], Train Loss: 0.08160\n",
      "Epoch: 01 [  964/20298 (  5%)], Train Loss: 0.07900\n",
      "Epoch: 01 [ 1004/20298 (  5%)], Train Loss: 0.07912\n",
      "Epoch: 01 [ 1044/20298 (  5%)], Train Loss: 0.07748\n",
      "Epoch: 01 [ 1084/20298 (  5%)], Train Loss: 0.07671\n",
      "Epoch: 01 [ 1124/20298 (  6%)], Train Loss: 0.07705\n",
      "Epoch: 01 [ 1164/20298 (  6%)], Train Loss: 0.07691\n",
      "Epoch: 01 [ 1204/20298 (  6%)], Train Loss: 0.07596\n",
      "Epoch: 01 [ 1244/20298 (  6%)], Train Loss: 0.07614\n",
      "Epoch: 01 [ 1284/20298 (  6%)], Train Loss: 0.07649\n",
      "Epoch: 01 [ 1324/20298 (  7%)], Train Loss: 0.07602\n",
      "Epoch: 01 [ 1364/20298 (  7%)], Train Loss: 0.07558\n",
      "Epoch: 01 [ 1404/20298 (  7%)], Train Loss: 0.07505\n",
      "Epoch: 01 [ 1444/20298 (  7%)], Train Loss: 0.07448\n",
      "Epoch: 01 [ 1484/20298 (  7%)], Train Loss: 0.07340\n",
      "Epoch: 01 [ 1524/20298 (  8%)], Train Loss: 0.07326\n",
      "Epoch: 01 [ 1564/20298 (  8%)], Train Loss: 0.07305\n",
      "Epoch: 01 [ 1604/20298 (  8%)], Train Loss: 0.07243\n",
      "Epoch: 01 [ 1644/20298 (  8%)], Train Loss: 0.07191\n",
      "Epoch: 01 [ 1684/20298 (  8%)], Train Loss: 0.07131\n",
      "Epoch: 01 [ 1724/20298 (  8%)], Train Loss: 0.07126\n",
      "Epoch: 01 [ 1764/20298 (  9%)], Train Loss: 0.07019\n",
      "Epoch: 01 [ 1804/20298 (  9%)], Train Loss: 0.06939\n",
      "Epoch: 01 [ 1844/20298 (  9%)], Train Loss: 0.06882\n",
      "Epoch: 01 [ 1884/20298 (  9%)], Train Loss: 0.06906\n",
      "Epoch: 01 [ 1924/20298 (  9%)], Train Loss: 0.06862\n",
      "Epoch: 01 [ 1964/20298 ( 10%)], Train Loss: 0.06795\n",
      "Epoch: 01 [ 2004/20298 ( 10%)], Train Loss: 0.06775\n",
      "Epoch: 01 [ 2044/20298 ( 10%)], Train Loss: 0.06701\n",
      "Epoch: 01 [ 2084/20298 ( 10%)], Train Loss: 0.06686\n",
      "Epoch: 01 [ 2124/20298 ( 10%)], Train Loss: 0.06731\n",
      "Epoch: 01 [ 2164/20298 ( 11%)], Train Loss: 0.06659\n",
      "Epoch: 01 [ 2204/20298 ( 11%)], Train Loss: 0.06568\n",
      "Epoch: 01 [ 2244/20298 ( 11%)], Train Loss: 0.06536\n",
      "Epoch: 01 [ 2284/20298 ( 11%)], Train Loss: 0.06493\n",
      "Epoch: 01 [ 2324/20298 ( 11%)], Train Loss: 0.06506\n",
      "Epoch: 01 [ 2364/20298 ( 12%)], Train Loss: 0.06429\n",
      "Epoch: 01 [ 2404/20298 ( 12%)], Train Loss: 0.06359\n",
      "Epoch: 01 [ 2444/20298 ( 12%)], Train Loss: 0.06355\n",
      "Epoch: 01 [ 2484/20298 ( 12%)], Train Loss: 0.06305\n",
      "Epoch: 01 [ 2524/20298 ( 12%)], Train Loss: 0.06316\n",
      "Epoch: 01 [ 2564/20298 ( 13%)], Train Loss: 0.06284\n",
      "Epoch: 01 [ 2604/20298 ( 13%)], Train Loss: 0.06285\n",
      "Epoch: 01 [ 2644/20298 ( 13%)], Train Loss: 0.06252\n",
      "Epoch: 01 [ 2684/20298 ( 13%)], Train Loss: 0.06197\n",
      "Epoch: 01 [ 2724/20298 ( 13%)], Train Loss: 0.06173\n",
      "Epoch: 01 [ 2764/20298 ( 14%)], Train Loss: 0.06126\n",
      "Epoch: 01 [ 2804/20298 ( 14%)], Train Loss: 0.06152\n",
      "Epoch: 01 [ 2844/20298 ( 14%)], Train Loss: 0.06124\n",
      "Epoch: 01 [ 2884/20298 ( 14%)], Train Loss: 0.06132\n",
      "Epoch: 01 [ 2924/20298 ( 14%)], Train Loss: 0.06102\n",
      "Epoch: 01 [ 2964/20298 ( 15%)], Train Loss: 0.06056\n",
      "Epoch: 01 [ 3004/20298 ( 15%)], Train Loss: 0.06012\n",
      "Epoch: 01 [ 3044/20298 ( 15%)], Train Loss: 0.05999\n",
      "Epoch: 01 [ 3084/20298 ( 15%)], Train Loss: 0.05965\n",
      "Epoch: 01 [ 3124/20298 ( 15%)], Train Loss: 0.05914\n",
      "Epoch: 01 [ 3164/20298 ( 16%)], Train Loss: 0.05897\n",
      "Epoch: 01 [ 3204/20298 ( 16%)], Train Loss: 0.05835\n",
      "Epoch: 01 [ 3244/20298 ( 16%)], Train Loss: 0.05793\n",
      "Epoch: 01 [ 3284/20298 ( 16%)], Train Loss: 0.05752\n",
      "Epoch: 01 [ 3324/20298 ( 16%)], Train Loss: 0.05734\n",
      "Epoch: 01 [ 3364/20298 ( 17%)], Train Loss: 0.05689\n",
      "Epoch: 01 [ 3404/20298 ( 17%)], Train Loss: 0.05732\n",
      "Epoch: 01 [ 3444/20298 ( 17%)], Train Loss: 0.05694\n",
      "Epoch: 01 [ 3484/20298 ( 17%)], Train Loss: 0.05672\n",
      "Epoch: 01 [ 3524/20298 ( 17%)], Train Loss: 0.05665\n",
      "Epoch: 01 [ 3564/20298 ( 18%)], Train Loss: 0.05654\n",
      "Epoch: 01 [ 3604/20298 ( 18%)], Train Loss: 0.05655\n",
      "Epoch: 01 [ 3644/20298 ( 18%)], Train Loss: 0.05650\n",
      "Epoch: 01 [ 3684/20298 ( 18%)], Train Loss: 0.05615\n",
      "Epoch: 01 [ 3724/20298 ( 18%)], Train Loss: 0.05605\n",
      "Epoch: 01 [ 3764/20298 ( 19%)], Train Loss: 0.05630\n",
      "Epoch: 01 [ 3804/20298 ( 19%)], Train Loss: 0.05586\n",
      "Epoch: 01 [ 3844/20298 ( 19%)], Train Loss: 0.05550\n",
      "Epoch: 01 [ 3884/20298 ( 19%)], Train Loss: 0.05545\n",
      "Epoch: 01 [ 3924/20298 ( 19%)], Train Loss: 0.05506\n",
      "Epoch: 01 [ 3964/20298 ( 20%)], Train Loss: 0.05475\n",
      "Epoch: 01 [ 4004/20298 ( 20%)], Train Loss: 0.05467\n",
      "Epoch: 01 [ 4044/20298 ( 20%)], Train Loss: 0.05477\n",
      "Epoch: 01 [ 4084/20298 ( 20%)], Train Loss: 0.05452\n",
      "Epoch: 01 [ 4124/20298 ( 20%)], Train Loss: 0.05410\n",
      "Epoch: 01 [ 4164/20298 ( 21%)], Train Loss: 0.05381\n",
      "Epoch: 01 [ 4204/20298 ( 21%)], Train Loss: 0.05394\n",
      "Epoch: 01 [ 4244/20298 ( 21%)], Train Loss: 0.05383\n",
      "Epoch: 01 [ 4284/20298 ( 21%)], Train Loss: 0.05359\n",
      "Epoch: 01 [ 4324/20298 ( 21%)], Train Loss: 0.05321\n",
      "Epoch: 01 [ 4364/20298 ( 21%)], Train Loss: 0.05335\n",
      "Epoch: 01 [ 4404/20298 ( 22%)], Train Loss: 0.05321\n",
      "Epoch: 01 [ 4444/20298 ( 22%)], Train Loss: 0.05300\n",
      "Epoch: 01 [ 4484/20298 ( 22%)], Train Loss: 0.05298\n",
      "Epoch: 01 [ 4524/20298 ( 22%)], Train Loss: 0.05287\n",
      "Epoch: 01 [ 4564/20298 ( 22%)], Train Loss: 0.05272\n",
      "Epoch: 01 [ 4604/20298 ( 23%)], Train Loss: 0.05256\n",
      "Epoch: 01 [ 4644/20298 ( 23%)], Train Loss: 0.05224\n",
      "Epoch: 01 [ 4684/20298 ( 23%)], Train Loss: 0.05215\n",
      "Epoch: 01 [ 4724/20298 ( 23%)], Train Loss: 0.05190\n",
      "Epoch: 01 [ 4764/20298 ( 23%)], Train Loss: 0.05171\n",
      "Epoch: 01 [ 4804/20298 ( 24%)], Train Loss: 0.05148\n",
      "Epoch: 01 [ 4844/20298 ( 24%)], Train Loss: 0.05143\n",
      "Epoch: 01 [ 4884/20298 ( 24%)], Train Loss: 0.05123\n",
      "Epoch: 01 [ 4924/20298 ( 24%)], Train Loss: 0.05094\n",
      "Epoch: 01 [ 4964/20298 ( 24%)], Train Loss: 0.05061\n",
      "Epoch: 01 [ 5004/20298 ( 25%)], Train Loss: 0.05041\n",
      "Epoch: 01 [ 5044/20298 ( 25%)], Train Loss: 0.05042\n",
      "Epoch: 01 [ 5084/20298 ( 25%)], Train Loss: 0.05022\n",
      "Epoch: 01 [ 5124/20298 ( 25%)], Train Loss: 0.05018\n",
      "Epoch: 01 [ 5164/20298 ( 25%)], Train Loss: 0.05023\n",
      "Epoch: 01 [ 5204/20298 ( 26%)], Train Loss: 0.05013\n",
      "Epoch: 01 [ 5244/20298 ( 26%)], Train Loss: 0.04989\n",
      "Epoch: 01 [ 5284/20298 ( 26%)], Train Loss: 0.04961\n",
      "Epoch: 01 [ 5324/20298 ( 26%)], Train Loss: 0.04962\n",
      "Epoch: 01 [ 5364/20298 ( 26%)], Train Loss: 0.04946\n",
      "Epoch: 01 [ 5404/20298 ( 27%)], Train Loss: 0.04924\n",
      "Epoch: 01 [ 5444/20298 ( 27%)], Train Loss: 0.04910\n",
      "Epoch: 01 [ 5484/20298 ( 27%)], Train Loss: 0.04886\n",
      "Epoch: 01 [ 5524/20298 ( 27%)], Train Loss: 0.04883\n",
      "Epoch: 01 [ 5564/20298 ( 27%)], Train Loss: 0.04871\n",
      "Epoch: 01 [ 5604/20298 ( 28%)], Train Loss: 0.04856\n",
      "Epoch: 01 [ 5644/20298 ( 28%)], Train Loss: 0.04858\n",
      "Epoch: 01 [ 5684/20298 ( 28%)], Train Loss: 0.04846\n",
      "Epoch: 01 [ 5724/20298 ( 28%)], Train Loss: 0.04840\n",
      "Epoch: 01 [ 5764/20298 ( 28%)], Train Loss: 0.04839\n",
      "Epoch: 01 [ 5804/20298 ( 29%)], Train Loss: 0.04815\n",
      "Epoch: 01 [ 5844/20298 ( 29%)], Train Loss: 0.04815\n",
      "Epoch: 01 [ 5884/20298 ( 29%)], Train Loss: 0.04801\n",
      "Epoch: 01 [ 5924/20298 ( 29%)], Train Loss: 0.04795\n",
      "Epoch: 01 [ 5964/20298 ( 29%)], Train Loss: 0.04778\n",
      "Epoch: 01 [ 6004/20298 ( 30%)], Train Loss: 0.04777\n",
      "Epoch: 01 [ 6044/20298 ( 30%)], Train Loss: 0.04764\n",
      "Epoch: 01 [ 6084/20298 ( 30%)], Train Loss: 0.04745\n",
      "Epoch: 01 [ 6124/20298 ( 30%)], Train Loss: 0.04744\n",
      "Epoch: 01 [ 6164/20298 ( 30%)], Train Loss: 0.04728\n",
      "Epoch: 01 [ 6204/20298 ( 31%)], Train Loss: 0.04745\n",
      "Epoch: 01 [ 6244/20298 ( 31%)], Train Loss: 0.04728\n",
      "Epoch: 01 [ 6284/20298 ( 31%)], Train Loss: 0.04722\n",
      "Epoch: 01 [ 6324/20298 ( 31%)], Train Loss: 0.04701\n",
      "Epoch: 01 [ 6364/20298 ( 31%)], Train Loss: 0.04706\n",
      "Epoch: 01 [ 6404/20298 ( 32%)], Train Loss: 0.04700\n",
      "Epoch: 01 [ 6444/20298 ( 32%)], Train Loss: 0.04693\n",
      "Epoch: 01 [ 6484/20298 ( 32%)], Train Loss: 0.04681\n",
      "Epoch: 01 [ 6524/20298 ( 32%)], Train Loss: 0.04668\n",
      "Epoch: 01 [ 6564/20298 ( 32%)], Train Loss: 0.04664\n",
      "Epoch: 01 [ 6604/20298 ( 33%)], Train Loss: 0.04651\n",
      "Epoch: 01 [ 6644/20298 ( 33%)], Train Loss: 0.04656\n",
      "Epoch: 01 [ 6684/20298 ( 33%)], Train Loss: 0.04639\n",
      "Epoch: 01 [ 6724/20298 ( 33%)], Train Loss: 0.04629\n",
      "Epoch: 01 [ 6764/20298 ( 33%)], Train Loss: 0.04622\n",
      "Epoch: 01 [ 6804/20298 ( 34%)], Train Loss: 0.04605\n",
      "Epoch: 01 [ 6844/20298 ( 34%)], Train Loss: 0.04606\n",
      "Epoch: 01 [ 6884/20298 ( 34%)], Train Loss: 0.04600\n",
      "Epoch: 01 [ 6924/20298 ( 34%)], Train Loss: 0.04592\n",
      "Epoch: 01 [ 6964/20298 ( 34%)], Train Loss: 0.04594\n",
      "Epoch: 01 [ 7004/20298 ( 35%)], Train Loss: 0.04585\n",
      "Epoch: 01 [ 7044/20298 ( 35%)], Train Loss: 0.04567\n",
      "Epoch: 01 [ 7084/20298 ( 35%)], Train Loss: 0.04558\n",
      "Epoch: 01 [ 7124/20298 ( 35%)], Train Loss: 0.04542\n",
      "Epoch: 01 [ 7164/20298 ( 35%)], Train Loss: 0.04529\n",
      "Epoch: 01 [ 7204/20298 ( 35%)], Train Loss: 0.04522\n",
      "Epoch: 01 [ 7244/20298 ( 36%)], Train Loss: 0.04501\n",
      "Epoch: 01 [ 7284/20298 ( 36%)], Train Loss: 0.04495\n",
      "Epoch: 01 [ 7324/20298 ( 36%)], Train Loss: 0.04487\n",
      "Epoch: 01 [ 7364/20298 ( 36%)], Train Loss: 0.04481\n",
      "Epoch: 01 [ 7404/20298 ( 36%)], Train Loss: 0.04475\n",
      "Epoch: 01 [ 7444/20298 ( 37%)], Train Loss: 0.04471\n",
      "Epoch: 01 [ 7484/20298 ( 37%)], Train Loss: 0.04453\n",
      "Epoch: 01 [ 7524/20298 ( 37%)], Train Loss: 0.04441\n",
      "Epoch: 01 [ 7564/20298 ( 37%)], Train Loss: 0.04438\n",
      "Epoch: 01 [ 7604/20298 ( 37%)], Train Loss: 0.04424\n",
      "Epoch: 01 [ 7644/20298 ( 38%)], Train Loss: 0.04416\n",
      "Epoch: 01 [ 7684/20298 ( 38%)], Train Loss: 0.04412\n",
      "Epoch: 01 [ 7724/20298 ( 38%)], Train Loss: 0.04401\n",
      "Epoch: 01 [ 7764/20298 ( 38%)], Train Loss: 0.04416\n",
      "Epoch: 01 [ 7804/20298 ( 38%)], Train Loss: 0.04399\n",
      "Epoch: 01 [ 7844/20298 ( 39%)], Train Loss: 0.04391\n",
      "Epoch: 01 [ 7884/20298 ( 39%)], Train Loss: 0.04375\n",
      "Epoch: 01 [ 7924/20298 ( 39%)], Train Loss: 0.04365\n",
      "Epoch: 01 [ 7964/20298 ( 39%)], Train Loss: 0.04353\n",
      "Epoch: 01 [ 8004/20298 ( 39%)], Train Loss: 0.04342\n",
      "Epoch: 01 [ 8044/20298 ( 40%)], Train Loss: 0.04330\n",
      "Epoch: 01 [ 8084/20298 ( 40%)], Train Loss: 0.04324\n",
      "Epoch: 01 [ 8124/20298 ( 40%)], Train Loss: 0.04327\n",
      "Epoch: 01 [ 8164/20298 ( 40%)], Train Loss: 0.04316\n",
      "Epoch: 01 [ 8204/20298 ( 40%)], Train Loss: 0.04313\n",
      "Epoch: 01 [ 8244/20298 ( 41%)], Train Loss: 0.04300\n",
      "Epoch: 01 [ 8284/20298 ( 41%)], Train Loss: 0.04299\n",
      "Epoch: 01 [ 8324/20298 ( 41%)], Train Loss: 0.04295\n",
      "Epoch: 01 [ 8364/20298 ( 41%)], Train Loss: 0.04286\n",
      "Epoch: 01 [ 8404/20298 ( 41%)], Train Loss: 0.04287\n",
      "Epoch: 01 [ 8444/20298 ( 42%)], Train Loss: 0.04278\n",
      "Epoch: 01 [ 8484/20298 ( 42%)], Train Loss: 0.04270\n",
      "Epoch: 01 [ 8524/20298 ( 42%)], Train Loss: 0.04266\n",
      "Epoch: 01 [ 8564/20298 ( 42%)], Train Loss: 0.04253\n",
      "Epoch: 01 [ 8604/20298 ( 42%)], Train Loss: 0.04248\n",
      "Epoch: 01 [ 8644/20298 ( 43%)], Train Loss: 0.04237\n",
      "Epoch: 01 [ 8684/20298 ( 43%)], Train Loss: 0.04232\n",
      "Epoch: 01 [ 8724/20298 ( 43%)], Train Loss: 0.04221\n",
      "Epoch: 01 [ 8764/20298 ( 43%)], Train Loss: 0.04215\n",
      "Epoch: 01 [ 8804/20298 ( 43%)], Train Loss: 0.04209\n",
      "Epoch: 01 [ 8844/20298 ( 44%)], Train Loss: 0.04200\n",
      "Epoch: 01 [ 8884/20298 ( 44%)], Train Loss: 0.04187\n",
      "Epoch: 01 [ 8924/20298 ( 44%)], Train Loss: 0.04179\n",
      "Epoch: 01 [ 8964/20298 ( 44%)], Train Loss: 0.04173\n",
      "Epoch: 01 [ 9004/20298 ( 44%)], Train Loss: 0.04165\n",
      "Epoch: 01 [ 9044/20298 ( 45%)], Train Loss: 0.04164\n",
      "Epoch: 01 [ 9084/20298 ( 45%)], Train Loss: 0.04166\n",
      "Epoch: 01 [ 9124/20298 ( 45%)], Train Loss: 0.04162\n",
      "Epoch: 01 [ 9164/20298 ( 45%)], Train Loss: 0.04169\n",
      "Epoch: 01 [ 9204/20298 ( 45%)], Train Loss: 0.04160\n",
      "Epoch: 01 [ 9244/20298 ( 46%)], Train Loss: 0.04150\n",
      "Epoch: 01 [ 9284/20298 ( 46%)], Train Loss: 0.04146\n",
      "Epoch: 01 [ 9324/20298 ( 46%)], Train Loss: 0.04141\n",
      "Epoch: 01 [ 9364/20298 ( 46%)], Train Loss: 0.04140\n",
      "Epoch: 01 [ 9404/20298 ( 46%)], Train Loss: 0.04131\n",
      "Epoch: 01 [ 9444/20298 ( 47%)], Train Loss: 0.04126\n",
      "Epoch: 01 [ 9484/20298 ( 47%)], Train Loss: 0.04117\n",
      "Epoch: 01 [ 9524/20298 ( 47%)], Train Loss: 0.04119\n",
      "Epoch: 01 [ 9564/20298 ( 47%)], Train Loss: 0.04112\n",
      "Epoch: 01 [ 9604/20298 ( 47%)], Train Loss: 0.04108\n",
      "Epoch: 01 [ 9644/20298 ( 48%)], Train Loss: 0.04106\n",
      "Epoch: 01 [ 9684/20298 ( 48%)], Train Loss: 0.04108\n",
      "Epoch: 01 [ 9724/20298 ( 48%)], Train Loss: 0.04103\n",
      "Epoch: 01 [ 9764/20298 ( 48%)], Train Loss: 0.04100\n",
      "Epoch: 01 [ 9804/20298 ( 48%)], Train Loss: 0.04098\n",
      "Epoch: 01 [ 9844/20298 ( 48%)], Train Loss: 0.04098\n",
      "Epoch: 01 [ 9884/20298 ( 49%)], Train Loss: 0.04095\n",
      "Epoch: 01 [ 9924/20298 ( 49%)], Train Loss: 0.04094\n",
      "Epoch: 01 [ 9964/20298 ( 49%)], Train Loss: 0.04098\n",
      "Epoch: 01 [10004/20298 ( 49%)], Train Loss: 0.04099\n",
      "Epoch: 01 [10044/20298 ( 49%)], Train Loss: 0.04084\n",
      "Epoch: 01 [10084/20298 ( 50%)], Train Loss: 0.04077\n",
      "Epoch: 01 [10124/20298 ( 50%)], Train Loss: 0.04067\n",
      "Epoch: 01 [10164/20298 ( 50%)], Train Loss: 0.04060\n",
      "Epoch: 01 [10204/20298 ( 50%)], Train Loss: 0.04056\n",
      "Epoch: 01 [10244/20298 ( 50%)], Train Loss: 0.04056\n",
      "Epoch: 01 [10284/20298 ( 51%)], Train Loss: 0.04053\n",
      "Epoch: 01 [10324/20298 ( 51%)], Train Loss: 0.04054\n",
      "Epoch: 01 [10364/20298 ( 51%)], Train Loss: 0.04048\n",
      "Epoch: 01 [10404/20298 ( 51%)], Train Loss: 0.04045\n",
      "Epoch: 01 [10444/20298 ( 51%)], Train Loss: 0.04040\n",
      "Epoch: 01 [10484/20298 ( 52%)], Train Loss: 0.04032\n",
      "Epoch: 01 [10524/20298 ( 52%)], Train Loss: 0.04028\n",
      "Epoch: 01 [10564/20298 ( 52%)], Train Loss: 0.04024\n",
      "Epoch: 01 [10604/20298 ( 52%)], Train Loss: 0.04017\n",
      "Epoch: 01 [10644/20298 ( 52%)], Train Loss: 0.04007\n",
      "Epoch: 01 [10684/20298 ( 53%)], Train Loss: 0.04004\n",
      "Epoch: 01 [10724/20298 ( 53%)], Train Loss: 0.03998\n",
      "Epoch: 01 [10764/20298 ( 53%)], Train Loss: 0.03997\n",
      "Epoch: 01 [10804/20298 ( 53%)], Train Loss: 0.03994\n",
      "Epoch: 01 [10844/20298 ( 53%)], Train Loss: 0.03983\n",
      "Epoch: 01 [10884/20298 ( 54%)], Train Loss: 0.03970\n",
      "Epoch: 01 [10924/20298 ( 54%)], Train Loss: 0.03974\n",
      "Epoch: 01 [10964/20298 ( 54%)], Train Loss: 0.03967\n",
      "Epoch: 01 [11004/20298 ( 54%)], Train Loss: 0.03971\n",
      "Epoch: 01 [11044/20298 ( 54%)], Train Loss: 0.03968\n",
      "Epoch: 01 [11084/20298 ( 55%)], Train Loss: 0.03960\n",
      "Epoch: 01 [11124/20298 ( 55%)], Train Loss: 0.03954\n",
      "Epoch: 01 [11164/20298 ( 55%)], Train Loss: 0.03960\n",
      "Epoch: 01 [11204/20298 ( 55%)], Train Loss: 0.03958\n",
      "Epoch: 01 [11244/20298 ( 55%)], Train Loss: 0.03960\n",
      "Epoch: 01 [11284/20298 ( 56%)], Train Loss: 0.03962\n",
      "Epoch: 01 [11324/20298 ( 56%)], Train Loss: 0.03952\n",
      "Epoch: 01 [11364/20298 ( 56%)], Train Loss: 0.03945\n",
      "Epoch: 01 [11404/20298 ( 56%)], Train Loss: 0.03952\n",
      "Epoch: 01 [11444/20298 ( 56%)], Train Loss: 0.03957\n",
      "Epoch: 01 [11484/20298 ( 57%)], Train Loss: 0.03950\n",
      "Epoch: 01 [11524/20298 ( 57%)], Train Loss: 0.03945\n",
      "Epoch: 01 [11564/20298 ( 57%)], Train Loss: 0.03937\n",
      "Epoch: 01 [11604/20298 ( 57%)], Train Loss: 0.03924\n",
      "Epoch: 01 [11644/20298 ( 57%)], Train Loss: 0.03921\n",
      "Epoch: 01 [11684/20298 ( 58%)], Train Loss: 0.03919\n",
      "Epoch: 01 [11724/20298 ( 58%)], Train Loss: 0.03915\n",
      "Epoch: 01 [11764/20298 ( 58%)], Train Loss: 0.03904\n",
      "Epoch: 01 [11804/20298 ( 58%)], Train Loss: 0.03892\n",
      "Epoch: 01 [11844/20298 ( 58%)], Train Loss: 0.03901\n",
      "Epoch: 01 [11884/20298 ( 59%)], Train Loss: 0.03897\n",
      "Epoch: 01 [11924/20298 ( 59%)], Train Loss: 0.03898\n",
      "Epoch: 01 [11964/20298 ( 59%)], Train Loss: 0.03900\n",
      "Epoch: 01 [12004/20298 ( 59%)], Train Loss: 0.03896\n",
      "Epoch: 01 [12044/20298 ( 59%)], Train Loss: 0.03897\n",
      "Epoch: 01 [12084/20298 ( 60%)], Train Loss: 0.03894\n",
      "Epoch: 01 [12124/20298 ( 60%)], Train Loss: 0.03888\n",
      "Epoch: 01 [12164/20298 ( 60%)], Train Loss: 0.03881\n",
      "Epoch: 01 [12204/20298 ( 60%)], Train Loss: 0.03887\n",
      "Epoch: 01 [12244/20298 ( 60%)], Train Loss: 0.03885\n",
      "Epoch: 01 [12284/20298 ( 61%)], Train Loss: 0.03876\n",
      "Epoch: 01 [12324/20298 ( 61%)], Train Loss: 0.03872\n",
      "Epoch: 01 [12364/20298 ( 61%)], Train Loss: 0.03873\n",
      "Epoch: 01 [12404/20298 ( 61%)], Train Loss: 0.03876\n",
      "Epoch: 01 [12444/20298 ( 61%)], Train Loss: 0.03876\n",
      "Epoch: 01 [12484/20298 ( 62%)], Train Loss: 0.03888\n",
      "Epoch: 01 [12524/20298 ( 62%)], Train Loss: 0.03883\n",
      "Epoch: 01 [12564/20298 ( 62%)], Train Loss: 0.03876\n",
      "Epoch: 01 [12604/20298 ( 62%)], Train Loss: 0.03882\n",
      "Epoch: 01 [12644/20298 ( 62%)], Train Loss: 0.03883\n",
      "Epoch: 01 [12684/20298 ( 62%)], Train Loss: 0.03893\n",
      "Epoch: 01 [12724/20298 ( 63%)], Train Loss: 0.03894\n",
      "Epoch: 01 [12764/20298 ( 63%)], Train Loss: 0.03888\n",
      "Epoch: 01 [12804/20298 ( 63%)], Train Loss: 0.03883\n",
      "Epoch: 01 [12844/20298 ( 63%)], Train Loss: 0.03882\n",
      "Epoch: 01 [12884/20298 ( 63%)], Train Loss: 0.03883\n",
      "Epoch: 01 [12924/20298 ( 64%)], Train Loss: 0.03887\n",
      "Epoch: 01 [12964/20298 ( 64%)], Train Loss: 0.03887\n",
      "Epoch: 01 [13004/20298 ( 64%)], Train Loss: 0.03888\n",
      "Epoch: 01 [13044/20298 ( 64%)], Train Loss: 0.03887\n",
      "Epoch: 01 [13084/20298 ( 64%)], Train Loss: 0.03880\n",
      "Epoch: 01 [13124/20298 ( 65%)], Train Loss: 0.03873\n",
      "Epoch: 01 [13164/20298 ( 65%)], Train Loss: 0.03875\n",
      "Epoch: 01 [13204/20298 ( 65%)], Train Loss: 0.03880\n",
      "Epoch: 01 [13244/20298 ( 65%)], Train Loss: 0.03885\n",
      "Epoch: 01 [13284/20298 ( 65%)], Train Loss: 0.03882\n",
      "Epoch: 01 [13324/20298 ( 66%)], Train Loss: 0.03897\n",
      "Epoch: 01 [13364/20298 ( 66%)], Train Loss: 0.03899\n",
      "Epoch: 01 [13404/20298 ( 66%)], Train Loss: 0.03897\n",
      "Epoch: 01 [13444/20298 ( 66%)], Train Loss: 0.03905\n",
      "Epoch: 01 [13484/20298 ( 66%)], Train Loss: 0.03898\n",
      "Epoch: 01 [13524/20298 ( 67%)], Train Loss: 0.03899\n",
      "Epoch: 01 [13564/20298 ( 67%)], Train Loss: 0.03894\n",
      "Epoch: 01 [13604/20298 ( 67%)], Train Loss: 0.03894\n",
      "Epoch: 01 [13644/20298 ( 67%)], Train Loss: 0.03891\n",
      "Epoch: 01 [13684/20298 ( 67%)], Train Loss: 0.03885\n",
      "Epoch: 01 [13724/20298 ( 68%)], Train Loss: 0.03881\n",
      "Epoch: 01 [13764/20298 ( 68%)], Train Loss: 0.03878\n",
      "Epoch: 01 [13804/20298 ( 68%)], Train Loss: 0.03887\n",
      "Epoch: 01 [13844/20298 ( 68%)], Train Loss: 0.03894\n",
      "Epoch: 01 [13884/20298 ( 68%)], Train Loss: 0.03888\n",
      "Epoch: 01 [13924/20298 ( 69%)], Train Loss: 0.03884\n",
      "Epoch: 01 [13964/20298 ( 69%)], Train Loss: 0.03880\n",
      "Epoch: 01 [14004/20298 ( 69%)], Train Loss: 0.03872\n",
      "Epoch: 01 [14044/20298 ( 69%)], Train Loss: 0.03864\n",
      "Epoch: 01 [14084/20298 ( 69%)], Train Loss: 0.03865\n",
      "Epoch: 01 [14124/20298 ( 70%)], Train Loss: 0.03869\n",
      "Epoch: 01 [14164/20298 ( 70%)], Train Loss: 0.03865\n",
      "Epoch: 01 [14204/20298 ( 70%)], Train Loss: 0.03863\n",
      "Epoch: 01 [14244/20298 ( 70%)], Train Loss: 0.03864\n",
      "Epoch: 01 [14284/20298 ( 70%)], Train Loss: 0.03858\n",
      "Epoch: 01 [14324/20298 ( 71%)], Train Loss: 0.03856\n",
      "Epoch: 01 [14364/20298 ( 71%)], Train Loss: 0.03856\n",
      "Epoch: 01 [14404/20298 ( 71%)], Train Loss: 0.03850\n",
      "Epoch: 01 [14444/20298 ( 71%)], Train Loss: 0.03844\n",
      "Epoch: 01 [14484/20298 ( 71%)], Train Loss: 0.03844\n",
      "Epoch: 01 [14524/20298 ( 72%)], Train Loss: 0.03842\n",
      "Epoch: 01 [14564/20298 ( 72%)], Train Loss: 0.03836\n",
      "Epoch: 01 [14604/20298 ( 72%)], Train Loss: 0.03842\n",
      "Epoch: 01 [14644/20298 ( 72%)], Train Loss: 0.03838\n",
      "Epoch: 01 [14684/20298 ( 72%)], Train Loss: 0.03835\n",
      "Epoch: 01 [14724/20298 ( 73%)], Train Loss: 0.03832\n",
      "Epoch: 01 [14764/20298 ( 73%)], Train Loss: 0.03831\n",
      "Epoch: 01 [14804/20298 ( 73%)], Train Loss: 0.03835\n",
      "Epoch: 01 [14844/20298 ( 73%)], Train Loss: 0.03828\n",
      "Epoch: 01 [14884/20298 ( 73%)], Train Loss: 0.03825\n",
      "Epoch: 01 [14924/20298 ( 74%)], Train Loss: 0.03831\n",
      "Epoch: 01 [14964/20298 ( 74%)], Train Loss: 0.03828\n",
      "Epoch: 01 [15004/20298 ( 74%)], Train Loss: 0.03824\n",
      "Epoch: 01 [15044/20298 ( 74%)], Train Loss: 0.03823\n",
      "Epoch: 01 [15084/20298 ( 74%)], Train Loss: 0.03823\n",
      "Epoch: 01 [15124/20298 ( 75%)], Train Loss: 0.03822\n",
      "Epoch: 01 [15164/20298 ( 75%)], Train Loss: 0.03823\n",
      "Epoch: 01 [15204/20298 ( 75%)], Train Loss: 0.03817\n",
      "Epoch: 01 [15244/20298 ( 75%)], Train Loss: 0.03813\n",
      "Epoch: 01 [15284/20298 ( 75%)], Train Loss: 0.03811\n",
      "Epoch: 01 [15324/20298 ( 75%)], Train Loss: 0.03819\n",
      "Epoch: 01 [15364/20298 ( 76%)], Train Loss: 0.03818\n",
      "Epoch: 01 [15404/20298 ( 76%)], Train Loss: 0.03816\n",
      "Epoch: 01 [15444/20298 ( 76%)], Train Loss: 0.03810\n",
      "Epoch: 01 [15484/20298 ( 76%)], Train Loss: 0.03802\n",
      "Epoch: 01 [15524/20298 ( 76%)], Train Loss: 0.03799\n",
      "Epoch: 01 [15564/20298 ( 77%)], Train Loss: 0.03801\n",
      "Epoch: 01 [15604/20298 ( 77%)], Train Loss: 0.03794\n",
      "Epoch: 01 [15644/20298 ( 77%)], Train Loss: 0.03788\n",
      "Epoch: 01 [15684/20298 ( 77%)], Train Loss: 0.03790\n",
      "Epoch: 01 [15724/20298 ( 77%)], Train Loss: 0.03789\n",
      "Epoch: 01 [15764/20298 ( 78%)], Train Loss: 0.03782\n",
      "Epoch: 01 [15804/20298 ( 78%)], Train Loss: 0.03785\n",
      "Epoch: 01 [15844/20298 ( 78%)], Train Loss: 0.03788\n",
      "Epoch: 01 [15884/20298 ( 78%)], Train Loss: 0.03795\n",
      "Epoch: 01 [15924/20298 ( 78%)], Train Loss: 0.03795\n",
      "Epoch: 01 [15964/20298 ( 79%)], Train Loss: 0.03793\n",
      "Epoch: 01 [16004/20298 ( 79%)], Train Loss: 0.03793\n",
      "Epoch: 01 [16044/20298 ( 79%)], Train Loss: 0.03801\n",
      "Epoch: 01 [16084/20298 ( 79%)], Train Loss: 0.03797\n",
      "Epoch: 01 [16124/20298 ( 79%)], Train Loss: 0.03799\n",
      "Epoch: 01 [16164/20298 ( 80%)], Train Loss: 0.03804\n",
      "Epoch: 01 [16204/20298 ( 80%)], Train Loss: 0.03806\n",
      "Epoch: 01 [16244/20298 ( 80%)], Train Loss: 0.03808\n",
      "Epoch: 01 [16284/20298 ( 80%)], Train Loss: 0.03808\n",
      "Epoch: 01 [16324/20298 ( 80%)], Train Loss: 0.03808\n",
      "Epoch: 01 [16364/20298 ( 81%)], Train Loss: 0.03805\n",
      "Epoch: 01 [16404/20298 ( 81%)], Train Loss: 0.03800\n",
      "Epoch: 01 [16444/20298 ( 81%)], Train Loss: 0.03801\n",
      "Epoch: 01 [16484/20298 ( 81%)], Train Loss: 0.03795\n",
      "Epoch: 01 [16524/20298 ( 81%)], Train Loss: 0.03792\n",
      "Epoch: 01 [16564/20298 ( 82%)], Train Loss: 0.03792\n",
      "Epoch: 01 [16604/20298 ( 82%)], Train Loss: 0.03789\n",
      "Epoch: 01 [16644/20298 ( 82%)], Train Loss: 0.03786\n",
      "Epoch: 01 [16684/20298 ( 82%)], Train Loss: 0.03785\n",
      "Epoch: 01 [16724/20298 ( 82%)], Train Loss: 0.03789\n",
      "Epoch: 01 [16764/20298 ( 83%)], Train Loss: 0.03787\n",
      "Epoch: 01 [16804/20298 ( 83%)], Train Loss: 0.03785\n",
      "Epoch: 01 [16844/20298 ( 83%)], Train Loss: 0.03787\n",
      "Epoch: 01 [16884/20298 ( 83%)], Train Loss: 0.03783\n",
      "Epoch: 01 [16924/20298 ( 83%)], Train Loss: 0.03777\n",
      "Epoch: 01 [16964/20298 ( 84%)], Train Loss: 0.03778\n",
      "Epoch: 01 [17004/20298 ( 84%)], Train Loss: 0.03776\n",
      "Epoch: 01 [17044/20298 ( 84%)], Train Loss: 0.03784\n",
      "Epoch: 01 [17084/20298 ( 84%)], Train Loss: 0.03783\n",
      "Epoch: 01 [17124/20298 ( 84%)], Train Loss: 0.03785\n",
      "Epoch: 01 [17164/20298 ( 85%)], Train Loss: 0.03783\n",
      "Epoch: 01 [17204/20298 ( 85%)], Train Loss: 0.03781\n",
      "Epoch: 01 [17244/20298 ( 85%)], Train Loss: 0.03779\n",
      "Epoch: 01 [17284/20298 ( 85%)], Train Loss: 0.03778\n",
      "Epoch: 01 [17324/20298 ( 85%)], Train Loss: 0.03779\n",
      "Epoch: 01 [17364/20298 ( 86%)], Train Loss: 0.03781\n",
      "Epoch: 01 [17404/20298 ( 86%)], Train Loss: 0.03777\n",
      "Epoch: 01 [17444/20298 ( 86%)], Train Loss: 0.03774\n",
      "Epoch: 01 [17484/20298 ( 86%)], Train Loss: 0.03775\n",
      "Epoch: 01 [17524/20298 ( 86%)], Train Loss: 0.03773\n",
      "Epoch: 01 [17564/20298 ( 87%)], Train Loss: 0.03768\n",
      "Epoch: 01 [17604/20298 ( 87%)], Train Loss: 0.03769\n",
      "Epoch: 01 [17644/20298 ( 87%)], Train Loss: 0.03774\n",
      "Epoch: 01 [17684/20298 ( 87%)], Train Loss: 0.03769\n",
      "Epoch: 01 [17724/20298 ( 87%)], Train Loss: 0.03762\n",
      "Epoch: 01 [17764/20298 ( 88%)], Train Loss: 0.03757\n",
      "Epoch: 01 [17804/20298 ( 88%)], Train Loss: 0.03755\n",
      "Epoch: 01 [17844/20298 ( 88%)], Train Loss: 0.03753\n",
      "Epoch: 01 [17884/20298 ( 88%)], Train Loss: 0.03748\n",
      "Epoch: 01 [17924/20298 ( 88%)], Train Loss: 0.03744\n",
      "Epoch: 01 [17964/20298 ( 89%)], Train Loss: 0.03752\n",
      "Epoch: 01 [18004/20298 ( 89%)], Train Loss: 0.03749\n",
      "Epoch: 01 [18044/20298 ( 89%)], Train Loss: 0.03744\n",
      "Epoch: 01 [18084/20298 ( 89%)], Train Loss: 0.03741\n",
      "Epoch: 01 [18124/20298 ( 89%)], Train Loss: 0.03736\n",
      "Epoch: 01 [18164/20298 ( 89%)], Train Loss: 0.03738\n",
      "Epoch: 01 [18204/20298 ( 90%)], Train Loss: 0.03738\n",
      "Epoch: 01 [18244/20298 ( 90%)], Train Loss: 0.03734\n",
      "Epoch: 01 [18284/20298 ( 90%)], Train Loss: 0.03737\n",
      "Epoch: 01 [18324/20298 ( 90%)], Train Loss: 0.03738\n",
      "Epoch: 01 [18364/20298 ( 90%)], Train Loss: 0.03743\n",
      "Epoch: 01 [18404/20298 ( 91%)], Train Loss: 0.03741\n",
      "Epoch: 01 [18444/20298 ( 91%)], Train Loss: 0.03743\n",
      "Epoch: 01 [18484/20298 ( 91%)], Train Loss: 0.03744\n",
      "Epoch: 01 [18524/20298 ( 91%)], Train Loss: 0.03743\n",
      "Epoch: 01 [18564/20298 ( 91%)], Train Loss: 0.03741\n",
      "Epoch: 01 [18604/20298 ( 92%)], Train Loss: 0.03740\n",
      "Epoch: 01 [18644/20298 ( 92%)], Train Loss: 0.03742\n",
      "Epoch: 01 [18684/20298 ( 92%)], Train Loss: 0.03743\n",
      "Epoch: 01 [18724/20298 ( 92%)], Train Loss: 0.03746\n",
      "Epoch: 01 [18764/20298 ( 92%)], Train Loss: 0.03751\n",
      "Epoch: 01 [18804/20298 ( 93%)], Train Loss: 0.03750\n",
      "Epoch: 01 [18844/20298 ( 93%)], Train Loss: 0.03747\n",
      "Epoch: 01 [18884/20298 ( 93%)], Train Loss: 0.03748\n",
      "Epoch: 01 [18924/20298 ( 93%)], Train Loss: 0.03742\n",
      "Epoch: 01 [18964/20298 ( 93%)], Train Loss: 0.03748\n",
      "Epoch: 01 [19004/20298 ( 94%)], Train Loss: 0.03746\n",
      "Epoch: 01 [19044/20298 ( 94%)], Train Loss: 0.03743\n",
      "Epoch: 01 [19084/20298 ( 94%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19124/20298 ( 94%)], Train Loss: 0.03740\n",
      "Epoch: 01 [19164/20298 ( 94%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19204/20298 ( 95%)], Train Loss: 0.03748\n",
      "Epoch: 01 [19244/20298 ( 95%)], Train Loss: 0.03743\n",
      "Epoch: 01 [19284/20298 ( 95%)], Train Loss: 0.03746\n",
      "Epoch: 01 [19324/20298 ( 95%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19364/20298 ( 95%)], Train Loss: 0.03742\n",
      "Epoch: 01 [19404/20298 ( 96%)], Train Loss: 0.03746\n",
      "Epoch: 01 [19444/20298 ( 96%)], Train Loss: 0.03745\n",
      "Epoch: 01 [19484/20298 ( 96%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19524/20298 ( 96%)], Train Loss: 0.03747\n",
      "Epoch: 01 [19564/20298 ( 96%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19604/20298 ( 97%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19644/20298 ( 97%)], Train Loss: 0.03747\n",
      "Epoch: 01 [19684/20298 ( 97%)], Train Loss: 0.03746\n",
      "Epoch: 01 [19724/20298 ( 97%)], Train Loss: 0.03745\n",
      "Epoch: 01 [19764/20298 ( 97%)], Train Loss: 0.03741\n",
      "Epoch: 01 [19804/20298 ( 98%)], Train Loss: 0.03739\n",
      "Epoch: 01 [19844/20298 ( 98%)], Train Loss: 0.03738\n",
      "Epoch: 01 [19884/20298 ( 98%)], Train Loss: 0.03741\n",
      "Epoch: 01 [19924/20298 ( 98%)], Train Loss: 0.03738\n",
      "Epoch: 01 [19964/20298 ( 98%)], Train Loss: 0.03734\n",
      "Epoch: 01 [20004/20298 ( 99%)], Train Loss: 0.03729\n",
      "Epoch: 01 [20044/20298 ( 99%)], Train Loss: 0.03734\n",
      "Epoch: 01 [20084/20298 ( 99%)], Train Loss: 0.03732\n",
      "Epoch: 01 [20124/20298 ( 99%)], Train Loss: 0.03729\n",
      "Epoch: 01 [20164/20298 ( 99%)], Train Loss: 0.03739\n",
      "Epoch: 01 [20204/20298 (100%)], Train Loss: 0.03738\n",
      "Epoch: 01 [20244/20298 (100%)], Train Loss: 0.03735\n",
      "Epoch: 01 [20284/20298 (100%)], Train Loss: 0.03732\n",
      "Epoch: 01 [20298/20298 (100%)], Train Loss: 0.03730\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.31044\n",
      "\n",
      "Total Training Time: 5345.942857027054secs, Average Training Time per Epoch: 2672.971428513527secs.\n",
      "Total Validation Time: 242.50897574424744secs, Average Validation Time per Epoch: 121.25448787212372secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 2\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 19954, Num examples Valid=3115\n",
      "Total Training Steps: 1248, Total Warmup Steps: 124\n",
      "Epoch: 00 [    4/19954 (  0%)], Train Loss: 0.72485\n",
      "Epoch: 00 [   44/19954 (  0%)], Train Loss: 0.72362\n",
      "Epoch: 00 [   84/19954 (  0%)], Train Loss: 0.72297\n",
      "Epoch: 00 [  124/19954 (  1%)], Train Loss: 0.72150\n",
      "Epoch: 00 [  164/19954 (  1%)], Train Loss: 0.72208\n",
      "Epoch: 00 [  204/19954 (  1%)], Train Loss: 0.72124\n",
      "Epoch: 00 [  244/19954 (  1%)], Train Loss: 0.71800\n",
      "Epoch: 00 [  284/19954 (  1%)], Train Loss: 0.71496\n",
      "Epoch: 00 [  324/19954 (  2%)], Train Loss: 0.71099\n",
      "Epoch: 00 [  364/19954 (  2%)], Train Loss: 0.70697\n",
      "Epoch: 00 [  404/19954 (  2%)], Train Loss: 0.70308\n",
      "Epoch: 00 [  444/19954 (  2%)], Train Loss: 0.69901\n",
      "Epoch: 00 [  484/19954 (  2%)], Train Loss: 0.69408\n",
      "Epoch: 00 [  524/19954 (  3%)], Train Loss: 0.68818\n",
      "Epoch: 00 [  564/19954 (  3%)], Train Loss: 0.68064\n",
      "Epoch: 00 [  604/19954 (  3%)], Train Loss: 0.67431\n",
      "Epoch: 00 [  644/19954 (  3%)], Train Loss: 0.66663\n",
      "Epoch: 00 [  684/19954 (  3%)], Train Loss: 0.65953\n",
      "Epoch: 00 [  724/19954 (  4%)], Train Loss: 0.65046\n",
      "Epoch: 00 [  764/19954 (  4%)], Train Loss: 0.64096\n",
      "Epoch: 00 [  804/19954 (  4%)], Train Loss: 0.63299\n",
      "Epoch: 00 [  844/19954 (  4%)], Train Loss: 0.62380\n",
      "Epoch: 00 [  884/19954 (  4%)], Train Loss: 0.61490\n",
      "Epoch: 00 [  924/19954 (  5%)], Train Loss: 0.60426\n",
      "Epoch: 00 [  964/19954 (  5%)], Train Loss: 0.59273\n",
      "Epoch: 00 [ 1004/19954 (  5%)], Train Loss: 0.58358\n",
      "Epoch: 00 [ 1044/19954 (  5%)], Train Loss: 0.57350\n",
      "Epoch: 00 [ 1084/19954 (  5%)], Train Loss: 0.56094\n",
      "Epoch: 00 [ 1124/19954 (  6%)], Train Loss: 0.55043\n",
      "Epoch: 00 [ 1164/19954 (  6%)], Train Loss: 0.54060\n",
      "Epoch: 00 [ 1204/19954 (  6%)], Train Loss: 0.52865\n",
      "Epoch: 00 [ 1244/19954 (  6%)], Train Loss: 0.51879\n",
      "Epoch: 00 [ 1284/19954 (  6%)], Train Loss: 0.50708\n",
      "Epoch: 00 [ 1324/19954 (  7%)], Train Loss: 0.49645\n",
      "Epoch: 00 [ 1364/19954 (  7%)], Train Loss: 0.48764\n",
      "Epoch: 00 [ 1404/19954 (  7%)], Train Loss: 0.47689\n",
      "Epoch: 00 [ 1444/19954 (  7%)], Train Loss: 0.46684\n",
      "Epoch: 00 [ 1484/19954 (  7%)], Train Loss: 0.45878\n",
      "Epoch: 00 [ 1524/19954 (  8%)], Train Loss: 0.45132\n",
      "Epoch: 00 [ 1564/19954 (  8%)], Train Loss: 0.44421\n",
      "Epoch: 00 [ 1604/19954 (  8%)], Train Loss: 0.43688\n",
      "Epoch: 00 [ 1644/19954 (  8%)], Train Loss: 0.42900\n",
      "Epoch: 00 [ 1684/19954 (  8%)], Train Loss: 0.42364\n",
      "Epoch: 00 [ 1724/19954 (  9%)], Train Loss: 0.41641\n",
      "Epoch: 00 [ 1764/19954 (  9%)], Train Loss: 0.40974\n",
      "Epoch: 00 [ 1804/19954 (  9%)], Train Loss: 0.40368\n",
      "Epoch: 00 [ 1844/19954 (  9%)], Train Loss: 0.39843\n",
      "Epoch: 00 [ 1884/19954 (  9%)], Train Loss: 0.39150\n",
      "Epoch: 00 [ 1924/19954 ( 10%)], Train Loss: 0.38549\n",
      "Epoch: 00 [ 1964/19954 ( 10%)], Train Loss: 0.38046\n",
      "Epoch: 00 [ 2004/19954 ( 10%)], Train Loss: 0.37495\n",
      "Epoch: 00 [ 2044/19954 ( 10%)], Train Loss: 0.36981\n",
      "Epoch: 00 [ 2084/19954 ( 10%)], Train Loss: 0.36492\n",
      "Epoch: 00 [ 2124/19954 ( 11%)], Train Loss: 0.36062\n",
      "Epoch: 00 [ 2164/19954 ( 11%)], Train Loss: 0.35643\n",
      "Epoch: 00 [ 2204/19954 ( 11%)], Train Loss: 0.35227\n",
      "Epoch: 00 [ 2244/19954 ( 11%)], Train Loss: 0.34787\n",
      "Epoch: 00 [ 2284/19954 ( 11%)], Train Loss: 0.34269\n",
      "Epoch: 00 [ 2324/19954 ( 12%)], Train Loss: 0.33921\n",
      "Epoch: 00 [ 2364/19954 ( 12%)], Train Loss: 0.33707\n",
      "Epoch: 00 [ 2404/19954 ( 12%)], Train Loss: 0.33403\n",
      "Epoch: 00 [ 2444/19954 ( 12%)], Train Loss: 0.32957\n",
      "Epoch: 00 [ 2484/19954 ( 12%)], Train Loss: 0.32608\n",
      "Epoch: 00 [ 2524/19954 ( 13%)], Train Loss: 0.32169\n",
      "Epoch: 00 [ 2564/19954 ( 13%)], Train Loss: 0.31820\n",
      "Epoch: 00 [ 2604/19954 ( 13%)], Train Loss: 0.31529\n",
      "Epoch: 00 [ 2644/19954 ( 13%)], Train Loss: 0.31259\n",
      "Epoch: 00 [ 2684/19954 ( 13%)], Train Loss: 0.30905\n",
      "Epoch: 00 [ 2724/19954 ( 14%)], Train Loss: 0.30567\n",
      "Epoch: 00 [ 2764/19954 ( 14%)], Train Loss: 0.30316\n",
      "Epoch: 00 [ 2804/19954 ( 14%)], Train Loss: 0.30000\n",
      "Epoch: 00 [ 2844/19954 ( 14%)], Train Loss: 0.29683\n",
      "Epoch: 00 [ 2884/19954 ( 14%)], Train Loss: 0.29410\n",
      "Epoch: 00 [ 2924/19954 ( 15%)], Train Loss: 0.29174\n",
      "Epoch: 00 [ 2964/19954 ( 15%)], Train Loss: 0.28935\n",
      "Epoch: 00 [ 3004/19954 ( 15%)], Train Loss: 0.28679\n",
      "Epoch: 00 [ 3044/19954 ( 15%)], Train Loss: 0.28433\n",
      "Epoch: 00 [ 3084/19954 ( 15%)], Train Loss: 0.28214\n",
      "Epoch: 00 [ 3124/19954 ( 16%)], Train Loss: 0.27963\n",
      "Epoch: 00 [ 3164/19954 ( 16%)], Train Loss: 0.27745\n",
      "Epoch: 00 [ 3204/19954 ( 16%)], Train Loss: 0.27485\n",
      "Epoch: 00 [ 3244/19954 ( 16%)], Train Loss: 0.27270\n",
      "Epoch: 00 [ 3284/19954 ( 16%)], Train Loss: 0.27127\n",
      "Epoch: 00 [ 3324/19954 ( 17%)], Train Loss: 0.26865\n",
      "Epoch: 00 [ 3364/19954 ( 17%)], Train Loss: 0.26645\n",
      "Epoch: 00 [ 3404/19954 ( 17%)], Train Loss: 0.26484\n",
      "Epoch: 00 [ 3444/19954 ( 17%)], Train Loss: 0.26274\n",
      "Epoch: 00 [ 3484/19954 ( 17%)], Train Loss: 0.26060\n",
      "Epoch: 00 [ 3524/19954 ( 18%)], Train Loss: 0.25849\n",
      "Epoch: 00 [ 3564/19954 ( 18%)], Train Loss: 0.25644\n",
      "Epoch: 00 [ 3604/19954 ( 18%)], Train Loss: 0.25468\n",
      "Epoch: 00 [ 3644/19954 ( 18%)], Train Loss: 0.25266\n",
      "Epoch: 00 [ 3684/19954 ( 18%)], Train Loss: 0.25099\n",
      "Epoch: 00 [ 3724/19954 ( 19%)], Train Loss: 0.24941\n",
      "Epoch: 00 [ 3764/19954 ( 19%)], Train Loss: 0.24758\n",
      "Epoch: 00 [ 3804/19954 ( 19%)], Train Loss: 0.24577\n",
      "Epoch: 00 [ 3844/19954 ( 19%)], Train Loss: 0.24447\n",
      "Epoch: 00 [ 3884/19954 ( 19%)], Train Loss: 0.24342\n",
      "Epoch: 00 [ 3924/19954 ( 20%)], Train Loss: 0.24181\n",
      "Epoch: 00 [ 3964/19954 ( 20%)], Train Loss: 0.24021\n",
      "Epoch: 00 [ 4004/19954 ( 20%)], Train Loss: 0.23882\n",
      "Epoch: 00 [ 4044/19954 ( 20%)], Train Loss: 0.23764\n",
      "Epoch: 00 [ 4084/19954 ( 20%)], Train Loss: 0.23624\n",
      "Epoch: 00 [ 4124/19954 ( 21%)], Train Loss: 0.23497\n",
      "Epoch: 00 [ 4164/19954 ( 21%)], Train Loss: 0.23402\n",
      "Epoch: 00 [ 4204/19954 ( 21%)], Train Loss: 0.23280\n",
      "Epoch: 00 [ 4244/19954 ( 21%)], Train Loss: 0.23173\n",
      "Epoch: 00 [ 4284/19954 ( 21%)], Train Loss: 0.23056\n",
      "Epoch: 00 [ 4324/19954 ( 22%)], Train Loss: 0.22917\n",
      "Epoch: 00 [ 4364/19954 ( 22%)], Train Loss: 0.22782\n",
      "Epoch: 00 [ 4404/19954 ( 22%)], Train Loss: 0.22684\n",
      "Epoch: 00 [ 4444/19954 ( 22%)], Train Loss: 0.22572\n",
      "Epoch: 00 [ 4484/19954 ( 22%)], Train Loss: 0.22446\n",
      "Epoch: 00 [ 4524/19954 ( 23%)], Train Loss: 0.22403\n",
      "Epoch: 00 [ 4564/19954 ( 23%)], Train Loss: 0.22288\n",
      "Epoch: 00 [ 4604/19954 ( 23%)], Train Loss: 0.22169\n",
      "Epoch: 00 [ 4644/19954 ( 23%)], Train Loss: 0.22080\n",
      "Epoch: 00 [ 4684/19954 ( 23%)], Train Loss: 0.21974\n",
      "Epoch: 00 [ 4724/19954 ( 24%)], Train Loss: 0.21896\n",
      "Epoch: 00 [ 4764/19954 ( 24%)], Train Loss: 0.21809\n",
      "Epoch: 00 [ 4804/19954 ( 24%)], Train Loss: 0.21761\n",
      "Epoch: 00 [ 4844/19954 ( 24%)], Train Loss: 0.21627\n",
      "Epoch: 00 [ 4884/19954 ( 24%)], Train Loss: 0.21546\n",
      "Epoch: 00 [ 4924/19954 ( 25%)], Train Loss: 0.21457\n",
      "Epoch: 00 [ 4964/19954 ( 25%)], Train Loss: 0.21378\n",
      "Epoch: 00 [ 5004/19954 ( 25%)], Train Loss: 0.21240\n",
      "Epoch: 00 [ 5044/19954 ( 25%)], Train Loss: 0.21140\n",
      "Epoch: 00 [ 5084/19954 ( 25%)], Train Loss: 0.21026\n",
      "Epoch: 00 [ 5124/19954 ( 26%)], Train Loss: 0.20919\n",
      "Epoch: 00 [ 5164/19954 ( 26%)], Train Loss: 0.20850\n",
      "Epoch: 00 [ 5204/19954 ( 26%)], Train Loss: 0.20759\n",
      "Epoch: 00 [ 5244/19954 ( 26%)], Train Loss: 0.20692\n",
      "Epoch: 00 [ 5284/19954 ( 26%)], Train Loss: 0.20601\n",
      "Epoch: 00 [ 5324/19954 ( 27%)], Train Loss: 0.20526\n",
      "Epoch: 00 [ 5364/19954 ( 27%)], Train Loss: 0.20429\n",
      "Epoch: 00 [ 5404/19954 ( 27%)], Train Loss: 0.20330\n",
      "Epoch: 00 [ 5444/19954 ( 27%)], Train Loss: 0.20282\n",
      "Epoch: 00 [ 5484/19954 ( 27%)], Train Loss: 0.20182\n",
      "Epoch: 00 [ 5524/19954 ( 28%)], Train Loss: 0.20107\n",
      "Epoch: 00 [ 5564/19954 ( 28%)], Train Loss: 0.20051\n",
      "Epoch: 00 [ 5604/19954 ( 28%)], Train Loss: 0.19986\n",
      "Epoch: 00 [ 5644/19954 ( 28%)], Train Loss: 0.19915\n",
      "Epoch: 00 [ 5684/19954 ( 28%)], Train Loss: 0.19857\n",
      "Epoch: 00 [ 5724/19954 ( 29%)], Train Loss: 0.19771\n",
      "Epoch: 00 [ 5764/19954 ( 29%)], Train Loss: 0.19706\n",
      "Epoch: 00 [ 5804/19954 ( 29%)], Train Loss: 0.19651\n",
      "Epoch: 00 [ 5844/19954 ( 29%)], Train Loss: 0.19614\n",
      "Epoch: 00 [ 5884/19954 ( 29%)], Train Loss: 0.19571\n",
      "Epoch: 00 [ 5924/19954 ( 30%)], Train Loss: 0.19510\n",
      "Epoch: 00 [ 5964/19954 ( 30%)], Train Loss: 0.19451\n",
      "Epoch: 00 [ 6004/19954 ( 30%)], Train Loss: 0.19371\n",
      "Epoch: 00 [ 6044/19954 ( 30%)], Train Loss: 0.19301\n",
      "Epoch: 00 [ 6084/19954 ( 30%)], Train Loss: 0.19220\n",
      "Epoch: 00 [ 6124/19954 ( 31%)], Train Loss: 0.19155\n",
      "Epoch: 00 [ 6164/19954 ( 31%)], Train Loss: 0.19072\n",
      "Epoch: 00 [ 6204/19954 ( 31%)], Train Loss: 0.19040\n",
      "Epoch: 00 [ 6244/19954 ( 31%)], Train Loss: 0.18968\n",
      "Epoch: 00 [ 6284/19954 ( 31%)], Train Loss: 0.18926\n",
      "Epoch: 00 [ 6324/19954 ( 32%)], Train Loss: 0.18861\n",
      "Epoch: 00 [ 6364/19954 ( 32%)], Train Loss: 0.18807\n",
      "Epoch: 00 [ 6404/19954 ( 32%)], Train Loss: 0.18726\n",
      "Epoch: 00 [ 6444/19954 ( 32%)], Train Loss: 0.18691\n",
      "Epoch: 00 [ 6484/19954 ( 32%)], Train Loss: 0.18611\n",
      "Epoch: 00 [ 6524/19954 ( 33%)], Train Loss: 0.18568\n",
      "Epoch: 00 [ 6564/19954 ( 33%)], Train Loss: 0.18513\n",
      "Epoch: 00 [ 6604/19954 ( 33%)], Train Loss: 0.18430\n",
      "Epoch: 00 [ 6644/19954 ( 33%)], Train Loss: 0.18382\n",
      "Epoch: 00 [ 6684/19954 ( 33%)], Train Loss: 0.18309\n",
      "Epoch: 00 [ 6724/19954 ( 34%)], Train Loss: 0.18252\n",
      "Epoch: 00 [ 6764/19954 ( 34%)], Train Loss: 0.18215\n",
      "Epoch: 00 [ 6804/19954 ( 34%)], Train Loss: 0.18162\n",
      "Epoch: 00 [ 6844/19954 ( 34%)], Train Loss: 0.18130\n",
      "Epoch: 00 [ 6884/19954 ( 34%)], Train Loss: 0.18073\n",
      "Epoch: 00 [ 6924/19954 ( 35%)], Train Loss: 0.18031\n",
      "Epoch: 00 [ 6964/19954 ( 35%)], Train Loss: 0.17977\n",
      "Epoch: 00 [ 7004/19954 ( 35%)], Train Loss: 0.17910\n",
      "Epoch: 00 [ 7044/19954 ( 35%)], Train Loss: 0.17858\n",
      "Epoch: 00 [ 7084/19954 ( 36%)], Train Loss: 0.17798\n",
      "Epoch: 00 [ 7124/19954 ( 36%)], Train Loss: 0.17783\n",
      "Epoch: 00 [ 7164/19954 ( 36%)], Train Loss: 0.17782\n",
      "Epoch: 00 [ 7204/19954 ( 36%)], Train Loss: 0.17732\n",
      "Epoch: 00 [ 7244/19954 ( 36%)], Train Loss: 0.17684\n",
      "Epoch: 00 [ 7284/19954 ( 37%)], Train Loss: 0.17662\n",
      "Epoch: 00 [ 7324/19954 ( 37%)], Train Loss: 0.17610\n",
      "Epoch: 00 [ 7364/19954 ( 37%)], Train Loss: 0.17579\n",
      "Epoch: 00 [ 7404/19954 ( 37%)], Train Loss: 0.17541\n",
      "Epoch: 00 [ 7444/19954 ( 37%)], Train Loss: 0.17497\n",
      "Epoch: 00 [ 7484/19954 ( 38%)], Train Loss: 0.17448\n",
      "Epoch: 00 [ 7524/19954 ( 38%)], Train Loss: 0.17416\n",
      "Epoch: 00 [ 7564/19954 ( 38%)], Train Loss: 0.17385\n",
      "Epoch: 00 [ 7604/19954 ( 38%)], Train Loss: 0.17339\n",
      "Epoch: 00 [ 7644/19954 ( 38%)], Train Loss: 0.17287\n",
      "Epoch: 00 [ 7684/19954 ( 39%)], Train Loss: 0.17245\n",
      "Epoch: 00 [ 7724/19954 ( 39%)], Train Loss: 0.17250\n",
      "Epoch: 00 [ 7764/19954 ( 39%)], Train Loss: 0.17219\n",
      "Epoch: 00 [ 7804/19954 ( 39%)], Train Loss: 0.17167\n",
      "Epoch: 00 [ 7844/19954 ( 39%)], Train Loss: 0.17134\n",
      "Epoch: 00 [ 7884/19954 ( 40%)], Train Loss: 0.17091\n",
      "Epoch: 00 [ 7924/19954 ( 40%)], Train Loss: 0.17053\n",
      "Epoch: 00 [ 7964/19954 ( 40%)], Train Loss: 0.17014\n",
      "Epoch: 00 [ 8004/19954 ( 40%)], Train Loss: 0.16958\n",
      "Epoch: 00 [ 8044/19954 ( 40%)], Train Loss: 0.16931\n",
      "Epoch: 00 [ 8084/19954 ( 41%)], Train Loss: 0.16886\n",
      "Epoch: 00 [ 8124/19954 ( 41%)], Train Loss: 0.16839\n",
      "Epoch: 00 [ 8164/19954 ( 41%)], Train Loss: 0.16817\n",
      "Epoch: 00 [ 8204/19954 ( 41%)], Train Loss: 0.16765\n",
      "Epoch: 00 [ 8244/19954 ( 41%)], Train Loss: 0.16738\n",
      "Epoch: 00 [ 8284/19954 ( 42%)], Train Loss: 0.16700\n",
      "Epoch: 00 [ 8324/19954 ( 42%)], Train Loss: 0.16657\n",
      "Epoch: 00 [ 8364/19954 ( 42%)], Train Loss: 0.16611\n",
      "Epoch: 00 [ 8404/19954 ( 42%)], Train Loss: 0.16602\n",
      "Epoch: 00 [ 8444/19954 ( 42%)], Train Loss: 0.16572\n",
      "Epoch: 00 [ 8484/19954 ( 43%)], Train Loss: 0.16537\n",
      "Epoch: 00 [ 8524/19954 ( 43%)], Train Loss: 0.16501\n",
      "Epoch: 00 [ 8564/19954 ( 43%)], Train Loss: 0.16484\n",
      "Epoch: 00 [ 8604/19954 ( 43%)], Train Loss: 0.16442\n",
      "Epoch: 00 [ 8644/19954 ( 43%)], Train Loss: 0.16398\n",
      "Epoch: 00 [ 8684/19954 ( 44%)], Train Loss: 0.16371\n",
      "Epoch: 00 [ 8724/19954 ( 44%)], Train Loss: 0.16355\n",
      "Epoch: 00 [ 8764/19954 ( 44%)], Train Loss: 0.16318\n",
      "Epoch: 00 [ 8804/19954 ( 44%)], Train Loss: 0.16270\n",
      "Epoch: 00 [ 8844/19954 ( 44%)], Train Loss: 0.16226\n",
      "Epoch: 00 [ 8884/19954 ( 45%)], Train Loss: 0.16198\n",
      "Epoch: 00 [ 8924/19954 ( 45%)], Train Loss: 0.16149\n",
      "Epoch: 00 [ 8964/19954 ( 45%)], Train Loss: 0.16105\n",
      "Epoch: 00 [ 9004/19954 ( 45%)], Train Loss: 0.16081\n",
      "Epoch: 00 [ 9044/19954 ( 45%)], Train Loss: 0.16066\n",
      "Epoch: 00 [ 9084/19954 ( 46%)], Train Loss: 0.16036\n",
      "Epoch: 00 [ 9124/19954 ( 46%)], Train Loss: 0.15994\n",
      "Epoch: 00 [ 9164/19954 ( 46%)], Train Loss: 0.15942\n",
      "Epoch: 00 [ 9204/19954 ( 46%)], Train Loss: 0.15915\n",
      "Epoch: 00 [ 9244/19954 ( 46%)], Train Loss: 0.15899\n",
      "Epoch: 00 [ 9284/19954 ( 47%)], Train Loss: 0.15874\n",
      "Epoch: 00 [ 9324/19954 ( 47%)], Train Loss: 0.15834\n",
      "Epoch: 00 [ 9364/19954 ( 47%)], Train Loss: 0.15816\n",
      "Epoch: 00 [ 9404/19954 ( 47%)], Train Loss: 0.15806\n",
      "Epoch: 00 [ 9444/19954 ( 47%)], Train Loss: 0.15785\n",
      "Epoch: 00 [ 9484/19954 ( 48%)], Train Loss: 0.15768\n",
      "Epoch: 00 [ 9524/19954 ( 48%)], Train Loss: 0.15726\n",
      "Epoch: 00 [ 9564/19954 ( 48%)], Train Loss: 0.15675\n",
      "Epoch: 00 [ 9604/19954 ( 48%)], Train Loss: 0.15653\n",
      "Epoch: 00 [ 9644/19954 ( 48%)], Train Loss: 0.15621\n",
      "Epoch: 00 [ 9684/19954 ( 49%)], Train Loss: 0.15593\n",
      "Epoch: 00 [ 9724/19954 ( 49%)], Train Loss: 0.15569\n",
      "Epoch: 00 [ 9764/19954 ( 49%)], Train Loss: 0.15531\n",
      "Epoch: 00 [ 9804/19954 ( 49%)], Train Loss: 0.15495\n",
      "Epoch: 00 [ 9844/19954 ( 49%)], Train Loss: 0.15466\n",
      "Epoch: 00 [ 9884/19954 ( 50%)], Train Loss: 0.15432\n",
      "Epoch: 00 [ 9924/19954 ( 50%)], Train Loss: 0.15398\n",
      "Epoch: 00 [ 9964/19954 ( 50%)], Train Loss: 0.15351\n",
      "Epoch: 00 [10004/19954 ( 50%)], Train Loss: 0.15341\n",
      "Epoch: 00 [10044/19954 ( 50%)], Train Loss: 0.15305\n",
      "Epoch: 00 [10084/19954 ( 51%)], Train Loss: 0.15289\n",
      "Epoch: 00 [10124/19954 ( 51%)], Train Loss: 0.15260\n",
      "Epoch: 00 [10164/19954 ( 51%)], Train Loss: 0.15257\n",
      "Epoch: 00 [10204/19954 ( 51%)], Train Loss: 0.15253\n",
      "Epoch: 00 [10244/19954 ( 51%)], Train Loss: 0.15234\n",
      "Epoch: 00 [10284/19954 ( 52%)], Train Loss: 0.15199\n",
      "Epoch: 00 [10324/19954 ( 52%)], Train Loss: 0.15180\n",
      "Epoch: 00 [10364/19954 ( 52%)], Train Loss: 0.15164\n",
      "Epoch: 00 [10404/19954 ( 52%)], Train Loss: 0.15146\n",
      "Epoch: 00 [10444/19954 ( 52%)], Train Loss: 0.15138\n",
      "Epoch: 00 [10484/19954 ( 53%)], Train Loss: 0.15132\n",
      "Epoch: 00 [10524/19954 ( 53%)], Train Loss: 0.15111\n",
      "Epoch: 00 [10564/19954 ( 53%)], Train Loss: 0.15081\n",
      "Epoch: 00 [10604/19954 ( 53%)], Train Loss: 0.15066\n",
      "Epoch: 00 [10644/19954 ( 53%)], Train Loss: 0.15043\n",
      "Epoch: 00 [10684/19954 ( 54%)], Train Loss: 0.15008\n",
      "Epoch: 00 [10724/19954 ( 54%)], Train Loss: 0.14994\n",
      "Epoch: 00 [10764/19954 ( 54%)], Train Loss: 0.14976\n",
      "Epoch: 00 [10804/19954 ( 54%)], Train Loss: 0.14977\n",
      "Epoch: 00 [10844/19954 ( 54%)], Train Loss: 0.14953\n",
      "Epoch: 00 [10884/19954 ( 55%)], Train Loss: 0.14941\n",
      "Epoch: 00 [10924/19954 ( 55%)], Train Loss: 0.14918\n",
      "Epoch: 00 [10964/19954 ( 55%)], Train Loss: 0.14881\n",
      "Epoch: 00 [11004/19954 ( 55%)], Train Loss: 0.14869\n",
      "Epoch: 00 [11044/19954 ( 55%)], Train Loss: 0.14848\n",
      "Epoch: 00 [11084/19954 ( 56%)], Train Loss: 0.14830\n",
      "Epoch: 00 [11124/19954 ( 56%)], Train Loss: 0.14808\n",
      "Epoch: 00 [11164/19954 ( 56%)], Train Loss: 0.14775\n",
      "Epoch: 00 [11204/19954 ( 56%)], Train Loss: 0.14765\n",
      "Epoch: 00 [11244/19954 ( 56%)], Train Loss: 0.14741\n",
      "Epoch: 00 [11284/19954 ( 57%)], Train Loss: 0.14706\n",
      "Epoch: 00 [11324/19954 ( 57%)], Train Loss: 0.14693\n",
      "Epoch: 00 [11364/19954 ( 57%)], Train Loss: 0.14677\n",
      "Epoch: 00 [11404/19954 ( 57%)], Train Loss: 0.14661\n",
      "Epoch: 00 [11444/19954 ( 57%)], Train Loss: 0.14640\n",
      "Epoch: 00 [11484/19954 ( 58%)], Train Loss: 0.14613\n",
      "Epoch: 00 [11524/19954 ( 58%)], Train Loss: 0.14592\n",
      "Epoch: 00 [11564/19954 ( 58%)], Train Loss: 0.14566\n",
      "Epoch: 00 [11604/19954 ( 58%)], Train Loss: 0.14567\n",
      "Epoch: 00 [11644/19954 ( 58%)], Train Loss: 0.14542\n",
      "Epoch: 00 [11684/19954 ( 59%)], Train Loss: 0.14526\n",
      "Epoch: 00 [11724/19954 ( 59%)], Train Loss: 0.14504\n",
      "Epoch: 00 [11764/19954 ( 59%)], Train Loss: 0.14482\n",
      "Epoch: 00 [11804/19954 ( 59%)], Train Loss: 0.14457\n",
      "Epoch: 00 [11844/19954 ( 59%)], Train Loss: 0.14427\n",
      "Epoch: 00 [11884/19954 ( 60%)], Train Loss: 0.14419\n",
      "Epoch: 00 [11924/19954 ( 60%)], Train Loss: 0.14403\n",
      "Epoch: 00 [11964/19954 ( 60%)], Train Loss: 0.14378\n",
      "Epoch: 00 [12004/19954 ( 60%)], Train Loss: 0.14354\n",
      "Epoch: 00 [12044/19954 ( 60%)], Train Loss: 0.14342\n",
      "Epoch: 00 [12084/19954 ( 61%)], Train Loss: 0.14333\n",
      "Epoch: 00 [12124/19954 ( 61%)], Train Loss: 0.14324\n",
      "Epoch: 00 [12164/19954 ( 61%)], Train Loss: 0.14297\n",
      "Epoch: 00 [12204/19954 ( 61%)], Train Loss: 0.14289\n",
      "Epoch: 00 [12244/19954 ( 61%)], Train Loss: 0.14274\n",
      "Epoch: 00 [12284/19954 ( 62%)], Train Loss: 0.14254\n",
      "Epoch: 00 [12324/19954 ( 62%)], Train Loss: 0.14227\n",
      "Epoch: 00 [12364/19954 ( 62%)], Train Loss: 0.14205\n",
      "Epoch: 00 [12404/19954 ( 62%)], Train Loss: 0.14177\n",
      "Epoch: 00 [12444/19954 ( 62%)], Train Loss: 0.14177\n",
      "Epoch: 00 [12484/19954 ( 63%)], Train Loss: 0.14153\n",
      "Epoch: 00 [12524/19954 ( 63%)], Train Loss: 0.14142\n",
      "Epoch: 00 [12564/19954 ( 63%)], Train Loss: 0.14118\n",
      "Epoch: 00 [12604/19954 ( 63%)], Train Loss: 0.14086\n",
      "Epoch: 00 [12644/19954 ( 63%)], Train Loss: 0.14064\n",
      "Epoch: 00 [12684/19954 ( 64%)], Train Loss: 0.14033\n",
      "Epoch: 00 [12724/19954 ( 64%)], Train Loss: 0.14016\n",
      "Epoch: 00 [12764/19954 ( 64%)], Train Loss: 0.14015\n",
      "Epoch: 00 [12804/19954 ( 64%)], Train Loss: 0.14006\n",
      "Epoch: 00 [12844/19954 ( 64%)], Train Loss: 0.13983\n",
      "Epoch: 00 [12884/19954 ( 65%)], Train Loss: 0.13960\n",
      "Epoch: 00 [12924/19954 ( 65%)], Train Loss: 0.13932\n",
      "Epoch: 00 [12964/19954 ( 65%)], Train Loss: 0.13927\n",
      "Epoch: 00 [13004/19954 ( 65%)], Train Loss: 0.13904\n",
      "Epoch: 00 [13044/19954 ( 65%)], Train Loss: 0.13886\n",
      "Epoch: 00 [13084/19954 ( 66%)], Train Loss: 0.13862\n",
      "Epoch: 00 [13124/19954 ( 66%)], Train Loss: 0.13852\n",
      "Epoch: 00 [13164/19954 ( 66%)], Train Loss: 0.13837\n",
      "Epoch: 00 [13204/19954 ( 66%)], Train Loss: 0.13826\n",
      "Epoch: 00 [13244/19954 ( 66%)], Train Loss: 0.13802\n",
      "Epoch: 00 [13284/19954 ( 67%)], Train Loss: 0.13796\n",
      "Epoch: 00 [13324/19954 ( 67%)], Train Loss: 0.13783\n",
      "Epoch: 00 [13364/19954 ( 67%)], Train Loss: 0.13771\n",
      "Epoch: 00 [13404/19954 ( 67%)], Train Loss: 0.13758\n",
      "Epoch: 00 [13444/19954 ( 67%)], Train Loss: 0.13753\n",
      "Epoch: 00 [13484/19954 ( 68%)], Train Loss: 0.13738\n",
      "Epoch: 00 [13524/19954 ( 68%)], Train Loss: 0.13729\n",
      "Epoch: 00 [13564/19954 ( 68%)], Train Loss: 0.13742\n",
      "Epoch: 00 [13604/19954 ( 68%)], Train Loss: 0.13744\n",
      "Epoch: 00 [13644/19954 ( 68%)], Train Loss: 0.13731\n",
      "Epoch: 00 [13684/19954 ( 69%)], Train Loss: 0.13724\n",
      "Epoch: 00 [13724/19954 ( 69%)], Train Loss: 0.13704\n",
      "Epoch: 00 [13764/19954 ( 69%)], Train Loss: 0.13687\n",
      "Epoch: 00 [13804/19954 ( 69%)], Train Loss: 0.13672\n",
      "Epoch: 00 [13844/19954 ( 69%)], Train Loss: 0.13652\n",
      "Epoch: 00 [13884/19954 ( 70%)], Train Loss: 0.13648\n",
      "Epoch: 00 [13924/19954 ( 70%)], Train Loss: 0.13634\n",
      "Epoch: 00 [13964/19954 ( 70%)], Train Loss: 0.13622\n",
      "Epoch: 00 [14004/19954 ( 70%)], Train Loss: 0.13623\n",
      "Epoch: 00 [14044/19954 ( 70%)], Train Loss: 0.13619\n",
      "Epoch: 00 [14084/19954 ( 71%)], Train Loss: 0.13622\n",
      "Epoch: 00 [14124/19954 ( 71%)], Train Loss: 0.13618\n",
      "Epoch: 00 [14164/19954 ( 71%)], Train Loss: 0.13594\n",
      "Epoch: 00 [14204/19954 ( 71%)], Train Loss: 0.13580\n",
      "Epoch: 00 [14244/19954 ( 71%)], Train Loss: 0.13571\n",
      "Epoch: 00 [14284/19954 ( 72%)], Train Loss: 0.13559\n",
      "Epoch: 00 [14324/19954 ( 72%)], Train Loss: 0.13540\n",
      "Epoch: 00 [14364/19954 ( 72%)], Train Loss: 0.13540\n",
      "Epoch: 00 [14404/19954 ( 72%)], Train Loss: 0.13519\n",
      "Epoch: 00 [14444/19954 ( 72%)], Train Loss: 0.13499\n",
      "Epoch: 00 [14484/19954 ( 73%)], Train Loss: 0.13482\n",
      "Epoch: 00 [14524/19954 ( 73%)], Train Loss: 0.13460\n",
      "Epoch: 00 [14564/19954 ( 73%)], Train Loss: 0.13447\n",
      "Epoch: 00 [14604/19954 ( 73%)], Train Loss: 0.13429\n",
      "Epoch: 00 [14644/19954 ( 73%)], Train Loss: 0.13424\n",
      "Epoch: 00 [14684/19954 ( 74%)], Train Loss: 0.13402\n",
      "Epoch: 00 [14724/19954 ( 74%)], Train Loss: 0.13377\n",
      "Epoch: 00 [14764/19954 ( 74%)], Train Loss: 0.13361\n",
      "Epoch: 00 [14804/19954 ( 74%)], Train Loss: 0.13342\n",
      "Epoch: 00 [14844/19954 ( 74%)], Train Loss: 0.13327\n",
      "Epoch: 00 [14884/19954 ( 75%)], Train Loss: 0.13306\n",
      "Epoch: 00 [14924/19954 ( 75%)], Train Loss: 0.13311\n",
      "Epoch: 00 [14964/19954 ( 75%)], Train Loss: 0.13287\n",
      "Epoch: 00 [15004/19954 ( 75%)], Train Loss: 0.13281\n",
      "Epoch: 00 [15044/19954 ( 75%)], Train Loss: 0.13265\n",
      "Epoch: 00 [15084/19954 ( 76%)], Train Loss: 0.13251\n",
      "Epoch: 00 [15124/19954 ( 76%)], Train Loss: 0.13235\n",
      "Epoch: 00 [15164/19954 ( 76%)], Train Loss: 0.13213\n",
      "Epoch: 00 [15204/19954 ( 76%)], Train Loss: 0.13202\n",
      "Epoch: 00 [15244/19954 ( 76%)], Train Loss: 0.13199\n",
      "Epoch: 00 [15284/19954 ( 77%)], Train Loss: 0.13192\n",
      "Epoch: 00 [15324/19954 ( 77%)], Train Loss: 0.13180\n",
      "Epoch: 00 [15364/19954 ( 77%)], Train Loss: 0.13160\n",
      "Epoch: 00 [15404/19954 ( 77%)], Train Loss: 0.13140\n",
      "Epoch: 00 [15444/19954 ( 77%)], Train Loss: 0.13134\n",
      "Epoch: 00 [15484/19954 ( 78%)], Train Loss: 0.13131\n",
      "Epoch: 00 [15524/19954 ( 78%)], Train Loss: 0.13111\n",
      "Epoch: 00 [15564/19954 ( 78%)], Train Loss: 0.13097\n",
      "Epoch: 00 [15604/19954 ( 78%)], Train Loss: 0.13091\n",
      "Epoch: 00 [15644/19954 ( 78%)], Train Loss: 0.13069\n",
      "Epoch: 00 [15684/19954 ( 79%)], Train Loss: 0.13064\n",
      "Epoch: 00 [15724/19954 ( 79%)], Train Loss: 0.13056\n",
      "Epoch: 00 [15764/19954 ( 79%)], Train Loss: 0.13036\n",
      "Epoch: 00 [15804/19954 ( 79%)], Train Loss: 0.13018\n",
      "Epoch: 00 [15844/19954 ( 79%)], Train Loss: 0.12999\n",
      "Epoch: 00 [15884/19954 ( 80%)], Train Loss: 0.12986\n",
      "Epoch: 00 [15924/19954 ( 80%)], Train Loss: 0.12967\n",
      "Epoch: 00 [15964/19954 ( 80%)], Train Loss: 0.12951\n",
      "Epoch: 00 [16004/19954 ( 80%)], Train Loss: 0.12940\n",
      "Epoch: 00 [16044/19954 ( 80%)], Train Loss: 0.12946\n",
      "Epoch: 00 [16084/19954 ( 81%)], Train Loss: 0.12923\n",
      "Epoch: 00 [16124/19954 ( 81%)], Train Loss: 0.12921\n",
      "Epoch: 00 [16164/19954 ( 81%)], Train Loss: 0.12919\n",
      "Epoch: 00 [16204/19954 ( 81%)], Train Loss: 0.12909\n",
      "Epoch: 00 [16244/19954 ( 81%)], Train Loss: 0.12899\n",
      "Epoch: 00 [16284/19954 ( 82%)], Train Loss: 0.12887\n",
      "Epoch: 00 [16324/19954 ( 82%)], Train Loss: 0.12884\n",
      "Epoch: 00 [16364/19954 ( 82%)], Train Loss: 0.12873\n",
      "Epoch: 00 [16404/19954 ( 82%)], Train Loss: 0.12860\n",
      "Epoch: 00 [16444/19954 ( 82%)], Train Loss: 0.12842\n",
      "Epoch: 00 [16484/19954 ( 83%)], Train Loss: 0.12835\n",
      "Epoch: 00 [16524/19954 ( 83%)], Train Loss: 0.12829\n",
      "Epoch: 00 [16564/19954 ( 83%)], Train Loss: 0.12806\n",
      "Epoch: 00 [16604/19954 ( 83%)], Train Loss: 0.12794\n",
      "Epoch: 00 [16644/19954 ( 83%)], Train Loss: 0.12778\n",
      "Epoch: 00 [16684/19954 ( 84%)], Train Loss: 0.12764\n",
      "Epoch: 00 [16724/19954 ( 84%)], Train Loss: 0.12754\n",
      "Epoch: 00 [16764/19954 ( 84%)], Train Loss: 0.12734\n",
      "Epoch: 00 [16804/19954 ( 84%)], Train Loss: 0.12738\n",
      "Epoch: 00 [16844/19954 ( 84%)], Train Loss: 0.12717\n",
      "Epoch: 00 [16884/19954 ( 85%)], Train Loss: 0.12713\n",
      "Epoch: 00 [16924/19954 ( 85%)], Train Loss: 0.12708\n",
      "Epoch: 00 [16964/19954 ( 85%)], Train Loss: 0.12698\n",
      "Epoch: 00 [17004/19954 ( 85%)], Train Loss: 0.12687\n",
      "Epoch: 00 [17044/19954 ( 85%)], Train Loss: 0.12677\n",
      "Epoch: 00 [17084/19954 ( 86%)], Train Loss: 0.12661\n",
      "Epoch: 00 [17124/19954 ( 86%)], Train Loss: 0.12649\n",
      "Epoch: 00 [17164/19954 ( 86%)], Train Loss: 0.12635\n",
      "Epoch: 00 [17204/19954 ( 86%)], Train Loss: 0.12617\n",
      "Epoch: 00 [17244/19954 ( 86%)], Train Loss: 0.12603\n",
      "Epoch: 00 [17284/19954 ( 87%)], Train Loss: 0.12597\n",
      "Epoch: 00 [17324/19954 ( 87%)], Train Loss: 0.12592\n",
      "Epoch: 00 [17364/19954 ( 87%)], Train Loss: 0.12576\n",
      "Epoch: 00 [17404/19954 ( 87%)], Train Loss: 0.12561\n",
      "Epoch: 00 [17444/19954 ( 87%)], Train Loss: 0.12545\n",
      "Epoch: 00 [17484/19954 ( 88%)], Train Loss: 0.12529\n",
      "Epoch: 00 [17524/19954 ( 88%)], Train Loss: 0.12524\n",
      "Epoch: 00 [17564/19954 ( 88%)], Train Loss: 0.12511\n",
      "Epoch: 00 [17604/19954 ( 88%)], Train Loss: 0.12504\n",
      "Epoch: 00 [17644/19954 ( 88%)], Train Loss: 0.12496\n",
      "Epoch: 00 [17684/19954 ( 89%)], Train Loss: 0.12500\n",
      "Epoch: 00 [17724/19954 ( 89%)], Train Loss: 0.12492\n",
      "Epoch: 00 [17764/19954 ( 89%)], Train Loss: 0.12487\n",
      "Epoch: 00 [17804/19954 ( 89%)], Train Loss: 0.12481\n",
      "Epoch: 00 [17844/19954 ( 89%)], Train Loss: 0.12475\n",
      "Epoch: 00 [17884/19954 ( 90%)], Train Loss: 0.12481\n",
      "Epoch: 00 [17924/19954 ( 90%)], Train Loss: 0.12480\n",
      "Epoch: 00 [17964/19954 ( 90%)], Train Loss: 0.12472\n",
      "Epoch: 00 [18004/19954 ( 90%)], Train Loss: 0.12464\n",
      "Epoch: 00 [18044/19954 ( 90%)], Train Loss: 0.12457\n",
      "Epoch: 00 [18084/19954 ( 91%)], Train Loss: 0.12445\n",
      "Epoch: 00 [18124/19954 ( 91%)], Train Loss: 0.12428\n",
      "Epoch: 00 [18164/19954 ( 91%)], Train Loss: 0.12415\n",
      "Epoch: 00 [18204/19954 ( 91%)], Train Loss: 0.12407\n",
      "Epoch: 00 [18244/19954 ( 91%)], Train Loss: 0.12398\n",
      "Epoch: 00 [18284/19954 ( 92%)], Train Loss: 0.12391\n",
      "Epoch: 00 [18324/19954 ( 92%)], Train Loss: 0.12388\n",
      "Epoch: 00 [18364/19954 ( 92%)], Train Loss: 0.12368\n",
      "Epoch: 00 [18404/19954 ( 92%)], Train Loss: 0.12360\n",
      "Epoch: 00 [18444/19954 ( 92%)], Train Loss: 0.12356\n",
      "Epoch: 00 [18484/19954 ( 93%)], Train Loss: 0.12356\n",
      "Epoch: 00 [18524/19954 ( 93%)], Train Loss: 0.12350\n",
      "Epoch: 00 [18564/19954 ( 93%)], Train Loss: 0.12339\n",
      "Epoch: 00 [18604/19954 ( 93%)], Train Loss: 0.12329\n",
      "Epoch: 00 [18644/19954 ( 93%)], Train Loss: 0.12318\n",
      "Epoch: 00 [18684/19954 ( 94%)], Train Loss: 0.12314\n",
      "Epoch: 00 [18724/19954 ( 94%)], Train Loss: 0.12305\n",
      "Epoch: 00 [18764/19954 ( 94%)], Train Loss: 0.12293\n",
      "Epoch: 00 [18804/19954 ( 94%)], Train Loss: 0.12292\n",
      "Epoch: 00 [18844/19954 ( 94%)], Train Loss: 0.12276\n",
      "Epoch: 00 [18884/19954 ( 95%)], Train Loss: 0.12263\n",
      "Epoch: 00 [18924/19954 ( 95%)], Train Loss: 0.12249\n",
      "Epoch: 00 [18964/19954 ( 95%)], Train Loss: 0.12236\n",
      "Epoch: 00 [19004/19954 ( 95%)], Train Loss: 0.12224\n",
      "Epoch: 00 [19044/19954 ( 95%)], Train Loss: 0.12218\n",
      "Epoch: 00 [19084/19954 ( 96%)], Train Loss: 0.12213\n",
      "Epoch: 00 [19124/19954 ( 96%)], Train Loss: 0.12219\n",
      "Epoch: 00 [19164/19954 ( 96%)], Train Loss: 0.12214\n",
      "Epoch: 00 [19204/19954 ( 96%)], Train Loss: 0.12196\n",
      "Epoch: 00 [19244/19954 ( 96%)], Train Loss: 0.12186\n",
      "Epoch: 00 [19284/19954 ( 97%)], Train Loss: 0.12176\n",
      "Epoch: 00 [19324/19954 ( 97%)], Train Loss: 0.12168\n",
      "Epoch: 00 [19364/19954 ( 97%)], Train Loss: 0.12166\n",
      "Epoch: 00 [19404/19954 ( 97%)], Train Loss: 0.12154\n",
      "Epoch: 00 [19444/19954 ( 97%)], Train Loss: 0.12142\n",
      "Epoch: 00 [19484/19954 ( 98%)], Train Loss: 0.12140\n",
      "Epoch: 00 [19524/19954 ( 98%)], Train Loss: 0.12146\n",
      "Epoch: 00 [19564/19954 ( 98%)], Train Loss: 0.12140\n",
      "Epoch: 00 [19604/19954 ( 98%)], Train Loss: 0.12137\n",
      "Epoch: 00 [19644/19954 ( 98%)], Train Loss: 0.12127\n",
      "Epoch: 00 [19684/19954 ( 99%)], Train Loss: 0.12114\n",
      "Epoch: 00 [19724/19954 ( 99%)], Train Loss: 0.12101\n",
      "Epoch: 00 [19764/19954 ( 99%)], Train Loss: 0.12097\n",
      "Epoch: 00 [19804/19954 ( 99%)], Train Loss: 0.12092\n",
      "Epoch: 00 [19844/19954 ( 99%)], Train Loss: 0.12081\n",
      "Epoch: 00 [19884/19954 (100%)], Train Loss: 0.12072\n",
      "Epoch: 00 [19924/19954 (100%)], Train Loss: 0.12061\n",
      "Epoch: 00 [19954/19954 (100%)], Train Loss: 0.12066\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.19920\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.19920\n",
      "Saving model checkpoint to output/checkpoint-fold-2.\n",
      "\n",
      "Epoch: 01 [    4/19954 (  0%)], Train Loss: 0.11133\n",
      "Epoch: 01 [   44/19954 (  0%)], Train Loss: 0.07271\n",
      "Epoch: 01 [   84/19954 (  0%)], Train Loss: 0.07671\n",
      "Epoch: 01 [  124/19954 (  1%)], Train Loss: 0.07505\n",
      "Epoch: 01 [  164/19954 (  1%)], Train Loss: 0.07981\n",
      "Epoch: 01 [  204/19954 (  1%)], Train Loss: 0.09269\n",
      "Epoch: 01 [  244/19954 (  1%)], Train Loss: 0.09324\n",
      "Epoch: 01 [  284/19954 (  1%)], Train Loss: 0.09030\n",
      "Epoch: 01 [  324/19954 (  2%)], Train Loss: 0.08665\n",
      "Epoch: 01 [  364/19954 (  2%)], Train Loss: 0.08940\n",
      "Epoch: 01 [  404/19954 (  2%)], Train Loss: 0.08912\n",
      "Epoch: 01 [  444/19954 (  2%)], Train Loss: 0.08901\n",
      "Epoch: 01 [  484/19954 (  2%)], Train Loss: 0.08766\n",
      "Epoch: 01 [  524/19954 (  3%)], Train Loss: 0.08650\n",
      "Epoch: 01 [  564/19954 (  3%)], Train Loss: 0.08271\n",
      "Epoch: 01 [  604/19954 (  3%)], Train Loss: 0.08196\n",
      "Epoch: 01 [  644/19954 (  3%)], Train Loss: 0.08275\n",
      "Epoch: 01 [  684/19954 (  3%)], Train Loss: 0.08138\n",
      "Epoch: 01 [  724/19954 (  4%)], Train Loss: 0.08071\n",
      "Epoch: 01 [  764/19954 (  4%)], Train Loss: 0.08022\n",
      "Epoch: 01 [  804/19954 (  4%)], Train Loss: 0.07917\n",
      "Epoch: 01 [  844/19954 (  4%)], Train Loss: 0.07799\n",
      "Epoch: 01 [  884/19954 (  4%)], Train Loss: 0.07709\n",
      "Epoch: 01 [  924/19954 (  5%)], Train Loss: 0.07560\n",
      "Epoch: 01 [  964/19954 (  5%)], Train Loss: 0.07514\n",
      "Epoch: 01 [ 1004/19954 (  5%)], Train Loss: 0.07646\n",
      "Epoch: 01 [ 1044/19954 (  5%)], Train Loss: 0.07728\n",
      "Epoch: 01 [ 1084/19954 (  5%)], Train Loss: 0.07580\n",
      "Epoch: 01 [ 1124/19954 (  6%)], Train Loss: 0.07526\n",
      "Epoch: 01 [ 1164/19954 (  6%)], Train Loss: 0.07576\n",
      "Epoch: 01 [ 1204/19954 (  6%)], Train Loss: 0.07482\n",
      "Epoch: 01 [ 1244/19954 (  6%)], Train Loss: 0.07401\n",
      "Epoch: 01 [ 1284/19954 (  6%)], Train Loss: 0.07322\n",
      "Epoch: 01 [ 1324/19954 (  7%)], Train Loss: 0.07363\n",
      "Epoch: 01 [ 1364/19954 (  7%)], Train Loss: 0.07377\n",
      "Epoch: 01 [ 1404/19954 (  7%)], Train Loss: 0.07288\n",
      "Epoch: 01 [ 1444/19954 (  7%)], Train Loss: 0.07222\n",
      "Epoch: 01 [ 1484/19954 (  7%)], Train Loss: 0.07172\n",
      "Epoch: 01 [ 1524/19954 (  8%)], Train Loss: 0.07169\n",
      "Epoch: 01 [ 1564/19954 (  8%)], Train Loss: 0.07139\n",
      "Epoch: 01 [ 1604/19954 (  8%)], Train Loss: 0.07135\n",
      "Epoch: 01 [ 1644/19954 (  8%)], Train Loss: 0.07058\n",
      "Epoch: 01 [ 1684/19954 (  8%)], Train Loss: 0.07124\n",
      "Epoch: 01 [ 1724/19954 (  9%)], Train Loss: 0.07022\n",
      "Epoch: 01 [ 1764/19954 (  9%)], Train Loss: 0.07002\n",
      "Epoch: 01 [ 1804/19954 (  9%)], Train Loss: 0.06965\n",
      "Epoch: 01 [ 1844/19954 (  9%)], Train Loss: 0.06911\n",
      "Epoch: 01 [ 1884/19954 (  9%)], Train Loss: 0.06811\n",
      "Epoch: 01 [ 1924/19954 ( 10%)], Train Loss: 0.06769\n",
      "Epoch: 01 [ 1964/19954 ( 10%)], Train Loss: 0.06745\n",
      "Epoch: 01 [ 2004/19954 ( 10%)], Train Loss: 0.06708\n",
      "Epoch: 01 [ 2044/19954 ( 10%)], Train Loss: 0.06658\n",
      "Epoch: 01 [ 2084/19954 ( 10%)], Train Loss: 0.06620\n",
      "Epoch: 01 [ 2124/19954 ( 11%)], Train Loss: 0.06617\n",
      "Epoch: 01 [ 2164/19954 ( 11%)], Train Loss: 0.06656\n",
      "Epoch: 01 [ 2204/19954 ( 11%)], Train Loss: 0.06642\n",
      "Epoch: 01 [ 2244/19954 ( 11%)], Train Loss: 0.06612\n",
      "Epoch: 01 [ 2284/19954 ( 11%)], Train Loss: 0.06531\n",
      "Epoch: 01 [ 2324/19954 ( 12%)], Train Loss: 0.06544\n",
      "Epoch: 01 [ 2364/19954 ( 12%)], Train Loss: 0.06618\n",
      "Epoch: 01 [ 2404/19954 ( 12%)], Train Loss: 0.06604\n",
      "Epoch: 01 [ 2444/19954 ( 12%)], Train Loss: 0.06520\n",
      "Epoch: 01 [ 2484/19954 ( 12%)], Train Loss: 0.06464\n",
      "Epoch: 01 [ 2524/19954 ( 13%)], Train Loss: 0.06397\n",
      "Epoch: 01 [ 2564/19954 ( 13%)], Train Loss: 0.06332\n",
      "Epoch: 01 [ 2604/19954 ( 13%)], Train Loss: 0.06339\n",
      "Epoch: 01 [ 2644/19954 ( 13%)], Train Loss: 0.06315\n",
      "Epoch: 01 [ 2684/19954 ( 13%)], Train Loss: 0.06250\n",
      "Epoch: 01 [ 2724/19954 ( 14%)], Train Loss: 0.06206\n",
      "Epoch: 01 [ 2764/19954 ( 14%)], Train Loss: 0.06182\n",
      "Epoch: 01 [ 2804/19954 ( 14%)], Train Loss: 0.06110\n",
      "Epoch: 01 [ 2844/19954 ( 14%)], Train Loss: 0.06085\n",
      "Epoch: 01 [ 2884/19954 ( 14%)], Train Loss: 0.06043\n",
      "Epoch: 01 [ 2924/19954 ( 15%)], Train Loss: 0.06021\n",
      "Epoch: 01 [ 2964/19954 ( 15%)], Train Loss: 0.05987\n",
      "Epoch: 01 [ 3004/19954 ( 15%)], Train Loss: 0.05950\n",
      "Epoch: 01 [ 3044/19954 ( 15%)], Train Loss: 0.05900\n",
      "Epoch: 01 [ 3084/19954 ( 15%)], Train Loss: 0.05857\n",
      "Epoch: 01 [ 3124/19954 ( 16%)], Train Loss: 0.05821\n",
      "Epoch: 01 [ 3164/19954 ( 16%)], Train Loss: 0.05811\n",
      "Epoch: 01 [ 3204/19954 ( 16%)], Train Loss: 0.05760\n",
      "Epoch: 01 [ 3244/19954 ( 16%)], Train Loss: 0.05765\n",
      "Epoch: 01 [ 3284/19954 ( 16%)], Train Loss: 0.05762\n",
      "Epoch: 01 [ 3324/19954 ( 17%)], Train Loss: 0.05724\n",
      "Epoch: 01 [ 3364/19954 ( 17%)], Train Loss: 0.05700\n",
      "Epoch: 01 [ 3404/19954 ( 17%)], Train Loss: 0.05691\n",
      "Epoch: 01 [ 3444/19954 ( 17%)], Train Loss: 0.05646\n",
      "Epoch: 01 [ 3484/19954 ( 17%)], Train Loss: 0.05611\n",
      "Epoch: 01 [ 3524/19954 ( 18%)], Train Loss: 0.05567\n",
      "Epoch: 01 [ 3564/19954 ( 18%)], Train Loss: 0.05547\n",
      "Epoch: 01 [ 3604/19954 ( 18%)], Train Loss: 0.05534\n",
      "Epoch: 01 [ 3644/19954 ( 18%)], Train Loss: 0.05500\n",
      "Epoch: 01 [ 3684/19954 ( 18%)], Train Loss: 0.05468\n",
      "Epoch: 01 [ 3724/19954 ( 19%)], Train Loss: 0.05457\n",
      "Epoch: 01 [ 3764/19954 ( 19%)], Train Loss: 0.05424\n",
      "Epoch: 01 [ 3804/19954 ( 19%)], Train Loss: 0.05397\n",
      "Epoch: 01 [ 3844/19954 ( 19%)], Train Loss: 0.05378\n",
      "Epoch: 01 [ 3884/19954 ( 19%)], Train Loss: 0.05396\n",
      "Epoch: 01 [ 3924/19954 ( 20%)], Train Loss: 0.05373\n",
      "Epoch: 01 [ 3964/19954 ( 20%)], Train Loss: 0.05372\n",
      "Epoch: 01 [ 4004/19954 ( 20%)], Train Loss: 0.05344\n",
      "Epoch: 01 [ 4044/19954 ( 20%)], Train Loss: 0.05325\n",
      "Epoch: 01 [ 4084/19954 ( 20%)], Train Loss: 0.05294\n",
      "Epoch: 01 [ 4124/19954 ( 21%)], Train Loss: 0.05298\n",
      "Epoch: 01 [ 4164/19954 ( 21%)], Train Loss: 0.05305\n",
      "Epoch: 01 [ 4204/19954 ( 21%)], Train Loss: 0.05282\n",
      "Epoch: 01 [ 4244/19954 ( 21%)], Train Loss: 0.05288\n",
      "Epoch: 01 [ 4284/19954 ( 21%)], Train Loss: 0.05286\n",
      "Epoch: 01 [ 4324/19954 ( 22%)], Train Loss: 0.05262\n",
      "Epoch: 01 [ 4364/19954 ( 22%)], Train Loss: 0.05243\n",
      "Epoch: 01 [ 4404/19954 ( 22%)], Train Loss: 0.05236\n",
      "Epoch: 01 [ 4444/19954 ( 22%)], Train Loss: 0.05230\n",
      "Epoch: 01 [ 4484/19954 ( 22%)], Train Loss: 0.05209\n",
      "Epoch: 01 [ 4524/19954 ( 23%)], Train Loss: 0.05233\n",
      "Epoch: 01 [ 4564/19954 ( 23%)], Train Loss: 0.05215\n",
      "Epoch: 01 [ 4604/19954 ( 23%)], Train Loss: 0.05195\n",
      "Epoch: 01 [ 4644/19954 ( 23%)], Train Loss: 0.05194\n",
      "Epoch: 01 [ 4684/19954 ( 23%)], Train Loss: 0.05187\n",
      "Epoch: 01 [ 4724/19954 ( 24%)], Train Loss: 0.05181\n",
      "Epoch: 01 [ 4764/19954 ( 24%)], Train Loss: 0.05161\n",
      "Epoch: 01 [ 4804/19954 ( 24%)], Train Loss: 0.05165\n",
      "Epoch: 01 [ 4844/19954 ( 24%)], Train Loss: 0.05142\n",
      "Epoch: 01 [ 4884/19954 ( 24%)], Train Loss: 0.05140\n",
      "Epoch: 01 [ 4924/19954 ( 25%)], Train Loss: 0.05132\n",
      "Epoch: 01 [ 4964/19954 ( 25%)], Train Loss: 0.05130\n",
      "Epoch: 01 [ 5004/19954 ( 25%)], Train Loss: 0.05099\n",
      "Epoch: 01 [ 5044/19954 ( 25%)], Train Loss: 0.05079\n",
      "Epoch: 01 [ 5084/19954 ( 25%)], Train Loss: 0.05052\n",
      "Epoch: 01 [ 5124/19954 ( 26%)], Train Loss: 0.05033\n",
      "Epoch: 01 [ 5164/19954 ( 26%)], Train Loss: 0.05026\n",
      "Epoch: 01 [ 5204/19954 ( 26%)], Train Loss: 0.05009\n",
      "Epoch: 01 [ 5244/19954 ( 26%)], Train Loss: 0.04989\n",
      "Epoch: 01 [ 5284/19954 ( 26%)], Train Loss: 0.04975\n",
      "Epoch: 01 [ 5324/19954 ( 27%)], Train Loss: 0.04962\n",
      "Epoch: 01 [ 5364/19954 ( 27%)], Train Loss: 0.04945\n",
      "Epoch: 01 [ 5404/19954 ( 27%)], Train Loss: 0.04925\n",
      "Epoch: 01 [ 5444/19954 ( 27%)], Train Loss: 0.04908\n",
      "Epoch: 01 [ 5484/19954 ( 27%)], Train Loss: 0.04882\n",
      "Epoch: 01 [ 5524/19954 ( 28%)], Train Loss: 0.04860\n",
      "Epoch: 01 [ 5564/19954 ( 28%)], Train Loss: 0.04852\n",
      "Epoch: 01 [ 5604/19954 ( 28%)], Train Loss: 0.04837\n",
      "Epoch: 01 [ 5644/19954 ( 28%)], Train Loss: 0.04843\n",
      "Epoch: 01 [ 5684/19954 ( 28%)], Train Loss: 0.04838\n",
      "Epoch: 01 [ 5724/19954 ( 29%)], Train Loss: 0.04821\n",
      "Epoch: 01 [ 5764/19954 ( 29%)], Train Loss: 0.04811\n",
      "Epoch: 01 [ 5804/19954 ( 29%)], Train Loss: 0.04805\n",
      "Epoch: 01 [ 5844/19954 ( 29%)], Train Loss: 0.04823\n",
      "Epoch: 01 [ 5884/19954 ( 29%)], Train Loss: 0.04834\n",
      "Epoch: 01 [ 5924/19954 ( 30%)], Train Loss: 0.04818\n",
      "Epoch: 01 [ 5964/19954 ( 30%)], Train Loss: 0.04818\n",
      "Epoch: 01 [ 6004/19954 ( 30%)], Train Loss: 0.04806\n",
      "Epoch: 01 [ 6044/19954 ( 30%)], Train Loss: 0.04800\n",
      "Epoch: 01 [ 6084/19954 ( 30%)], Train Loss: 0.04792\n",
      "Epoch: 01 [ 6124/19954 ( 31%)], Train Loss: 0.04778\n",
      "Epoch: 01 [ 6164/19954 ( 31%)], Train Loss: 0.04773\n",
      "Epoch: 01 [ 6204/19954 ( 31%)], Train Loss: 0.04781\n",
      "Epoch: 01 [ 6244/19954 ( 31%)], Train Loss: 0.04771\n",
      "Epoch: 01 [ 6284/19954 ( 31%)], Train Loss: 0.04764\n",
      "Epoch: 01 [ 6324/19954 ( 32%)], Train Loss: 0.04751\n",
      "Epoch: 01 [ 6364/19954 ( 32%)], Train Loss: 0.04749\n",
      "Epoch: 01 [ 6404/19954 ( 32%)], Train Loss: 0.04730\n",
      "Epoch: 01 [ 6444/19954 ( 32%)], Train Loss: 0.04727\n",
      "Epoch: 01 [ 6484/19954 ( 32%)], Train Loss: 0.04708\n",
      "Epoch: 01 [ 6524/19954 ( 33%)], Train Loss: 0.04708\n",
      "Epoch: 01 [ 6564/19954 ( 33%)], Train Loss: 0.04700\n",
      "Epoch: 01 [ 6604/19954 ( 33%)], Train Loss: 0.04684\n",
      "Epoch: 01 [ 6644/19954 ( 33%)], Train Loss: 0.04677\n",
      "Epoch: 01 [ 6684/19954 ( 33%)], Train Loss: 0.04660\n",
      "Epoch: 01 [ 6724/19954 ( 34%)], Train Loss: 0.04645\n",
      "Epoch: 01 [ 6764/19954 ( 34%)], Train Loss: 0.04633\n",
      "Epoch: 01 [ 6804/19954 ( 34%)], Train Loss: 0.04629\n",
      "Epoch: 01 [ 6844/19954 ( 34%)], Train Loss: 0.04626\n",
      "Epoch: 01 [ 6884/19954 ( 34%)], Train Loss: 0.04614\n",
      "Epoch: 01 [ 6924/19954 ( 35%)], Train Loss: 0.04610\n",
      "Epoch: 01 [ 6964/19954 ( 35%)], Train Loss: 0.04606\n",
      "Epoch: 01 [ 7004/19954 ( 35%)], Train Loss: 0.04591\n",
      "Epoch: 01 [ 7044/19954 ( 35%)], Train Loss: 0.04589\n",
      "Epoch: 01 [ 7084/19954 ( 36%)], Train Loss: 0.04573\n",
      "Epoch: 01 [ 7124/19954 ( 36%)], Train Loss: 0.04590\n",
      "Epoch: 01 [ 7164/19954 ( 36%)], Train Loss: 0.04608\n",
      "Epoch: 01 [ 7204/19954 ( 36%)], Train Loss: 0.04595\n",
      "Epoch: 01 [ 7244/19954 ( 36%)], Train Loss: 0.04598\n",
      "Epoch: 01 [ 7284/19954 ( 37%)], Train Loss: 0.04610\n",
      "Epoch: 01 [ 7324/19954 ( 37%)], Train Loss: 0.04601\n",
      "Epoch: 01 [ 7364/19954 ( 37%)], Train Loss: 0.04608\n",
      "Epoch: 01 [ 7404/19954 ( 37%)], Train Loss: 0.04597\n",
      "Epoch: 01 [ 7444/19954 ( 37%)], Train Loss: 0.04595\n",
      "Epoch: 01 [ 7484/19954 ( 38%)], Train Loss: 0.04582\n",
      "Epoch: 01 [ 7524/19954 ( 38%)], Train Loss: 0.04583\n",
      "Epoch: 01 [ 7564/19954 ( 38%)], Train Loss: 0.04577\n",
      "Epoch: 01 [ 7604/19954 ( 38%)], Train Loss: 0.04578\n",
      "Epoch: 01 [ 7644/19954 ( 38%)], Train Loss: 0.04561\n",
      "Epoch: 01 [ 7684/19954 ( 39%)], Train Loss: 0.04556\n",
      "Epoch: 01 [ 7724/19954 ( 39%)], Train Loss: 0.04592\n",
      "Epoch: 01 [ 7764/19954 ( 39%)], Train Loss: 0.04582\n",
      "Epoch: 01 [ 7804/19954 ( 39%)], Train Loss: 0.04577\n",
      "Epoch: 01 [ 7844/19954 ( 39%)], Train Loss: 0.04572\n",
      "Epoch: 01 [ 7884/19954 ( 40%)], Train Loss: 0.04564\n",
      "Epoch: 01 [ 7924/19954 ( 40%)], Train Loss: 0.04566\n",
      "Epoch: 01 [ 7964/19954 ( 40%)], Train Loss: 0.04565\n",
      "Epoch: 01 [ 8004/19954 ( 40%)], Train Loss: 0.04554\n",
      "Epoch: 01 [ 8044/19954 ( 40%)], Train Loss: 0.04552\n",
      "Epoch: 01 [ 8084/19954 ( 41%)], Train Loss: 0.04551\n",
      "Epoch: 01 [ 8124/19954 ( 41%)], Train Loss: 0.04539\n",
      "Epoch: 01 [ 8164/19954 ( 41%)], Train Loss: 0.04541\n",
      "Epoch: 01 [ 8204/19954 ( 41%)], Train Loss: 0.04531\n",
      "Epoch: 01 [ 8244/19954 ( 41%)], Train Loss: 0.04528\n",
      "Epoch: 01 [ 8284/19954 ( 42%)], Train Loss: 0.04528\n",
      "Epoch: 01 [ 8324/19954 ( 42%)], Train Loss: 0.04516\n",
      "Epoch: 01 [ 8364/19954 ( 42%)], Train Loss: 0.04509\n",
      "Epoch: 01 [ 8404/19954 ( 42%)], Train Loss: 0.04505\n",
      "Epoch: 01 [ 8444/19954 ( 42%)], Train Loss: 0.04498\n",
      "Epoch: 01 [ 8484/19954 ( 43%)], Train Loss: 0.04492\n",
      "Epoch: 01 [ 8524/19954 ( 43%)], Train Loss: 0.04482\n",
      "Epoch: 01 [ 8564/19954 ( 43%)], Train Loss: 0.04481\n",
      "Epoch: 01 [ 8604/19954 ( 43%)], Train Loss: 0.04468\n",
      "Epoch: 01 [ 8644/19954 ( 43%)], Train Loss: 0.04457\n",
      "Epoch: 01 [ 8684/19954 ( 44%)], Train Loss: 0.04458\n",
      "Epoch: 01 [ 8724/19954 ( 44%)], Train Loss: 0.04454\n",
      "Epoch: 01 [ 8764/19954 ( 44%)], Train Loss: 0.04453\n",
      "Epoch: 01 [ 8804/19954 ( 44%)], Train Loss: 0.04439\n",
      "Epoch: 01 [ 8844/19954 ( 44%)], Train Loss: 0.04429\n",
      "Epoch: 01 [ 8884/19954 ( 45%)], Train Loss: 0.04424\n",
      "Epoch: 01 [ 8924/19954 ( 45%)], Train Loss: 0.04409\n",
      "Epoch: 01 [ 8964/19954 ( 45%)], Train Loss: 0.04398\n",
      "Epoch: 01 [ 9004/19954 ( 45%)], Train Loss: 0.04401\n",
      "Epoch: 01 [ 9044/19954 ( 45%)], Train Loss: 0.04407\n",
      "Epoch: 01 [ 9084/19954 ( 46%)], Train Loss: 0.04400\n",
      "Epoch: 01 [ 9124/19954 ( 46%)], Train Loss: 0.04393\n",
      "Epoch: 01 [ 9164/19954 ( 46%)], Train Loss: 0.04377\n",
      "Epoch: 01 [ 9204/19954 ( 46%)], Train Loss: 0.04372\n",
      "Epoch: 01 [ 9244/19954 ( 46%)], Train Loss: 0.04370\n",
      "Epoch: 01 [ 9284/19954 ( 47%)], Train Loss: 0.04365\n",
      "Epoch: 01 [ 9324/19954 ( 47%)], Train Loss: 0.04354\n",
      "Epoch: 01 [ 9364/19954 ( 47%)], Train Loss: 0.04348\n",
      "Epoch: 01 [ 9404/19954 ( 47%)], Train Loss: 0.04352\n",
      "Epoch: 01 [ 9444/19954 ( 47%)], Train Loss: 0.04347\n",
      "Epoch: 01 [ 9484/19954 ( 48%)], Train Loss: 0.04340\n",
      "Epoch: 01 [ 9524/19954 ( 48%)], Train Loss: 0.04329\n",
      "Epoch: 01 [ 9564/19954 ( 48%)], Train Loss: 0.04318\n",
      "Epoch: 01 [ 9604/19954 ( 48%)], Train Loss: 0.04317\n",
      "Epoch: 01 [ 9644/19954 ( 48%)], Train Loss: 0.04306\n",
      "Epoch: 01 [ 9684/19954 ( 49%)], Train Loss: 0.04296\n",
      "Epoch: 01 [ 9724/19954 ( 49%)], Train Loss: 0.04294\n",
      "Epoch: 01 [ 9764/19954 ( 49%)], Train Loss: 0.04284\n",
      "Epoch: 01 [ 9804/19954 ( 49%)], Train Loss: 0.04273\n",
      "Epoch: 01 [ 9844/19954 ( 49%)], Train Loss: 0.04266\n",
      "Epoch: 01 [ 9884/19954 ( 50%)], Train Loss: 0.04261\n",
      "Epoch: 01 [ 9924/19954 ( 50%)], Train Loss: 0.04255\n",
      "Epoch: 01 [ 9964/19954 ( 50%)], Train Loss: 0.04243\n",
      "Epoch: 01 [10004/19954 ( 50%)], Train Loss: 0.04237\n",
      "Epoch: 01 [10044/19954 ( 50%)], Train Loss: 0.04226\n",
      "Epoch: 01 [10084/19954 ( 51%)], Train Loss: 0.04218\n",
      "Epoch: 01 [10124/19954 ( 51%)], Train Loss: 0.04214\n",
      "Epoch: 01 [10164/19954 ( 51%)], Train Loss: 0.04216\n",
      "Epoch: 01 [10204/19954 ( 51%)], Train Loss: 0.04224\n",
      "Epoch: 01 [10244/19954 ( 51%)], Train Loss: 0.04220\n",
      "Epoch: 01 [10284/19954 ( 52%)], Train Loss: 0.04210\n",
      "Epoch: 01 [10324/19954 ( 52%)], Train Loss: 0.04211\n",
      "Epoch: 01 [10364/19954 ( 52%)], Train Loss: 0.04210\n",
      "Epoch: 01 [10404/19954 ( 52%)], Train Loss: 0.04205\n",
      "Epoch: 01 [10444/19954 ( 52%)], Train Loss: 0.04206\n",
      "Epoch: 01 [10484/19954 ( 53%)], Train Loss: 0.04212\n",
      "Epoch: 01 [10524/19954 ( 53%)], Train Loss: 0.04219\n",
      "Epoch: 01 [10564/19954 ( 53%)], Train Loss: 0.04209\n",
      "Epoch: 01 [10604/19954 ( 53%)], Train Loss: 0.04210\n",
      "Epoch: 01 [10644/19954 ( 53%)], Train Loss: 0.04206\n",
      "Epoch: 01 [10684/19954 ( 54%)], Train Loss: 0.04199\n",
      "Epoch: 01 [10724/19954 ( 54%)], Train Loss: 0.04195\n",
      "Epoch: 01 [10764/19954 ( 54%)], Train Loss: 0.04192\n",
      "Epoch: 01 [10804/19954 ( 54%)], Train Loss: 0.04192\n",
      "Epoch: 01 [10844/19954 ( 54%)], Train Loss: 0.04186\n",
      "Epoch: 01 [10884/19954 ( 55%)], Train Loss: 0.04189\n",
      "Epoch: 01 [10924/19954 ( 55%)], Train Loss: 0.04182\n",
      "Epoch: 01 [10964/19954 ( 55%)], Train Loss: 0.04172\n",
      "Epoch: 01 [11004/19954 ( 55%)], Train Loss: 0.04171\n",
      "Epoch: 01 [11044/19954 ( 55%)], Train Loss: 0.04174\n",
      "Epoch: 01 [11084/19954 ( 56%)], Train Loss: 0.04169\n",
      "Epoch: 01 [11124/19954 ( 56%)], Train Loss: 0.04169\n",
      "Epoch: 01 [11164/19954 ( 56%)], Train Loss: 0.04159\n",
      "Epoch: 01 [11204/19954 ( 56%)], Train Loss: 0.04159\n",
      "Epoch: 01 [11244/19954 ( 56%)], Train Loss: 0.04156\n",
      "Epoch: 01 [11284/19954 ( 57%)], Train Loss: 0.04148\n",
      "Epoch: 01 [11324/19954 ( 57%)], Train Loss: 0.04151\n",
      "Epoch: 01 [11364/19954 ( 57%)], Train Loss: 0.04144\n",
      "Epoch: 01 [11404/19954 ( 57%)], Train Loss: 0.04143\n",
      "Epoch: 01 [11444/19954 ( 57%)], Train Loss: 0.04136\n",
      "Epoch: 01 [11484/19954 ( 58%)], Train Loss: 0.04127\n",
      "Epoch: 01 [11524/19954 ( 58%)], Train Loss: 0.04123\n",
      "Epoch: 01 [11564/19954 ( 58%)], Train Loss: 0.04115\n",
      "Epoch: 01 [11604/19954 ( 58%)], Train Loss: 0.04117\n",
      "Epoch: 01 [11644/19954 ( 58%)], Train Loss: 0.04116\n",
      "Epoch: 01 [11684/19954 ( 59%)], Train Loss: 0.04120\n",
      "Epoch: 01 [11724/19954 ( 59%)], Train Loss: 0.04118\n",
      "Epoch: 01 [11764/19954 ( 59%)], Train Loss: 0.04111\n",
      "Epoch: 01 [11804/19954 ( 59%)], Train Loss: 0.04108\n",
      "Epoch: 01 [11844/19954 ( 59%)], Train Loss: 0.04101\n",
      "Epoch: 01 [11884/19954 ( 60%)], Train Loss: 0.04101\n",
      "Epoch: 01 [11924/19954 ( 60%)], Train Loss: 0.04104\n",
      "Epoch: 01 [11964/19954 ( 60%)], Train Loss: 0.04097\n",
      "Epoch: 01 [12004/19954 ( 60%)], Train Loss: 0.04092\n",
      "Epoch: 01 [12044/19954 ( 60%)], Train Loss: 0.04094\n",
      "Epoch: 01 [12084/19954 ( 61%)], Train Loss: 0.04099\n",
      "Epoch: 01 [12124/19954 ( 61%)], Train Loss: 0.04101\n",
      "Epoch: 01 [12164/19954 ( 61%)], Train Loss: 0.04093\n",
      "Epoch: 01 [12204/19954 ( 61%)], Train Loss: 0.04098\n",
      "Epoch: 01 [12244/19954 ( 61%)], Train Loss: 0.04098\n",
      "Epoch: 01 [12284/19954 ( 62%)], Train Loss: 0.04091\n",
      "Epoch: 01 [12324/19954 ( 62%)], Train Loss: 0.04089\n",
      "Epoch: 01 [12364/19954 ( 62%)], Train Loss: 0.04084\n",
      "Epoch: 01 [12404/19954 ( 62%)], Train Loss: 0.04078\n",
      "Epoch: 01 [12444/19954 ( 62%)], Train Loss: 0.04080\n",
      "Epoch: 01 [12484/19954 ( 63%)], Train Loss: 0.04075\n",
      "Epoch: 01 [12524/19954 ( 63%)], Train Loss: 0.04081\n",
      "Epoch: 01 [12564/19954 ( 63%)], Train Loss: 0.04072\n",
      "Epoch: 01 [12604/19954 ( 63%)], Train Loss: 0.04062\n",
      "Epoch: 01 [12644/19954 ( 63%)], Train Loss: 0.04058\n",
      "Epoch: 01 [12684/19954 ( 64%)], Train Loss: 0.04054\n",
      "Epoch: 01 [12724/19954 ( 64%)], Train Loss: 0.04049\n",
      "Epoch: 01 [12764/19954 ( 64%)], Train Loss: 0.04060\n",
      "Epoch: 01 [12804/19954 ( 64%)], Train Loss: 0.04062\n",
      "Epoch: 01 [12844/19954 ( 64%)], Train Loss: 0.04058\n",
      "Epoch: 01 [12884/19954 ( 65%)], Train Loss: 0.04053\n",
      "Epoch: 01 [12924/19954 ( 65%)], Train Loss: 0.04049\n",
      "Epoch: 01 [12964/19954 ( 65%)], Train Loss: 0.04053\n",
      "Epoch: 01 [13004/19954 ( 65%)], Train Loss: 0.04044\n",
      "Epoch: 01 [13044/19954 ( 65%)], Train Loss: 0.04041\n",
      "Epoch: 01 [13084/19954 ( 66%)], Train Loss: 0.04038\n",
      "Epoch: 01 [13124/19954 ( 66%)], Train Loss: 0.04037\n",
      "Epoch: 01 [13164/19954 ( 66%)], Train Loss: 0.04035\n",
      "Epoch: 01 [13204/19954 ( 66%)], Train Loss: 0.04031\n",
      "Epoch: 01 [13244/19954 ( 66%)], Train Loss: 0.04025\n",
      "Epoch: 01 [13284/19954 ( 67%)], Train Loss: 0.04023\n",
      "Epoch: 01 [13324/19954 ( 67%)], Train Loss: 0.04025\n",
      "Epoch: 01 [13364/19954 ( 67%)], Train Loss: 0.04018\n",
      "Epoch: 01 [13404/19954 ( 67%)], Train Loss: 0.04013\n",
      "Epoch: 01 [13444/19954 ( 67%)], Train Loss: 0.04018\n",
      "Epoch: 01 [13484/19954 ( 68%)], Train Loss: 0.04013\n",
      "Epoch: 01 [13524/19954 ( 68%)], Train Loss: 0.04013\n",
      "Epoch: 01 [13564/19954 ( 68%)], Train Loss: 0.04029\n",
      "Epoch: 01 [13604/19954 ( 68%)], Train Loss: 0.04036\n",
      "Epoch: 01 [13644/19954 ( 68%)], Train Loss: 0.04034\n",
      "Epoch: 01 [13684/19954 ( 69%)], Train Loss: 0.04032\n",
      "Epoch: 01 [13724/19954 ( 69%)], Train Loss: 0.04024\n",
      "Epoch: 01 [13764/19954 ( 69%)], Train Loss: 0.04020\n",
      "Epoch: 01 [13804/19954 ( 69%)], Train Loss: 0.04020\n",
      "Epoch: 01 [13844/19954 ( 69%)], Train Loss: 0.04015\n",
      "Epoch: 01 [13884/19954 ( 70%)], Train Loss: 0.04018\n",
      "Epoch: 01 [13924/19954 ( 70%)], Train Loss: 0.04015\n",
      "Epoch: 01 [13964/19954 ( 70%)], Train Loss: 0.04014\n",
      "Epoch: 01 [14004/19954 ( 70%)], Train Loss: 0.04021\n",
      "Epoch: 01 [14044/19954 ( 70%)], Train Loss: 0.04024\n",
      "Epoch: 01 [14084/19954 ( 71%)], Train Loss: 0.04030\n",
      "Epoch: 01 [14124/19954 ( 71%)], Train Loss: 0.04038\n",
      "Epoch: 01 [14164/19954 ( 71%)], Train Loss: 0.04031\n",
      "Epoch: 01 [14204/19954 ( 71%)], Train Loss: 0.04027\n",
      "Epoch: 01 [14244/19954 ( 71%)], Train Loss: 0.04025\n",
      "Epoch: 01 [14284/19954 ( 72%)], Train Loss: 0.04024\n",
      "Epoch: 01 [14324/19954 ( 72%)], Train Loss: 0.04016\n",
      "Epoch: 01 [14364/19954 ( 72%)], Train Loss: 0.04021\n",
      "Epoch: 01 [14404/19954 ( 72%)], Train Loss: 0.04016\n",
      "Epoch: 01 [14444/19954 ( 72%)], Train Loss: 0.04012\n",
      "Epoch: 01 [14484/19954 ( 73%)], Train Loss: 0.04010\n",
      "Epoch: 01 [14524/19954 ( 73%)], Train Loss: 0.04004\n",
      "Epoch: 01 [14564/19954 ( 73%)], Train Loss: 0.04000\n",
      "Epoch: 01 [14604/19954 ( 73%)], Train Loss: 0.03997\n",
      "Epoch: 01 [14644/19954 ( 73%)], Train Loss: 0.03996\n",
      "Epoch: 01 [14684/19954 ( 74%)], Train Loss: 0.03989\n",
      "Epoch: 01 [14724/19954 ( 74%)], Train Loss: 0.03980\n",
      "Epoch: 01 [14764/19954 ( 74%)], Train Loss: 0.03974\n",
      "Epoch: 01 [14804/19954 ( 74%)], Train Loss: 0.03968\n",
      "Epoch: 01 [14844/19954 ( 74%)], Train Loss: 0.03962\n",
      "Epoch: 01 [14884/19954 ( 75%)], Train Loss: 0.03955\n",
      "Epoch: 01 [14924/19954 ( 75%)], Train Loss: 0.03955\n",
      "Epoch: 01 [14964/19954 ( 75%)], Train Loss: 0.03946\n",
      "Epoch: 01 [15004/19954 ( 75%)], Train Loss: 0.03948\n",
      "Epoch: 01 [15044/19954 ( 75%)], Train Loss: 0.03943\n",
      "Epoch: 01 [15084/19954 ( 76%)], Train Loss: 0.03940\n",
      "Epoch: 01 [15124/19954 ( 76%)], Train Loss: 0.03937\n",
      "Epoch: 01 [15164/19954 ( 76%)], Train Loss: 0.03930\n",
      "Epoch: 01 [15204/19954 ( 76%)], Train Loss: 0.03930\n",
      "Epoch: 01 [15244/19954 ( 76%)], Train Loss: 0.03929\n",
      "Epoch: 01 [15284/19954 ( 77%)], Train Loss: 0.03929\n",
      "Epoch: 01 [15324/19954 ( 77%)], Train Loss: 0.03930\n",
      "Epoch: 01 [15364/19954 ( 77%)], Train Loss: 0.03924\n",
      "Epoch: 01 [15404/19954 ( 77%)], Train Loss: 0.03919\n",
      "Epoch: 01 [15444/19954 ( 77%)], Train Loss: 0.03917\n",
      "Epoch: 01 [15484/19954 ( 78%)], Train Loss: 0.03918\n",
      "Epoch: 01 [15524/19954 ( 78%)], Train Loss: 0.03912\n",
      "Epoch: 01 [15564/19954 ( 78%)], Train Loss: 0.03910\n",
      "Epoch: 01 [15604/19954 ( 78%)], Train Loss: 0.03910\n",
      "Epoch: 01 [15644/19954 ( 78%)], Train Loss: 0.03905\n",
      "Epoch: 01 [15684/19954 ( 79%)], Train Loss: 0.03907\n",
      "Epoch: 01 [15724/19954 ( 79%)], Train Loss: 0.03908\n",
      "Epoch: 01 [15764/19954 ( 79%)], Train Loss: 0.03903\n",
      "Epoch: 01 [15804/19954 ( 79%)], Train Loss: 0.03896\n",
      "Epoch: 01 [15844/19954 ( 79%)], Train Loss: 0.03893\n",
      "Epoch: 01 [15884/19954 ( 80%)], Train Loss: 0.03889\n",
      "Epoch: 01 [15924/19954 ( 80%)], Train Loss: 0.03886\n",
      "Epoch: 01 [15964/19954 ( 80%)], Train Loss: 0.03881\n",
      "Epoch: 01 [16004/19954 ( 80%)], Train Loss: 0.03880\n",
      "Epoch: 01 [16044/19954 ( 80%)], Train Loss: 0.03889\n",
      "Epoch: 01 [16084/19954 ( 81%)], Train Loss: 0.03885\n",
      "Epoch: 01 [16124/19954 ( 81%)], Train Loss: 0.03882\n",
      "Epoch: 01 [16164/19954 ( 81%)], Train Loss: 0.03890\n",
      "Epoch: 01 [16204/19954 ( 81%)], Train Loss: 0.03885\n",
      "Epoch: 01 [16244/19954 ( 81%)], Train Loss: 0.03879\n",
      "Epoch: 01 [16284/19954 ( 82%)], Train Loss: 0.03878\n",
      "Epoch: 01 [16324/19954 ( 82%)], Train Loss: 0.03881\n",
      "Epoch: 01 [16364/19954 ( 82%)], Train Loss: 0.03878\n",
      "Epoch: 01 [16404/19954 ( 82%)], Train Loss: 0.03876\n",
      "Epoch: 01 [16444/19954 ( 82%)], Train Loss: 0.03872\n",
      "Epoch: 01 [16484/19954 ( 83%)], Train Loss: 0.03875\n",
      "Epoch: 01 [16524/19954 ( 83%)], Train Loss: 0.03878\n",
      "Epoch: 01 [16564/19954 ( 83%)], Train Loss: 0.03870\n",
      "Epoch: 01 [16604/19954 ( 83%)], Train Loss: 0.03869\n",
      "Epoch: 01 [16644/19954 ( 83%)], Train Loss: 0.03864\n",
      "Epoch: 01 [16684/19954 ( 84%)], Train Loss: 0.03858\n",
      "Epoch: 01 [16724/19954 ( 84%)], Train Loss: 0.03863\n",
      "Epoch: 01 [16764/19954 ( 84%)], Train Loss: 0.03857\n",
      "Epoch: 01 [16804/19954 ( 84%)], Train Loss: 0.03860\n",
      "Epoch: 01 [16844/19954 ( 84%)], Train Loss: 0.03854\n",
      "Epoch: 01 [16884/19954 ( 85%)], Train Loss: 0.03859\n",
      "Epoch: 01 [16924/19954 ( 85%)], Train Loss: 0.03860\n",
      "Epoch: 01 [16964/19954 ( 85%)], Train Loss: 0.03862\n",
      "Epoch: 01 [17004/19954 ( 85%)], Train Loss: 0.03860\n",
      "Epoch: 01 [17044/19954 ( 85%)], Train Loss: 0.03859\n",
      "Epoch: 01 [17084/19954 ( 86%)], Train Loss: 0.03857\n",
      "Epoch: 01 [17124/19954 ( 86%)], Train Loss: 0.03857\n",
      "Epoch: 01 [17164/19954 ( 86%)], Train Loss: 0.03854\n",
      "Epoch: 01 [17204/19954 ( 86%)], Train Loss: 0.03848\n",
      "Epoch: 01 [17244/19954 ( 86%)], Train Loss: 0.03844\n",
      "Epoch: 01 [17284/19954 ( 87%)], Train Loss: 0.03845\n",
      "Epoch: 01 [17324/19954 ( 87%)], Train Loss: 0.03847\n",
      "Epoch: 01 [17364/19954 ( 87%)], Train Loss: 0.03843\n",
      "Epoch: 01 [17404/19954 ( 87%)], Train Loss: 0.03839\n",
      "Epoch: 01 [17444/19954 ( 87%)], Train Loss: 0.03836\n",
      "Epoch: 01 [17484/19954 ( 88%)], Train Loss: 0.03831\n",
      "Epoch: 01 [17524/19954 ( 88%)], Train Loss: 0.03830\n",
      "Epoch: 01 [17564/19954 ( 88%)], Train Loss: 0.03829\n",
      "Epoch: 01 [17604/19954 ( 88%)], Train Loss: 0.03831\n",
      "Epoch: 01 [17644/19954 ( 88%)], Train Loss: 0.03830\n",
      "Epoch: 01 [17684/19954 ( 89%)], Train Loss: 0.03832\n",
      "Epoch: 01 [17724/19954 ( 89%)], Train Loss: 0.03831\n",
      "Epoch: 01 [17764/19954 ( 89%)], Train Loss: 0.03832\n",
      "Epoch: 01 [17804/19954 ( 89%)], Train Loss: 0.03835\n",
      "Epoch: 01 [17844/19954 ( 89%)], Train Loss: 0.03834\n",
      "Epoch: 01 [17884/19954 ( 90%)], Train Loss: 0.03836\n",
      "Epoch: 01 [17924/19954 ( 90%)], Train Loss: 0.03838\n",
      "Epoch: 01 [17964/19954 ( 90%)], Train Loss: 0.03834\n",
      "Epoch: 01 [18004/19954 ( 90%)], Train Loss: 0.03834\n",
      "Epoch: 01 [18044/19954 ( 90%)], Train Loss: 0.03832\n",
      "Epoch: 01 [18084/19954 ( 91%)], Train Loss: 0.03829\n",
      "Epoch: 01 [18124/19954 ( 91%)], Train Loss: 0.03824\n",
      "Epoch: 01 [18164/19954 ( 91%)], Train Loss: 0.03823\n",
      "Epoch: 01 [18204/19954 ( 91%)], Train Loss: 0.03822\n",
      "Epoch: 01 [18244/19954 ( 91%)], Train Loss: 0.03821\n",
      "Epoch: 01 [18284/19954 ( 92%)], Train Loss: 0.03821\n",
      "Epoch: 01 [18324/19954 ( 92%)], Train Loss: 0.03827\n",
      "Epoch: 01 [18364/19954 ( 92%)], Train Loss: 0.03821\n",
      "Epoch: 01 [18404/19954 ( 92%)], Train Loss: 0.03821\n",
      "Epoch: 01 [18444/19954 ( 92%)], Train Loss: 0.03823\n",
      "Epoch: 01 [18484/19954 ( 93%)], Train Loss: 0.03828\n",
      "Epoch: 01 [18524/19954 ( 93%)], Train Loss: 0.03828\n",
      "Epoch: 01 [18564/19954 ( 93%)], Train Loss: 0.03826\n",
      "Epoch: 01 [18604/19954 ( 93%)], Train Loss: 0.03824\n",
      "Epoch: 01 [18644/19954 ( 93%)], Train Loss: 0.03824\n",
      "Epoch: 01 [18684/19954 ( 94%)], Train Loss: 0.03822\n",
      "Epoch: 01 [18724/19954 ( 94%)], Train Loss: 0.03824\n",
      "Epoch: 01 [18764/19954 ( 94%)], Train Loss: 0.03821\n",
      "Epoch: 01 [18804/19954 ( 94%)], Train Loss: 0.03823\n",
      "Epoch: 01 [18844/19954 ( 94%)], Train Loss: 0.03819\n",
      "Epoch: 01 [18884/19954 ( 95%)], Train Loss: 0.03814\n",
      "Epoch: 01 [18924/19954 ( 95%)], Train Loss: 0.03810\n",
      "Epoch: 01 [18964/19954 ( 95%)], Train Loss: 0.03808\n",
      "Epoch: 01 [19004/19954 ( 95%)], Train Loss: 0.03804\n",
      "Epoch: 01 [19044/19954 ( 95%)], Train Loss: 0.03803\n",
      "Epoch: 01 [19084/19954 ( 96%)], Train Loss: 0.03806\n",
      "Epoch: 01 [19124/19954 ( 96%)], Train Loss: 0.03819\n",
      "Epoch: 01 [19164/19954 ( 96%)], Train Loss: 0.03817\n",
      "Epoch: 01 [19204/19954 ( 96%)], Train Loss: 0.03811\n",
      "Epoch: 01 [19244/19954 ( 96%)], Train Loss: 0.03807\n",
      "Epoch: 01 [19284/19954 ( 97%)], Train Loss: 0.03804\n",
      "Epoch: 01 [19324/19954 ( 97%)], Train Loss: 0.03801\n",
      "Epoch: 01 [19364/19954 ( 97%)], Train Loss: 0.03803\n",
      "Epoch: 01 [19404/19954 ( 97%)], Train Loss: 0.03799\n",
      "Epoch: 01 [19444/19954 ( 97%)], Train Loss: 0.03798\n",
      "Epoch: 01 [19484/19954 ( 98%)], Train Loss: 0.03799\n",
      "Epoch: 01 [19524/19954 ( 98%)], Train Loss: 0.03801\n",
      "Epoch: 01 [19564/19954 ( 98%)], Train Loss: 0.03798\n",
      "Epoch: 01 [19604/19954 ( 98%)], Train Loss: 0.03801\n",
      "Epoch: 01 [19644/19954 ( 98%)], Train Loss: 0.03798\n",
      "Epoch: 01 [19684/19954 ( 99%)], Train Loss: 0.03794\n",
      "Epoch: 01 [19724/19954 ( 99%)], Train Loss: 0.03789\n",
      "Epoch: 01 [19764/19954 ( 99%)], Train Loss: 0.03790\n",
      "Epoch: 01 [19804/19954 ( 99%)], Train Loss: 0.03790\n",
      "Epoch: 01 [19844/19954 ( 99%)], Train Loss: 0.03790\n",
      "Epoch: 01 [19884/19954 (100%)], Train Loss: 0.03789\n",
      "Epoch: 01 [19924/19954 (100%)], Train Loss: 0.03784\n",
      "Epoch: 01 [19954/19954 (100%)], Train Loss: 0.03788\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.24419\n",
      "\n",
      "Total Training Time: 5252.20063829422secs, Average Training Time per Epoch: 2626.10031914711secs.\n",
      "Total Validation Time: 272.1162021160126secs, Average Validation Time per Epoch: 136.0581010580063secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 3\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20158, Num examples Valid=2911\n",
      "Total Training Steps: 1260, Total Warmup Steps: 126\n",
      "Epoch: 00 [    4/20158 (  0%)], Train Loss: 0.73576\n",
      "Epoch: 00 [   44/20158 (  0%)], Train Loss: 0.72848\n",
      "Epoch: 00 [   84/20158 (  0%)], Train Loss: 0.72791\n",
      "Epoch: 00 [  124/20158 (  1%)], Train Loss: 0.72496\n",
      "Epoch: 00 [  164/20158 (  1%)], Train Loss: 0.72208\n",
      "Epoch: 00 [  204/20158 (  1%)], Train Loss: 0.72064\n",
      "Epoch: 00 [  244/20158 (  1%)], Train Loss: 0.71815\n",
      "Epoch: 00 [  284/20158 (  1%)], Train Loss: 0.71638\n",
      "Epoch: 00 [  324/20158 (  2%)], Train Loss: 0.71236\n",
      "Epoch: 00 [  364/20158 (  2%)], Train Loss: 0.70877\n",
      "Epoch: 00 [  404/20158 (  2%)], Train Loss: 0.70447\n",
      "Epoch: 00 [  444/20158 (  2%)], Train Loss: 0.69890\n",
      "Epoch: 00 [  484/20158 (  2%)], Train Loss: 0.69432\n",
      "Epoch: 00 [  524/20158 (  3%)], Train Loss: 0.68838\n",
      "Epoch: 00 [  564/20158 (  3%)], Train Loss: 0.68246\n",
      "Epoch: 00 [  604/20158 (  3%)], Train Loss: 0.67560\n",
      "Epoch: 00 [  644/20158 (  3%)], Train Loss: 0.66805\n",
      "Epoch: 00 [  684/20158 (  3%)], Train Loss: 0.66140\n",
      "Epoch: 00 [  724/20158 (  4%)], Train Loss: 0.65229\n",
      "Epoch: 00 [  764/20158 (  4%)], Train Loss: 0.64401\n",
      "Epoch: 00 [  804/20158 (  4%)], Train Loss: 0.63416\n",
      "Epoch: 00 [  844/20158 (  4%)], Train Loss: 0.62591\n",
      "Epoch: 00 [  884/20158 (  4%)], Train Loss: 0.61610\n",
      "Epoch: 00 [  924/20158 (  5%)], Train Loss: 0.60477\n",
      "Epoch: 00 [  964/20158 (  5%)], Train Loss: 0.59267\n",
      "Epoch: 00 [ 1004/20158 (  5%)], Train Loss: 0.58235\n",
      "Epoch: 00 [ 1044/20158 (  5%)], Train Loss: 0.57199\n",
      "Epoch: 00 [ 1084/20158 (  5%)], Train Loss: 0.56203\n",
      "Epoch: 00 [ 1124/20158 (  6%)], Train Loss: 0.55038\n",
      "Epoch: 00 [ 1164/20158 (  6%)], Train Loss: 0.54029\n",
      "Epoch: 00 [ 1204/20158 (  6%)], Train Loss: 0.52856\n",
      "Epoch: 00 [ 1244/20158 (  6%)], Train Loss: 0.51964\n",
      "Epoch: 00 [ 1284/20158 (  6%)], Train Loss: 0.50956\n",
      "Epoch: 00 [ 1324/20158 (  7%)], Train Loss: 0.49988\n",
      "Epoch: 00 [ 1364/20158 (  7%)], Train Loss: 0.48948\n",
      "Epoch: 00 [ 1404/20158 (  7%)], Train Loss: 0.48051\n",
      "Epoch: 00 [ 1444/20158 (  7%)], Train Loss: 0.47225\n",
      "Epoch: 00 [ 1484/20158 (  7%)], Train Loss: 0.46447\n",
      "Epoch: 00 [ 1524/20158 (  8%)], Train Loss: 0.45685\n",
      "Epoch: 00 [ 1564/20158 (  8%)], Train Loss: 0.44889\n",
      "Epoch: 00 [ 1604/20158 (  8%)], Train Loss: 0.44300\n",
      "Epoch: 00 [ 1644/20158 (  8%)], Train Loss: 0.43537\n",
      "Epoch: 00 [ 1684/20158 (  8%)], Train Loss: 0.42882\n",
      "Epoch: 00 [ 1724/20158 (  9%)], Train Loss: 0.42147\n",
      "Epoch: 00 [ 1764/20158 (  9%)], Train Loss: 0.41433\n",
      "Epoch: 00 [ 1804/20158 (  9%)], Train Loss: 0.40737\n",
      "Epoch: 00 [ 1844/20158 (  9%)], Train Loss: 0.40145\n",
      "Epoch: 00 [ 1884/20158 (  9%)], Train Loss: 0.39568\n",
      "Epoch: 00 [ 1924/20158 ( 10%)], Train Loss: 0.39120\n",
      "Epoch: 00 [ 1964/20158 ( 10%)], Train Loss: 0.38438\n",
      "Epoch: 00 [ 2004/20158 ( 10%)], Train Loss: 0.37889\n",
      "Epoch: 00 [ 2044/20158 ( 10%)], Train Loss: 0.37350\n",
      "Epoch: 00 [ 2084/20158 ( 10%)], Train Loss: 0.36926\n",
      "Epoch: 00 [ 2124/20158 ( 11%)], Train Loss: 0.36502\n",
      "Epoch: 00 [ 2164/20158 ( 11%)], Train Loss: 0.36122\n",
      "Epoch: 00 [ 2204/20158 ( 11%)], Train Loss: 0.35597\n",
      "Epoch: 00 [ 2244/20158 ( 11%)], Train Loss: 0.35088\n",
      "Epoch: 00 [ 2284/20158 ( 11%)], Train Loss: 0.34766\n",
      "Epoch: 00 [ 2324/20158 ( 12%)], Train Loss: 0.34389\n",
      "Epoch: 00 [ 2364/20158 ( 12%)], Train Loss: 0.33912\n",
      "Epoch: 00 [ 2404/20158 ( 12%)], Train Loss: 0.33566\n",
      "Epoch: 00 [ 2444/20158 ( 12%)], Train Loss: 0.33151\n",
      "Epoch: 00 [ 2484/20158 ( 12%)], Train Loss: 0.32750\n",
      "Epoch: 00 [ 2524/20158 ( 13%)], Train Loss: 0.32466\n",
      "Epoch: 00 [ 2564/20158 ( 13%)], Train Loss: 0.32149\n",
      "Epoch: 00 [ 2604/20158 ( 13%)], Train Loss: 0.31822\n",
      "Epoch: 00 [ 2644/20158 ( 13%)], Train Loss: 0.31494\n",
      "Epoch: 00 [ 2684/20158 ( 13%)], Train Loss: 0.31155\n",
      "Epoch: 00 [ 2724/20158 ( 14%)], Train Loss: 0.30878\n",
      "Epoch: 00 [ 2764/20158 ( 14%)], Train Loss: 0.30578\n",
      "Epoch: 00 [ 2804/20158 ( 14%)], Train Loss: 0.30341\n",
      "Epoch: 00 [ 2844/20158 ( 14%)], Train Loss: 0.30028\n",
      "Epoch: 00 [ 2884/20158 ( 14%)], Train Loss: 0.29724\n",
      "Epoch: 00 [ 2924/20158 ( 15%)], Train Loss: 0.29440\n",
      "Epoch: 00 [ 2964/20158 ( 15%)], Train Loss: 0.29217\n",
      "Epoch: 00 [ 3004/20158 ( 15%)], Train Loss: 0.28921\n",
      "Epoch: 00 [ 3044/20158 ( 15%)], Train Loss: 0.28671\n",
      "Epoch: 00 [ 3084/20158 ( 15%)], Train Loss: 0.28435\n",
      "Epoch: 00 [ 3124/20158 ( 15%)], Train Loss: 0.28206\n",
      "Epoch: 00 [ 3164/20158 ( 16%)], Train Loss: 0.27906\n",
      "Epoch: 00 [ 3204/20158 ( 16%)], Train Loss: 0.27657\n",
      "Epoch: 00 [ 3244/20158 ( 16%)], Train Loss: 0.27467\n",
      "Epoch: 00 [ 3284/20158 ( 16%)], Train Loss: 0.27242\n",
      "Epoch: 00 [ 3324/20158 ( 16%)], Train Loss: 0.27082\n",
      "Epoch: 00 [ 3364/20158 ( 17%)], Train Loss: 0.26837\n",
      "Epoch: 00 [ 3404/20158 ( 17%)], Train Loss: 0.26627\n",
      "Epoch: 00 [ 3444/20158 ( 17%)], Train Loss: 0.26400\n",
      "Epoch: 00 [ 3484/20158 ( 17%)], Train Loss: 0.26207\n",
      "Epoch: 00 [ 3524/20158 ( 17%)], Train Loss: 0.26032\n",
      "Epoch: 00 [ 3564/20158 ( 18%)], Train Loss: 0.25832\n",
      "Epoch: 00 [ 3604/20158 ( 18%)], Train Loss: 0.25685\n",
      "Epoch: 00 [ 3644/20158 ( 18%)], Train Loss: 0.25560\n",
      "Epoch: 00 [ 3684/20158 ( 18%)], Train Loss: 0.25379\n",
      "Epoch: 00 [ 3724/20158 ( 18%)], Train Loss: 0.25241\n",
      "Epoch: 00 [ 3764/20158 ( 19%)], Train Loss: 0.25080\n",
      "Epoch: 00 [ 3804/20158 ( 19%)], Train Loss: 0.24915\n",
      "Epoch: 00 [ 3844/20158 ( 19%)], Train Loss: 0.24789\n",
      "Epoch: 00 [ 3884/20158 ( 19%)], Train Loss: 0.24642\n",
      "Epoch: 00 [ 3924/20158 ( 19%)], Train Loss: 0.24510\n",
      "Epoch: 00 [ 3964/20158 ( 20%)], Train Loss: 0.24352\n",
      "Epoch: 00 [ 4004/20158 ( 20%)], Train Loss: 0.24202\n",
      "Epoch: 00 [ 4044/20158 ( 20%)], Train Loss: 0.24091\n",
      "Epoch: 00 [ 4084/20158 ( 20%)], Train Loss: 0.23964\n",
      "Epoch: 00 [ 4124/20158 ( 20%)], Train Loss: 0.23796\n",
      "Epoch: 00 [ 4164/20158 ( 21%)], Train Loss: 0.23664\n",
      "Epoch: 00 [ 4204/20158 ( 21%)], Train Loss: 0.23530\n",
      "Epoch: 00 [ 4244/20158 ( 21%)], Train Loss: 0.23377\n",
      "Epoch: 00 [ 4284/20158 ( 21%)], Train Loss: 0.23254\n",
      "Epoch: 00 [ 4324/20158 ( 21%)], Train Loss: 0.23107\n",
      "Epoch: 00 [ 4364/20158 ( 22%)], Train Loss: 0.23028\n",
      "Epoch: 00 [ 4404/20158 ( 22%)], Train Loss: 0.22907\n",
      "Epoch: 00 [ 4444/20158 ( 22%)], Train Loss: 0.22812\n",
      "Epoch: 00 [ 4484/20158 ( 22%)], Train Loss: 0.22705\n",
      "Epoch: 00 [ 4524/20158 ( 22%)], Train Loss: 0.22564\n",
      "Epoch: 00 [ 4564/20158 ( 23%)], Train Loss: 0.22477\n",
      "Epoch: 00 [ 4604/20158 ( 23%)], Train Loss: 0.22361\n",
      "Epoch: 00 [ 4644/20158 ( 23%)], Train Loss: 0.22226\n",
      "Epoch: 00 [ 4684/20158 ( 23%)], Train Loss: 0.22084\n",
      "Epoch: 00 [ 4724/20158 ( 23%)], Train Loss: 0.21938\n",
      "Epoch: 00 [ 4764/20158 ( 24%)], Train Loss: 0.21845\n",
      "Epoch: 00 [ 4804/20158 ( 24%)], Train Loss: 0.21732\n",
      "Epoch: 00 [ 4844/20158 ( 24%)], Train Loss: 0.21647\n",
      "Epoch: 00 [ 4884/20158 ( 24%)], Train Loss: 0.21570\n",
      "Epoch: 00 [ 4924/20158 ( 24%)], Train Loss: 0.21528\n",
      "Epoch: 00 [ 4964/20158 ( 25%)], Train Loss: 0.21420\n",
      "Epoch: 00 [ 5004/20158 ( 25%)], Train Loss: 0.21391\n",
      "Epoch: 00 [ 5044/20158 ( 25%)], Train Loss: 0.21289\n",
      "Epoch: 00 [ 5084/20158 ( 25%)], Train Loss: 0.21186\n",
      "Epoch: 00 [ 5124/20158 ( 25%)], Train Loss: 0.21079\n",
      "Epoch: 00 [ 5164/20158 ( 26%)], Train Loss: 0.20978\n",
      "Epoch: 00 [ 5204/20158 ( 26%)], Train Loss: 0.20911\n",
      "Epoch: 00 [ 5244/20158 ( 26%)], Train Loss: 0.20810\n",
      "Epoch: 00 [ 5284/20158 ( 26%)], Train Loss: 0.20717\n",
      "Epoch: 00 [ 5324/20158 ( 26%)], Train Loss: 0.20641\n",
      "Epoch: 00 [ 5364/20158 ( 27%)], Train Loss: 0.20565\n",
      "Epoch: 00 [ 5404/20158 ( 27%)], Train Loss: 0.20510\n",
      "Epoch: 00 [ 5444/20158 ( 27%)], Train Loss: 0.20437\n",
      "Epoch: 00 [ 5484/20158 ( 27%)], Train Loss: 0.20350\n",
      "Epoch: 00 [ 5524/20158 ( 27%)], Train Loss: 0.20278\n",
      "Epoch: 00 [ 5564/20158 ( 28%)], Train Loss: 0.20206\n",
      "Epoch: 00 [ 5604/20158 ( 28%)], Train Loss: 0.20149\n",
      "Epoch: 00 [ 5644/20158 ( 28%)], Train Loss: 0.20034\n",
      "Epoch: 00 [ 5684/20158 ( 28%)], Train Loss: 0.19963\n",
      "Epoch: 00 [ 5724/20158 ( 28%)], Train Loss: 0.19878\n",
      "Epoch: 00 [ 5764/20158 ( 29%)], Train Loss: 0.19789\n",
      "Epoch: 00 [ 5804/20158 ( 29%)], Train Loss: 0.19745\n",
      "Epoch: 00 [ 5844/20158 ( 29%)], Train Loss: 0.19681\n",
      "Epoch: 00 [ 5884/20158 ( 29%)], Train Loss: 0.19594\n",
      "Epoch: 00 [ 5924/20158 ( 29%)], Train Loss: 0.19506\n",
      "Epoch: 00 [ 5964/20158 ( 30%)], Train Loss: 0.19405\n",
      "Epoch: 00 [ 6004/20158 ( 30%)], Train Loss: 0.19354\n",
      "Epoch: 00 [ 6044/20158 ( 30%)], Train Loss: 0.19290\n",
      "Epoch: 00 [ 6084/20158 ( 30%)], Train Loss: 0.19239\n",
      "Epoch: 00 [ 6124/20158 ( 30%)], Train Loss: 0.19165\n",
      "Epoch: 00 [ 6164/20158 ( 31%)], Train Loss: 0.19124\n",
      "Epoch: 00 [ 6204/20158 ( 31%)], Train Loss: 0.19054\n",
      "Epoch: 00 [ 6244/20158 ( 31%)], Train Loss: 0.18984\n",
      "Epoch: 00 [ 6284/20158 ( 31%)], Train Loss: 0.18941\n",
      "Epoch: 00 [ 6324/20158 ( 31%)], Train Loss: 0.18873\n",
      "Epoch: 00 [ 6364/20158 ( 32%)], Train Loss: 0.18815\n",
      "Epoch: 00 [ 6404/20158 ( 32%)], Train Loss: 0.18744\n",
      "Epoch: 00 [ 6444/20158 ( 32%)], Train Loss: 0.18662\n",
      "Epoch: 00 [ 6484/20158 ( 32%)], Train Loss: 0.18636\n",
      "Epoch: 00 [ 6524/20158 ( 32%)], Train Loss: 0.18579\n",
      "Epoch: 00 [ 6564/20158 ( 33%)], Train Loss: 0.18522\n",
      "Epoch: 00 [ 6604/20158 ( 33%)], Train Loss: 0.18451\n",
      "Epoch: 00 [ 6644/20158 ( 33%)], Train Loss: 0.18381\n",
      "Epoch: 00 [ 6684/20158 ( 33%)], Train Loss: 0.18299\n",
      "Epoch: 00 [ 6724/20158 ( 33%)], Train Loss: 0.18251\n",
      "Epoch: 00 [ 6764/20158 ( 34%)], Train Loss: 0.18200\n",
      "Epoch: 00 [ 6804/20158 ( 34%)], Train Loss: 0.18163\n",
      "Epoch: 00 [ 6844/20158 ( 34%)], Train Loss: 0.18113\n",
      "Epoch: 00 [ 6884/20158 ( 34%)], Train Loss: 0.18064\n",
      "Epoch: 00 [ 6924/20158 ( 34%)], Train Loss: 0.18032\n",
      "Epoch: 00 [ 6964/20158 ( 35%)], Train Loss: 0.18009\n",
      "Epoch: 00 [ 7004/20158 ( 35%)], Train Loss: 0.17949\n",
      "Epoch: 00 [ 7044/20158 ( 35%)], Train Loss: 0.17875\n",
      "Epoch: 00 [ 7084/20158 ( 35%)], Train Loss: 0.17824\n",
      "Epoch: 00 [ 7124/20158 ( 35%)], Train Loss: 0.17767\n",
      "Epoch: 00 [ 7164/20158 ( 36%)], Train Loss: 0.17713\n",
      "Epoch: 00 [ 7204/20158 ( 36%)], Train Loss: 0.17665\n",
      "Epoch: 00 [ 7244/20158 ( 36%)], Train Loss: 0.17602\n",
      "Epoch: 00 [ 7284/20158 ( 36%)], Train Loss: 0.17529\n",
      "Epoch: 00 [ 7324/20158 ( 36%)], Train Loss: 0.17496\n",
      "Epoch: 00 [ 7364/20158 ( 37%)], Train Loss: 0.17453\n",
      "Epoch: 00 [ 7404/20158 ( 37%)], Train Loss: 0.17399\n",
      "Epoch: 00 [ 7444/20158 ( 37%)], Train Loss: 0.17349\n",
      "Epoch: 00 [ 7484/20158 ( 37%)], Train Loss: 0.17294\n",
      "Epoch: 00 [ 7524/20158 ( 37%)], Train Loss: 0.17241\n",
      "Epoch: 00 [ 7564/20158 ( 38%)], Train Loss: 0.17188\n",
      "Epoch: 00 [ 7604/20158 ( 38%)], Train Loss: 0.17139\n",
      "Epoch: 00 [ 7644/20158 ( 38%)], Train Loss: 0.17081\n",
      "Epoch: 00 [ 7684/20158 ( 38%)], Train Loss: 0.17052\n",
      "Epoch: 00 [ 7724/20158 ( 38%)], Train Loss: 0.17001\n",
      "Epoch: 00 [ 7764/20158 ( 39%)], Train Loss: 0.16968\n",
      "Epoch: 00 [ 7804/20158 ( 39%)], Train Loss: 0.16931\n",
      "Epoch: 00 [ 7844/20158 ( 39%)], Train Loss: 0.16866\n",
      "Epoch: 00 [ 7884/20158 ( 39%)], Train Loss: 0.16812\n",
      "Epoch: 00 [ 7924/20158 ( 39%)], Train Loss: 0.16782\n",
      "Epoch: 00 [ 7964/20158 ( 40%)], Train Loss: 0.16734\n",
      "Epoch: 00 [ 8004/20158 ( 40%)], Train Loss: 0.16694\n",
      "Epoch: 00 [ 8044/20158 ( 40%)], Train Loss: 0.16644\n",
      "Epoch: 00 [ 8084/20158 ( 40%)], Train Loss: 0.16616\n",
      "Epoch: 00 [ 8124/20158 ( 40%)], Train Loss: 0.16570\n",
      "Epoch: 00 [ 8164/20158 ( 41%)], Train Loss: 0.16522\n",
      "Epoch: 00 [ 8204/20158 ( 41%)], Train Loss: 0.16484\n",
      "Epoch: 00 [ 8244/20158 ( 41%)], Train Loss: 0.16436\n",
      "Epoch: 00 [ 8284/20158 ( 41%)], Train Loss: 0.16394\n",
      "Epoch: 00 [ 8324/20158 ( 41%)], Train Loss: 0.16347\n",
      "Epoch: 00 [ 8364/20158 ( 41%)], Train Loss: 0.16314\n",
      "Epoch: 00 [ 8404/20158 ( 42%)], Train Loss: 0.16272\n",
      "Epoch: 00 [ 8444/20158 ( 42%)], Train Loss: 0.16243\n",
      "Epoch: 00 [ 8484/20158 ( 42%)], Train Loss: 0.16214\n",
      "Epoch: 00 [ 8524/20158 ( 42%)], Train Loss: 0.16173\n",
      "Epoch: 00 [ 8564/20158 ( 42%)], Train Loss: 0.16130\n",
      "Epoch: 00 [ 8604/20158 ( 43%)], Train Loss: 0.16100\n",
      "Epoch: 00 [ 8644/20158 ( 43%)], Train Loss: 0.16069\n",
      "Epoch: 00 [ 8684/20158 ( 43%)], Train Loss: 0.16040\n",
      "Epoch: 00 [ 8724/20158 ( 43%)], Train Loss: 0.16004\n",
      "Epoch: 00 [ 8764/20158 ( 43%)], Train Loss: 0.15983\n",
      "Epoch: 00 [ 8804/20158 ( 44%)], Train Loss: 0.15950\n",
      "Epoch: 00 [ 8844/20158 ( 44%)], Train Loss: 0.15947\n",
      "Epoch: 00 [ 8884/20158 ( 44%)], Train Loss: 0.15892\n",
      "Epoch: 00 [ 8924/20158 ( 44%)], Train Loss: 0.15867\n",
      "Epoch: 00 [ 8964/20158 ( 44%)], Train Loss: 0.15831\n",
      "Epoch: 00 [ 9004/20158 ( 45%)], Train Loss: 0.15802\n",
      "Epoch: 00 [ 9044/20158 ( 45%)], Train Loss: 0.15770\n",
      "Epoch: 00 [ 9084/20158 ( 45%)], Train Loss: 0.15730\n",
      "Epoch: 00 [ 9124/20158 ( 45%)], Train Loss: 0.15700\n",
      "Epoch: 00 [ 9164/20158 ( 45%)], Train Loss: 0.15647\n",
      "Epoch: 00 [ 9204/20158 ( 46%)], Train Loss: 0.15620\n",
      "Epoch: 00 [ 9244/20158 ( 46%)], Train Loss: 0.15591\n",
      "Epoch: 00 [ 9284/20158 ( 46%)], Train Loss: 0.15567\n",
      "Epoch: 00 [ 9324/20158 ( 46%)], Train Loss: 0.15530\n",
      "Epoch: 00 [ 9364/20158 ( 46%)], Train Loss: 0.15494\n",
      "Epoch: 00 [ 9404/20158 ( 47%)], Train Loss: 0.15458\n",
      "Epoch: 00 [ 9444/20158 ( 47%)], Train Loss: 0.15426\n",
      "Epoch: 00 [ 9484/20158 ( 47%)], Train Loss: 0.15393\n",
      "Epoch: 00 [ 9524/20158 ( 47%)], Train Loss: 0.15373\n",
      "Epoch: 00 [ 9564/20158 ( 47%)], Train Loss: 0.15364\n",
      "Epoch: 00 [ 9604/20158 ( 48%)], Train Loss: 0.15335\n",
      "Epoch: 00 [ 9644/20158 ( 48%)], Train Loss: 0.15321\n",
      "Epoch: 00 [ 9684/20158 ( 48%)], Train Loss: 0.15273\n",
      "Epoch: 00 [ 9724/20158 ( 48%)], Train Loss: 0.15240\n",
      "Epoch: 00 [ 9764/20158 ( 48%)], Train Loss: 0.15206\n",
      "Epoch: 00 [ 9804/20158 ( 49%)], Train Loss: 0.15190\n",
      "Epoch: 00 [ 9844/20158 ( 49%)], Train Loss: 0.15168\n",
      "Epoch: 00 [ 9884/20158 ( 49%)], Train Loss: 0.15141\n",
      "Epoch: 00 [ 9924/20158 ( 49%)], Train Loss: 0.15107\n",
      "Epoch: 00 [ 9964/20158 ( 49%)], Train Loss: 0.15099\n",
      "Epoch: 00 [10004/20158 ( 50%)], Train Loss: 0.15100\n",
      "Epoch: 00 [10044/20158 ( 50%)], Train Loss: 0.15060\n",
      "Epoch: 00 [10084/20158 ( 50%)], Train Loss: 0.15029\n",
      "Epoch: 00 [10124/20158 ( 50%)], Train Loss: 0.15006\n",
      "Epoch: 00 [10164/20158 ( 50%)], Train Loss: 0.14976\n",
      "Epoch: 00 [10204/20158 ( 51%)], Train Loss: 0.14954\n",
      "Epoch: 00 [10244/20158 ( 51%)], Train Loss: 0.14939\n",
      "Epoch: 00 [10284/20158 ( 51%)], Train Loss: 0.14911\n",
      "Epoch: 00 [10324/20158 ( 51%)], Train Loss: 0.14875\n",
      "Epoch: 00 [10364/20158 ( 51%)], Train Loss: 0.14856\n",
      "Epoch: 00 [10404/20158 ( 52%)], Train Loss: 0.14824\n",
      "Epoch: 00 [10444/20158 ( 52%)], Train Loss: 0.14803\n",
      "Epoch: 00 [10484/20158 ( 52%)], Train Loss: 0.14793\n",
      "Epoch: 00 [10524/20158 ( 52%)], Train Loss: 0.14769\n",
      "Epoch: 00 [10564/20158 ( 52%)], Train Loss: 0.14742\n",
      "Epoch: 00 [10604/20158 ( 53%)], Train Loss: 0.14741\n",
      "Epoch: 00 [10644/20158 ( 53%)], Train Loss: 0.14729\n",
      "Epoch: 00 [10684/20158 ( 53%)], Train Loss: 0.14730\n",
      "Epoch: 00 [10724/20158 ( 53%)], Train Loss: 0.14726\n",
      "Epoch: 00 [10764/20158 ( 53%)], Train Loss: 0.14694\n",
      "Epoch: 00 [10804/20158 ( 54%)], Train Loss: 0.14682\n",
      "Epoch: 00 [10844/20158 ( 54%)], Train Loss: 0.14643\n",
      "Epoch: 00 [10884/20158 ( 54%)], Train Loss: 0.14607\n",
      "Epoch: 00 [10924/20158 ( 54%)], Train Loss: 0.14576\n",
      "Epoch: 00 [10964/20158 ( 54%)], Train Loss: 0.14552\n",
      "Epoch: 00 [11004/20158 ( 55%)], Train Loss: 0.14527\n",
      "Epoch: 00 [11044/20158 ( 55%)], Train Loss: 0.14495\n",
      "Epoch: 00 [11084/20158 ( 55%)], Train Loss: 0.14471\n",
      "Epoch: 00 [11124/20158 ( 55%)], Train Loss: 0.14447\n",
      "Epoch: 00 [11164/20158 ( 55%)], Train Loss: 0.14433\n",
      "Epoch: 00 [11204/20158 ( 56%)], Train Loss: 0.14412\n",
      "Epoch: 00 [11244/20158 ( 56%)], Train Loss: 0.14381\n",
      "Epoch: 00 [11284/20158 ( 56%)], Train Loss: 0.14363\n",
      "Epoch: 00 [11324/20158 ( 56%)], Train Loss: 0.14342\n",
      "Epoch: 00 [11364/20158 ( 56%)], Train Loss: 0.14329\n",
      "Epoch: 00 [11404/20158 ( 57%)], Train Loss: 0.14311\n",
      "Epoch: 00 [11444/20158 ( 57%)], Train Loss: 0.14297\n",
      "Epoch: 00 [11484/20158 ( 57%)], Train Loss: 0.14286\n",
      "Epoch: 00 [11524/20158 ( 57%)], Train Loss: 0.14263\n",
      "Epoch: 00 [11564/20158 ( 57%)], Train Loss: 0.14239\n",
      "Epoch: 00 [11604/20158 ( 58%)], Train Loss: 0.14221\n",
      "Epoch: 00 [11644/20158 ( 58%)], Train Loss: 0.14191\n",
      "Epoch: 00 [11684/20158 ( 58%)], Train Loss: 0.14178\n",
      "Epoch: 00 [11724/20158 ( 58%)], Train Loss: 0.14147\n",
      "Epoch: 00 [11764/20158 ( 58%)], Train Loss: 0.14134\n",
      "Epoch: 00 [11804/20158 ( 59%)], Train Loss: 0.14135\n",
      "Epoch: 00 [11844/20158 ( 59%)], Train Loss: 0.14111\n",
      "Epoch: 00 [11884/20158 ( 59%)], Train Loss: 0.14092\n",
      "Epoch: 00 [11924/20158 ( 59%)], Train Loss: 0.14077\n",
      "Epoch: 00 [11964/20158 ( 59%)], Train Loss: 0.14068\n",
      "Epoch: 00 [12004/20158 ( 60%)], Train Loss: 0.14041\n",
      "Epoch: 00 [12044/20158 ( 60%)], Train Loss: 0.14022\n",
      "Epoch: 00 [12084/20158 ( 60%)], Train Loss: 0.14014\n",
      "Epoch: 00 [12124/20158 ( 60%)], Train Loss: 0.13989\n",
      "Epoch: 00 [12164/20158 ( 60%)], Train Loss: 0.13969\n",
      "Epoch: 00 [12204/20158 ( 61%)], Train Loss: 0.13941\n",
      "Epoch: 00 [12244/20158 ( 61%)], Train Loss: 0.13951\n",
      "Epoch: 00 [12284/20158 ( 61%)], Train Loss: 0.13932\n",
      "Epoch: 00 [12324/20158 ( 61%)], Train Loss: 0.13905\n",
      "Epoch: 00 [12364/20158 ( 61%)], Train Loss: 0.13905\n",
      "Epoch: 00 [12404/20158 ( 62%)], Train Loss: 0.13906\n",
      "Epoch: 00 [12444/20158 ( 62%)], Train Loss: 0.13884\n",
      "Epoch: 00 [12484/20158 ( 62%)], Train Loss: 0.13876\n",
      "Epoch: 00 [12524/20158 ( 62%)], Train Loss: 0.13857\n",
      "Epoch: 00 [12564/20158 ( 62%)], Train Loss: 0.13854\n",
      "Epoch: 00 [12604/20158 ( 63%)], Train Loss: 0.13846\n",
      "Epoch: 00 [12644/20158 ( 63%)], Train Loss: 0.13836\n",
      "Epoch: 00 [12684/20158 ( 63%)], Train Loss: 0.13824\n",
      "Epoch: 00 [12724/20158 ( 63%)], Train Loss: 0.13802\n",
      "Epoch: 00 [12764/20158 ( 63%)], Train Loss: 0.13777\n",
      "Epoch: 00 [12804/20158 ( 64%)], Train Loss: 0.13775\n",
      "Epoch: 00 [12844/20158 ( 64%)], Train Loss: 0.13763\n",
      "Epoch: 00 [12884/20158 ( 64%)], Train Loss: 0.13752\n",
      "Epoch: 00 [12924/20158 ( 64%)], Train Loss: 0.13751\n",
      "Epoch: 00 [12964/20158 ( 64%)], Train Loss: 0.13748\n",
      "Epoch: 00 [13004/20158 ( 65%)], Train Loss: 0.13748\n",
      "Epoch: 00 [13044/20158 ( 65%)], Train Loss: 0.13734\n",
      "Epoch: 00 [13084/20158 ( 65%)], Train Loss: 0.13714\n",
      "Epoch: 00 [13124/20158 ( 65%)], Train Loss: 0.13707\n",
      "Epoch: 00 [13164/20158 ( 65%)], Train Loss: 0.13687\n",
      "Epoch: 00 [13204/20158 ( 66%)], Train Loss: 0.13690\n",
      "Epoch: 00 [13244/20158 ( 66%)], Train Loss: 0.13675\n",
      "Epoch: 00 [13284/20158 ( 66%)], Train Loss: 0.13646\n",
      "Epoch: 00 [13324/20158 ( 66%)], Train Loss: 0.13640\n",
      "Epoch: 00 [13364/20158 ( 66%)], Train Loss: 0.13635\n",
      "Epoch: 00 [13404/20158 ( 66%)], Train Loss: 0.13612\n",
      "Epoch: 00 [13444/20158 ( 67%)], Train Loss: 0.13600\n",
      "Epoch: 00 [13484/20158 ( 67%)], Train Loss: 0.13592\n",
      "Epoch: 00 [13524/20158 ( 67%)], Train Loss: 0.13595\n",
      "Epoch: 00 [13564/20158 ( 67%)], Train Loss: 0.13582\n",
      "Epoch: 00 [13604/20158 ( 67%)], Train Loss: 0.13572\n",
      "Epoch: 00 [13644/20158 ( 68%)], Train Loss: 0.13564\n",
      "Epoch: 00 [13684/20158 ( 68%)], Train Loss: 0.13542\n",
      "Epoch: 00 [13724/20158 ( 68%)], Train Loss: 0.13530\n",
      "Epoch: 00 [13764/20158 ( 68%)], Train Loss: 0.13512\n",
      "Epoch: 00 [13804/20158 ( 68%)], Train Loss: 0.13490\n",
      "Epoch: 00 [13844/20158 ( 69%)], Train Loss: 0.13485\n",
      "Epoch: 00 [13884/20158 ( 69%)], Train Loss: 0.13475\n",
      "Epoch: 00 [13924/20158 ( 69%)], Train Loss: 0.13464\n",
      "Epoch: 00 [13964/20158 ( 69%)], Train Loss: 0.13454\n",
      "Epoch: 00 [14004/20158 ( 69%)], Train Loss: 0.13452\n",
      "Epoch: 00 [14044/20158 ( 70%)], Train Loss: 0.13432\n",
      "Epoch: 00 [14084/20158 ( 70%)], Train Loss: 0.13419\n",
      "Epoch: 00 [14124/20158 ( 70%)], Train Loss: 0.13407\n",
      "Epoch: 00 [14164/20158 ( 70%)], Train Loss: 0.13391\n",
      "Epoch: 00 [14204/20158 ( 70%)], Train Loss: 0.13373\n",
      "Epoch: 00 [14244/20158 ( 71%)], Train Loss: 0.13343\n",
      "Epoch: 00 [14284/20158 ( 71%)], Train Loss: 0.13317\n",
      "Epoch: 00 [14324/20158 ( 71%)], Train Loss: 0.13304\n",
      "Epoch: 00 [14364/20158 ( 71%)], Train Loss: 0.13297\n",
      "Epoch: 00 [14404/20158 ( 71%)], Train Loss: 0.13295\n",
      "Epoch: 00 [14444/20158 ( 72%)], Train Loss: 0.13282\n",
      "Epoch: 00 [14484/20158 ( 72%)], Train Loss: 0.13270\n",
      "Epoch: 00 [14524/20158 ( 72%)], Train Loss: 0.13266\n",
      "Epoch: 00 [14564/20158 ( 72%)], Train Loss: 0.13253\n",
      "Epoch: 00 [14604/20158 ( 72%)], Train Loss: 0.13244\n",
      "Epoch: 00 [14644/20158 ( 73%)], Train Loss: 0.13222\n",
      "Epoch: 00 [14684/20158 ( 73%)], Train Loss: 0.13202\n",
      "Epoch: 00 [14724/20158 ( 73%)], Train Loss: 0.13181\n",
      "Epoch: 00 [14764/20158 ( 73%)], Train Loss: 0.13175\n",
      "Epoch: 00 [14804/20158 ( 73%)], Train Loss: 0.13167\n",
      "Epoch: 00 [14844/20158 ( 74%)], Train Loss: 0.13153\n",
      "Epoch: 00 [14884/20158 ( 74%)], Train Loss: 0.13157\n",
      "Epoch: 00 [14924/20158 ( 74%)], Train Loss: 0.13143\n",
      "Epoch: 00 [14964/20158 ( 74%)], Train Loss: 0.13118\n",
      "Epoch: 00 [15004/20158 ( 74%)], Train Loss: 0.13103\n",
      "Epoch: 00 [15044/20158 ( 75%)], Train Loss: 0.13093\n",
      "Epoch: 00 [15084/20158 ( 75%)], Train Loss: 0.13081\n",
      "Epoch: 00 [15124/20158 ( 75%)], Train Loss: 0.13078\n",
      "Epoch: 00 [15164/20158 ( 75%)], Train Loss: 0.13069\n",
      "Epoch: 00 [15204/20158 ( 75%)], Train Loss: 0.13046\n",
      "Epoch: 00 [15244/20158 ( 76%)], Train Loss: 0.13039\n",
      "Epoch: 00 [15284/20158 ( 76%)], Train Loss: 0.13036\n",
      "Epoch: 00 [15324/20158 ( 76%)], Train Loss: 0.13025\n",
      "Epoch: 00 [15364/20158 ( 76%)], Train Loss: 0.13015\n",
      "Epoch: 00 [15404/20158 ( 76%)], Train Loss: 0.13017\n",
      "Epoch: 00 [15444/20158 ( 77%)], Train Loss: 0.12999\n",
      "Epoch: 00 [15484/20158 ( 77%)], Train Loss: 0.12986\n",
      "Epoch: 00 [15524/20158 ( 77%)], Train Loss: 0.12976\n",
      "Epoch: 00 [15564/20158 ( 77%)], Train Loss: 0.12961\n",
      "Epoch: 00 [15604/20158 ( 77%)], Train Loss: 0.12948\n",
      "Epoch: 00 [15644/20158 ( 78%)], Train Loss: 0.12929\n",
      "Epoch: 00 [15684/20158 ( 78%)], Train Loss: 0.12910\n",
      "Epoch: 00 [15724/20158 ( 78%)], Train Loss: 0.12898\n",
      "Epoch: 00 [15764/20158 ( 78%)], Train Loss: 0.12879\n",
      "Epoch: 00 [15804/20158 ( 78%)], Train Loss: 0.12877\n",
      "Epoch: 00 [15844/20158 ( 79%)], Train Loss: 0.12872\n",
      "Epoch: 00 [15884/20158 ( 79%)], Train Loss: 0.12865\n",
      "Epoch: 00 [15924/20158 ( 79%)], Train Loss: 0.12858\n",
      "Epoch: 00 [15964/20158 ( 79%)], Train Loss: 0.12857\n",
      "Epoch: 00 [16004/20158 ( 79%)], Train Loss: 0.12846\n",
      "Epoch: 00 [16044/20158 ( 80%)], Train Loss: 0.12834\n",
      "Epoch: 00 [16084/20158 ( 80%)], Train Loss: 0.12823\n",
      "Epoch: 00 [16124/20158 ( 80%)], Train Loss: 0.12813\n",
      "Epoch: 00 [16164/20158 ( 80%)], Train Loss: 0.12804\n",
      "Epoch: 00 [16204/20158 ( 80%)], Train Loss: 0.12792\n",
      "Epoch: 00 [16244/20158 ( 81%)], Train Loss: 0.12781\n",
      "Epoch: 00 [16284/20158 ( 81%)], Train Loss: 0.12771\n",
      "Epoch: 00 [16324/20158 ( 81%)], Train Loss: 0.12757\n",
      "Epoch: 00 [16364/20158 ( 81%)], Train Loss: 0.12757\n",
      "Epoch: 00 [16404/20158 ( 81%)], Train Loss: 0.12746\n",
      "Epoch: 00 [16444/20158 ( 82%)], Train Loss: 0.12733\n",
      "Epoch: 00 [16484/20158 ( 82%)], Train Loss: 0.12731\n",
      "Epoch: 00 [16524/20158 ( 82%)], Train Loss: 0.12726\n",
      "Epoch: 00 [16564/20158 ( 82%)], Train Loss: 0.12706\n",
      "Epoch: 00 [16604/20158 ( 82%)], Train Loss: 0.12704\n",
      "Epoch: 00 [16644/20158 ( 83%)], Train Loss: 0.12685\n",
      "Epoch: 00 [16684/20158 ( 83%)], Train Loss: 0.12672\n",
      "Epoch: 00 [16724/20158 ( 83%)], Train Loss: 0.12652\n",
      "Epoch: 00 [16764/20158 ( 83%)], Train Loss: 0.12637\n",
      "Epoch: 00 [16804/20158 ( 83%)], Train Loss: 0.12639\n",
      "Epoch: 00 [16844/20158 ( 84%)], Train Loss: 0.12634\n",
      "Epoch: 00 [16884/20158 ( 84%)], Train Loss: 0.12613\n",
      "Epoch: 00 [16924/20158 ( 84%)], Train Loss: 0.12608\n",
      "Epoch: 00 [16964/20158 ( 84%)], Train Loss: 0.12590\n",
      "Epoch: 00 [17004/20158 ( 84%)], Train Loss: 0.12604\n",
      "Epoch: 00 [17044/20158 ( 85%)], Train Loss: 0.12591\n",
      "Epoch: 00 [17084/20158 ( 85%)], Train Loss: 0.12579\n",
      "Epoch: 00 [17124/20158 ( 85%)], Train Loss: 0.12568\n",
      "Epoch: 00 [17164/20158 ( 85%)], Train Loss: 0.12551\n",
      "Epoch: 00 [17204/20158 ( 85%)], Train Loss: 0.12537\n",
      "Epoch: 00 [17244/20158 ( 86%)], Train Loss: 0.12528\n",
      "Epoch: 00 [17284/20158 ( 86%)], Train Loss: 0.12515\n",
      "Epoch: 00 [17324/20158 ( 86%)], Train Loss: 0.12517\n",
      "Epoch: 00 [17364/20158 ( 86%)], Train Loss: 0.12504\n",
      "Epoch: 00 [17404/20158 ( 86%)], Train Loss: 0.12500\n",
      "Epoch: 00 [17444/20158 ( 87%)], Train Loss: 0.12491\n",
      "Epoch: 00 [17484/20158 ( 87%)], Train Loss: 0.12478\n",
      "Epoch: 00 [17524/20158 ( 87%)], Train Loss: 0.12470\n",
      "Epoch: 00 [17564/20158 ( 87%)], Train Loss: 0.12454\n",
      "Epoch: 00 [17604/20158 ( 87%)], Train Loss: 0.12440\n",
      "Epoch: 00 [17644/20158 ( 88%)], Train Loss: 0.12430\n",
      "Epoch: 00 [17684/20158 ( 88%)], Train Loss: 0.12421\n",
      "Epoch: 00 [17724/20158 ( 88%)], Train Loss: 0.12425\n",
      "Epoch: 00 [17764/20158 ( 88%)], Train Loss: 0.12421\n",
      "Epoch: 00 [17804/20158 ( 88%)], Train Loss: 0.12415\n",
      "Epoch: 00 [17844/20158 ( 89%)], Train Loss: 0.12414\n",
      "Epoch: 00 [17884/20158 ( 89%)], Train Loss: 0.12412\n",
      "Epoch: 00 [17924/20158 ( 89%)], Train Loss: 0.12398\n",
      "Epoch: 00 [17964/20158 ( 89%)], Train Loss: 0.12390\n",
      "Epoch: 00 [18004/20158 ( 89%)], Train Loss: 0.12377\n",
      "Epoch: 00 [18044/20158 ( 90%)], Train Loss: 0.12369\n",
      "Epoch: 00 [18084/20158 ( 90%)], Train Loss: 0.12364\n",
      "Epoch: 00 [18124/20158 ( 90%)], Train Loss: 0.12351\n",
      "Epoch: 00 [18164/20158 ( 90%)], Train Loss: 0.12332\n",
      "Epoch: 00 [18204/20158 ( 90%)], Train Loss: 0.12318\n",
      "Epoch: 00 [18244/20158 ( 91%)], Train Loss: 0.12312\n",
      "Epoch: 00 [18284/20158 ( 91%)], Train Loss: 0.12307\n",
      "Epoch: 00 [18324/20158 ( 91%)], Train Loss: 0.12296\n",
      "Epoch: 00 [18364/20158 ( 91%)], Train Loss: 0.12296\n",
      "Epoch: 00 [18404/20158 ( 91%)], Train Loss: 0.12289\n",
      "Epoch: 00 [18444/20158 ( 91%)], Train Loss: 0.12279\n",
      "Epoch: 00 [18484/20158 ( 92%)], Train Loss: 0.12269\n",
      "Epoch: 00 [18524/20158 ( 92%)], Train Loss: 0.12266\n",
      "Epoch: 00 [18564/20158 ( 92%)], Train Loss: 0.12268\n",
      "Epoch: 00 [18604/20158 ( 92%)], Train Loss: 0.12262\n",
      "Epoch: 00 [18644/20158 ( 92%)], Train Loss: 0.12259\n",
      "Epoch: 00 [18684/20158 ( 93%)], Train Loss: 0.12245\n",
      "Epoch: 00 [18724/20158 ( 93%)], Train Loss: 0.12237\n",
      "Epoch: 00 [18764/20158 ( 93%)], Train Loss: 0.12231\n",
      "Epoch: 00 [18804/20158 ( 93%)], Train Loss: 0.12224\n",
      "Epoch: 00 [18844/20158 ( 93%)], Train Loss: 0.12209\n",
      "Epoch: 00 [18884/20158 ( 94%)], Train Loss: 0.12196\n",
      "Epoch: 00 [18924/20158 ( 94%)], Train Loss: 0.12187\n",
      "Epoch: 00 [18964/20158 ( 94%)], Train Loss: 0.12177\n",
      "Epoch: 00 [19004/20158 ( 94%)], Train Loss: 0.12161\n",
      "Epoch: 00 [19044/20158 ( 94%)], Train Loss: 0.12147\n",
      "Epoch: 00 [19084/20158 ( 95%)], Train Loss: 0.12142\n",
      "Epoch: 00 [19124/20158 ( 95%)], Train Loss: 0.12132\n",
      "Epoch: 00 [19164/20158 ( 95%)], Train Loss: 0.12117\n",
      "Epoch: 00 [19204/20158 ( 95%)], Train Loss: 0.12109\n",
      "Epoch: 00 [19244/20158 ( 95%)], Train Loss: 0.12109\n",
      "Epoch: 00 [19284/20158 ( 96%)], Train Loss: 0.12101\n",
      "Epoch: 00 [19324/20158 ( 96%)], Train Loss: 0.12087\n",
      "Epoch: 00 [19364/20158 ( 96%)], Train Loss: 0.12083\n",
      "Epoch: 00 [19404/20158 ( 96%)], Train Loss: 0.12075\n",
      "Epoch: 00 [19444/20158 ( 96%)], Train Loss: 0.12066\n",
      "Epoch: 00 [19484/20158 ( 97%)], Train Loss: 0.12059\n",
      "Epoch: 00 [19524/20158 ( 97%)], Train Loss: 0.12044\n",
      "Epoch: 00 [19564/20158 ( 97%)], Train Loss: 0.12030\n",
      "Epoch: 00 [19604/20158 ( 97%)], Train Loss: 0.12028\n",
      "Epoch: 00 [19644/20158 ( 97%)], Train Loss: 0.12017\n",
      "Epoch: 00 [19684/20158 ( 98%)], Train Loss: 0.12013\n",
      "Epoch: 00 [19724/20158 ( 98%)], Train Loss: 0.11999\n",
      "Epoch: 00 [19764/20158 ( 98%)], Train Loss: 0.11985\n",
      "Epoch: 00 [19804/20158 ( 98%)], Train Loss: 0.11977\n",
      "Epoch: 00 [19844/20158 ( 98%)], Train Loss: 0.11975\n",
      "Epoch: 00 [19884/20158 ( 99%)], Train Loss: 0.11964\n",
      "Epoch: 00 [19924/20158 ( 99%)], Train Loss: 0.11960\n",
      "Epoch: 00 [19964/20158 ( 99%)], Train Loss: 0.11955\n",
      "Epoch: 00 [20004/20158 ( 99%)], Train Loss: 0.11953\n",
      "Epoch: 00 [20044/20158 ( 99%)], Train Loss: 0.11944\n",
      "Epoch: 00 [20084/20158 (100%)], Train Loss: 0.11931\n",
      "Epoch: 00 [20124/20158 (100%)], Train Loss: 0.11923\n",
      "Epoch: 00 [20158/20158 (100%)], Train Loss: 0.11921\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.26836\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.26836\n",
      "Saving model checkpoint to output/checkpoint-fold-3.\n",
      "\n",
      "Epoch: 01 [    4/20158 (  0%)], Train Loss: 0.04185\n",
      "Epoch: 01 [   44/20158 (  0%)], Train Loss: 0.05660\n",
      "Epoch: 01 [   84/20158 (  0%)], Train Loss: 0.06883\n",
      "Epoch: 01 [  124/20158 (  1%)], Train Loss: 0.07662\n",
      "Epoch: 01 [  164/20158 (  1%)], Train Loss: 0.07062\n",
      "Epoch: 01 [  204/20158 (  1%)], Train Loss: 0.07501\n",
      "Epoch: 01 [  244/20158 (  1%)], Train Loss: 0.07500\n",
      "Epoch: 01 [  284/20158 (  1%)], Train Loss: 0.08187\n",
      "Epoch: 01 [  324/20158 (  2%)], Train Loss: 0.08016\n",
      "Epoch: 01 [  364/20158 (  2%)], Train Loss: 0.08141\n",
      "Epoch: 01 [  404/20158 (  2%)], Train Loss: 0.08092\n",
      "Epoch: 01 [  444/20158 (  2%)], Train Loss: 0.07880\n",
      "Epoch: 01 [  484/20158 (  2%)], Train Loss: 0.08117\n",
      "Epoch: 01 [  524/20158 (  3%)], Train Loss: 0.08109\n",
      "Epoch: 01 [  564/20158 (  3%)], Train Loss: 0.08372\n",
      "Epoch: 01 [  604/20158 (  3%)], Train Loss: 0.08322\n",
      "Epoch: 01 [  644/20158 (  3%)], Train Loss: 0.08136\n",
      "Epoch: 01 [  684/20158 (  3%)], Train Loss: 0.08304\n",
      "Epoch: 01 [  724/20158 (  4%)], Train Loss: 0.08031\n",
      "Epoch: 01 [  764/20158 (  4%)], Train Loss: 0.08016\n",
      "Epoch: 01 [  804/20158 (  4%)], Train Loss: 0.07871\n",
      "Epoch: 01 [  844/20158 (  4%)], Train Loss: 0.07829\n",
      "Epoch: 01 [  884/20158 (  4%)], Train Loss: 0.07844\n",
      "Epoch: 01 [  924/20158 (  5%)], Train Loss: 0.07764\n",
      "Epoch: 01 [  964/20158 (  5%)], Train Loss: 0.07514\n",
      "Epoch: 01 [ 1004/20158 (  5%)], Train Loss: 0.07456\n",
      "Epoch: 01 [ 1044/20158 (  5%)], Train Loss: 0.07516\n",
      "Epoch: 01 [ 1084/20158 (  5%)], Train Loss: 0.07600\n",
      "Epoch: 01 [ 1124/20158 (  6%)], Train Loss: 0.07615\n",
      "Epoch: 01 [ 1164/20158 (  6%)], Train Loss: 0.07655\n",
      "Epoch: 01 [ 1204/20158 (  6%)], Train Loss: 0.07476\n",
      "Epoch: 01 [ 1244/20158 (  6%)], Train Loss: 0.07522\n",
      "Epoch: 01 [ 1284/20158 (  6%)], Train Loss: 0.07452\n",
      "Epoch: 01 [ 1324/20158 (  7%)], Train Loss: 0.07434\n",
      "Epoch: 01 [ 1364/20158 (  7%)], Train Loss: 0.07438\n",
      "Epoch: 01 [ 1404/20158 (  7%)], Train Loss: 0.07427\n",
      "Epoch: 01 [ 1444/20158 (  7%)], Train Loss: 0.07422\n",
      "Epoch: 01 [ 1484/20158 (  7%)], Train Loss: 0.07409\n",
      "Epoch: 01 [ 1524/20158 (  8%)], Train Loss: 0.07413\n",
      "Epoch: 01 [ 1564/20158 (  8%)], Train Loss: 0.07433\n",
      "Epoch: 01 [ 1604/20158 (  8%)], Train Loss: 0.07452\n",
      "Epoch: 01 [ 1644/20158 (  8%)], Train Loss: 0.07389\n",
      "Epoch: 01 [ 1684/20158 (  8%)], Train Loss: 0.07383\n",
      "Epoch: 01 [ 1724/20158 (  9%)], Train Loss: 0.07282\n",
      "Epoch: 01 [ 1764/20158 (  9%)], Train Loss: 0.07233\n",
      "Epoch: 01 [ 1804/20158 (  9%)], Train Loss: 0.07168\n",
      "Epoch: 01 [ 1844/20158 (  9%)], Train Loss: 0.07179\n",
      "Epoch: 01 [ 1884/20158 (  9%)], Train Loss: 0.07117\n",
      "Epoch: 01 [ 1924/20158 ( 10%)], Train Loss: 0.07120\n",
      "Epoch: 01 [ 1964/20158 ( 10%)], Train Loss: 0.07023\n",
      "Epoch: 01 [ 2004/20158 ( 10%)], Train Loss: 0.07018\n",
      "Epoch: 01 [ 2044/20158 ( 10%)], Train Loss: 0.06994\n",
      "Epoch: 01 [ 2084/20158 ( 10%)], Train Loss: 0.07001\n",
      "Epoch: 01 [ 2124/20158 ( 11%)], Train Loss: 0.06981\n",
      "Epoch: 01 [ 2164/20158 ( 11%)], Train Loss: 0.06977\n",
      "Epoch: 01 [ 2204/20158 ( 11%)], Train Loss: 0.06918\n",
      "Epoch: 01 [ 2244/20158 ( 11%)], Train Loss: 0.06860\n",
      "Epoch: 01 [ 2284/20158 ( 11%)], Train Loss: 0.06883\n",
      "Epoch: 01 [ 2324/20158 ( 12%)], Train Loss: 0.06903\n",
      "Epoch: 01 [ 2364/20158 ( 12%)], Train Loss: 0.06803\n",
      "Epoch: 01 [ 2404/20158 ( 12%)], Train Loss: 0.06787\n",
      "Epoch: 01 [ 2444/20158 ( 12%)], Train Loss: 0.06729\n",
      "Epoch: 01 [ 2484/20158 ( 12%)], Train Loss: 0.06673\n",
      "Epoch: 01 [ 2524/20158 ( 13%)], Train Loss: 0.06676\n",
      "Epoch: 01 [ 2564/20158 ( 13%)], Train Loss: 0.06682\n",
      "Epoch: 01 [ 2604/20158 ( 13%)], Train Loss: 0.06656\n",
      "Epoch: 01 [ 2644/20158 ( 13%)], Train Loss: 0.06634\n",
      "Epoch: 01 [ 2684/20158 ( 13%)], Train Loss: 0.06575\n",
      "Epoch: 01 [ 2724/20158 ( 14%)], Train Loss: 0.06567\n",
      "Epoch: 01 [ 2764/20158 ( 14%)], Train Loss: 0.06512\n",
      "Epoch: 01 [ 2804/20158 ( 14%)], Train Loss: 0.06504\n",
      "Epoch: 01 [ 2844/20158 ( 14%)], Train Loss: 0.06462\n",
      "Epoch: 01 [ 2884/20158 ( 14%)], Train Loss: 0.06403\n",
      "Epoch: 01 [ 2924/20158 ( 15%)], Train Loss: 0.06363\n",
      "Epoch: 01 [ 2964/20158 ( 15%)], Train Loss: 0.06355\n",
      "Epoch: 01 [ 3004/20158 ( 15%)], Train Loss: 0.06315\n",
      "Epoch: 01 [ 3044/20158 ( 15%)], Train Loss: 0.06295\n",
      "Epoch: 01 [ 3084/20158 ( 15%)], Train Loss: 0.06275\n",
      "Epoch: 01 [ 3124/20158 ( 15%)], Train Loss: 0.06228\n",
      "Epoch: 01 [ 3164/20158 ( 16%)], Train Loss: 0.06171\n",
      "Epoch: 01 [ 3204/20158 ( 16%)], Train Loss: 0.06120\n",
      "Epoch: 01 [ 3244/20158 ( 16%)], Train Loss: 0.06113\n",
      "Epoch: 01 [ 3284/20158 ( 16%)], Train Loss: 0.06088\n",
      "Epoch: 01 [ 3324/20158 ( 16%)], Train Loss: 0.06077\n",
      "Epoch: 01 [ 3364/20158 ( 17%)], Train Loss: 0.06040\n",
      "Epoch: 01 [ 3404/20158 ( 17%)], Train Loss: 0.06019\n",
      "Epoch: 01 [ 3444/20158 ( 17%)], Train Loss: 0.05961\n",
      "Epoch: 01 [ 3484/20158 ( 17%)], Train Loss: 0.05945\n",
      "Epoch: 01 [ 3524/20158 ( 17%)], Train Loss: 0.05922\n",
      "Epoch: 01 [ 3564/20158 ( 18%)], Train Loss: 0.05890\n",
      "Epoch: 01 [ 3604/20158 ( 18%)], Train Loss: 0.05867\n",
      "Epoch: 01 [ 3644/20158 ( 18%)], Train Loss: 0.05844\n",
      "Epoch: 01 [ 3684/20158 ( 18%)], Train Loss: 0.05813\n",
      "Epoch: 01 [ 3724/20158 ( 18%)], Train Loss: 0.05804\n",
      "Epoch: 01 [ 3764/20158 ( 19%)], Train Loss: 0.05790\n",
      "Epoch: 01 [ 3804/20158 ( 19%)], Train Loss: 0.05761\n",
      "Epoch: 01 [ 3844/20158 ( 19%)], Train Loss: 0.05738\n",
      "Epoch: 01 [ 3884/20158 ( 19%)], Train Loss: 0.05713\n",
      "Epoch: 01 [ 3924/20158 ( 19%)], Train Loss: 0.05707\n",
      "Epoch: 01 [ 3964/20158 ( 20%)], Train Loss: 0.05676\n",
      "Epoch: 01 [ 4004/20158 ( 20%)], Train Loss: 0.05671\n",
      "Epoch: 01 [ 4044/20158 ( 20%)], Train Loss: 0.05648\n",
      "Epoch: 01 [ 4084/20158 ( 20%)], Train Loss: 0.05629\n",
      "Epoch: 01 [ 4124/20158 ( 20%)], Train Loss: 0.05587\n",
      "Epoch: 01 [ 4164/20158 ( 21%)], Train Loss: 0.05556\n",
      "Epoch: 01 [ 4204/20158 ( 21%)], Train Loss: 0.05516\n",
      "Epoch: 01 [ 4244/20158 ( 21%)], Train Loss: 0.05482\n",
      "Epoch: 01 [ 4284/20158 ( 21%)], Train Loss: 0.05464\n",
      "Epoch: 01 [ 4324/20158 ( 21%)], Train Loss: 0.05426\n",
      "Epoch: 01 [ 4364/20158 ( 22%)], Train Loss: 0.05417\n",
      "Epoch: 01 [ 4404/20158 ( 22%)], Train Loss: 0.05400\n",
      "Epoch: 01 [ 4444/20158 ( 22%)], Train Loss: 0.05401\n",
      "Epoch: 01 [ 4484/20158 ( 22%)], Train Loss: 0.05382\n",
      "Epoch: 01 [ 4524/20158 ( 22%)], Train Loss: 0.05346\n",
      "Epoch: 01 [ 4564/20158 ( 23%)], Train Loss: 0.05320\n",
      "Epoch: 01 [ 4604/20158 ( 23%)], Train Loss: 0.05299\n",
      "Epoch: 01 [ 4644/20158 ( 23%)], Train Loss: 0.05275\n",
      "Epoch: 01 [ 4684/20158 ( 23%)], Train Loss: 0.05244\n",
      "Epoch: 01 [ 4724/20158 ( 23%)], Train Loss: 0.05212\n",
      "Epoch: 01 [ 4764/20158 ( 24%)], Train Loss: 0.05220\n",
      "Epoch: 01 [ 4804/20158 ( 24%)], Train Loss: 0.05218\n",
      "Epoch: 01 [ 4844/20158 ( 24%)], Train Loss: 0.05206\n",
      "Epoch: 01 [ 4884/20158 ( 24%)], Train Loss: 0.05185\n",
      "Epoch: 01 [ 4924/20158 ( 24%)], Train Loss: 0.05169\n",
      "Epoch: 01 [ 4964/20158 ( 25%)], Train Loss: 0.05148\n",
      "Epoch: 01 [ 5004/20158 ( 25%)], Train Loss: 0.05187\n",
      "Epoch: 01 [ 5044/20158 ( 25%)], Train Loss: 0.05165\n",
      "Epoch: 01 [ 5084/20158 ( 25%)], Train Loss: 0.05158\n",
      "Epoch: 01 [ 5124/20158 ( 25%)], Train Loss: 0.05143\n",
      "Epoch: 01 [ 5164/20158 ( 26%)], Train Loss: 0.05123\n",
      "Epoch: 01 [ 5204/20158 ( 26%)], Train Loss: 0.05124\n",
      "Epoch: 01 [ 5244/20158 ( 26%)], Train Loss: 0.05108\n",
      "Epoch: 01 [ 5284/20158 ( 26%)], Train Loss: 0.05092\n",
      "Epoch: 01 [ 5324/20158 ( 26%)], Train Loss: 0.05081\n",
      "Epoch: 01 [ 5364/20158 ( 27%)], Train Loss: 0.05065\n",
      "Epoch: 01 [ 5404/20158 ( 27%)], Train Loss: 0.05060\n",
      "Epoch: 01 [ 5444/20158 ( 27%)], Train Loss: 0.05055\n",
      "Epoch: 01 [ 5484/20158 ( 27%)], Train Loss: 0.05041\n",
      "Epoch: 01 [ 5524/20158 ( 27%)], Train Loss: 0.05019\n",
      "Epoch: 01 [ 5564/20158 ( 28%)], Train Loss: 0.05005\n",
      "Epoch: 01 [ 5604/20158 ( 28%)], Train Loss: 0.05000\n",
      "Epoch: 01 [ 5644/20158 ( 28%)], Train Loss: 0.04976\n",
      "Epoch: 01 [ 5684/20158 ( 28%)], Train Loss: 0.04958\n",
      "Epoch: 01 [ 5724/20158 ( 28%)], Train Loss: 0.04934\n",
      "Epoch: 01 [ 5764/20158 ( 29%)], Train Loss: 0.04917\n",
      "Epoch: 01 [ 5804/20158 ( 29%)], Train Loss: 0.04909\n",
      "Epoch: 01 [ 5844/20158 ( 29%)], Train Loss: 0.04891\n",
      "Epoch: 01 [ 5884/20158 ( 29%)], Train Loss: 0.04880\n",
      "Epoch: 01 [ 5924/20158 ( 29%)], Train Loss: 0.04858\n",
      "Epoch: 01 [ 5964/20158 ( 30%)], Train Loss: 0.04838\n",
      "Epoch: 01 [ 6004/20158 ( 30%)], Train Loss: 0.04828\n",
      "Epoch: 01 [ 6044/20158 ( 30%)], Train Loss: 0.04808\n",
      "Epoch: 01 [ 6084/20158 ( 30%)], Train Loss: 0.04791\n",
      "Epoch: 01 [ 6124/20158 ( 30%)], Train Loss: 0.04774\n",
      "Epoch: 01 [ 6164/20158 ( 31%)], Train Loss: 0.04779\n",
      "Epoch: 01 [ 6204/20158 ( 31%)], Train Loss: 0.04757\n",
      "Epoch: 01 [ 6244/20158 ( 31%)], Train Loss: 0.04742\n",
      "Epoch: 01 [ 6284/20158 ( 31%)], Train Loss: 0.04733\n",
      "Epoch: 01 [ 6324/20158 ( 31%)], Train Loss: 0.04719\n",
      "Epoch: 01 [ 6364/20158 ( 32%)], Train Loss: 0.04720\n",
      "Epoch: 01 [ 6404/20158 ( 32%)], Train Loss: 0.04698\n",
      "Epoch: 01 [ 6444/20158 ( 32%)], Train Loss: 0.04682\n",
      "Epoch: 01 [ 6484/20158 ( 32%)], Train Loss: 0.04696\n",
      "Epoch: 01 [ 6524/20158 ( 32%)], Train Loss: 0.04677\n",
      "Epoch: 01 [ 6564/20158 ( 33%)], Train Loss: 0.04667\n",
      "Epoch: 01 [ 6604/20158 ( 33%)], Train Loss: 0.04650\n",
      "Epoch: 01 [ 6644/20158 ( 33%)], Train Loss: 0.04631\n",
      "Epoch: 01 [ 6684/20158 ( 33%)], Train Loss: 0.04609\n",
      "Epoch: 01 [ 6724/20158 ( 33%)], Train Loss: 0.04617\n",
      "Epoch: 01 [ 6764/20158 ( 34%)], Train Loss: 0.04601\n",
      "Epoch: 01 [ 6804/20158 ( 34%)], Train Loss: 0.04617\n",
      "Epoch: 01 [ 6844/20158 ( 34%)], Train Loss: 0.04604\n",
      "Epoch: 01 [ 6884/20158 ( 34%)], Train Loss: 0.04588\n",
      "Epoch: 01 [ 6924/20158 ( 34%)], Train Loss: 0.04586\n",
      "Epoch: 01 [ 6964/20158 ( 35%)], Train Loss: 0.04594\n",
      "Epoch: 01 [ 7004/20158 ( 35%)], Train Loss: 0.04578\n",
      "Epoch: 01 [ 7044/20158 ( 35%)], Train Loss: 0.04558\n",
      "Epoch: 01 [ 7084/20158 ( 35%)], Train Loss: 0.04554\n",
      "Epoch: 01 [ 7124/20158 ( 35%)], Train Loss: 0.04536\n",
      "Epoch: 01 [ 7164/20158 ( 36%)], Train Loss: 0.04522\n",
      "Epoch: 01 [ 7204/20158 ( 36%)], Train Loss: 0.04513\n",
      "Epoch: 01 [ 7244/20158 ( 36%)], Train Loss: 0.04500\n",
      "Epoch: 01 [ 7284/20158 ( 36%)], Train Loss: 0.04484\n",
      "Epoch: 01 [ 7324/20158 ( 36%)], Train Loss: 0.04485\n",
      "Epoch: 01 [ 7364/20158 ( 37%)], Train Loss: 0.04476\n",
      "Epoch: 01 [ 7404/20158 ( 37%)], Train Loss: 0.04470\n",
      "Epoch: 01 [ 7444/20158 ( 37%)], Train Loss: 0.04461\n",
      "Epoch: 01 [ 7484/20158 ( 37%)], Train Loss: 0.04450\n",
      "Epoch: 01 [ 7524/20158 ( 37%)], Train Loss: 0.04436\n",
      "Epoch: 01 [ 7564/20158 ( 38%)], Train Loss: 0.04424\n",
      "Epoch: 01 [ 7604/20158 ( 38%)], Train Loss: 0.04420\n",
      "Epoch: 01 [ 7644/20158 ( 38%)], Train Loss: 0.04406\n",
      "Epoch: 01 [ 7684/20158 ( 38%)], Train Loss: 0.04400\n",
      "Epoch: 01 [ 7724/20158 ( 38%)], Train Loss: 0.04392\n",
      "Epoch: 01 [ 7764/20158 ( 39%)], Train Loss: 0.04391\n",
      "Epoch: 01 [ 7804/20158 ( 39%)], Train Loss: 0.04385\n",
      "Epoch: 01 [ 7844/20158 ( 39%)], Train Loss: 0.04370\n",
      "Epoch: 01 [ 7884/20158 ( 39%)], Train Loss: 0.04358\n",
      "Epoch: 01 [ 7924/20158 ( 39%)], Train Loss: 0.04346\n",
      "Epoch: 01 [ 7964/20158 ( 40%)], Train Loss: 0.04331\n",
      "Epoch: 01 [ 8004/20158 ( 40%)], Train Loss: 0.04319\n",
      "Epoch: 01 [ 8044/20158 ( 40%)], Train Loss: 0.04307\n",
      "Epoch: 01 [ 8084/20158 ( 40%)], Train Loss: 0.04306\n",
      "Epoch: 01 [ 8124/20158 ( 40%)], Train Loss: 0.04295\n",
      "Epoch: 01 [ 8164/20158 ( 41%)], Train Loss: 0.04279\n",
      "Epoch: 01 [ 8204/20158 ( 41%)], Train Loss: 0.04266\n",
      "Epoch: 01 [ 8244/20158 ( 41%)], Train Loss: 0.04254\n",
      "Epoch: 01 [ 8284/20158 ( 41%)], Train Loss: 0.04240\n",
      "Epoch: 01 [ 8324/20158 ( 41%)], Train Loss: 0.04228\n",
      "Epoch: 01 [ 8364/20158 ( 41%)], Train Loss: 0.04221\n",
      "Epoch: 01 [ 8404/20158 ( 42%)], Train Loss: 0.04206\n",
      "Epoch: 01 [ 8444/20158 ( 42%)], Train Loss: 0.04202\n",
      "Epoch: 01 [ 8484/20158 ( 42%)], Train Loss: 0.04193\n",
      "Epoch: 01 [ 8524/20158 ( 42%)], Train Loss: 0.04180\n",
      "Epoch: 01 [ 8564/20158 ( 42%)], Train Loss: 0.04166\n",
      "Epoch: 01 [ 8604/20158 ( 43%)], Train Loss: 0.04166\n",
      "Epoch: 01 [ 8644/20158 ( 43%)], Train Loss: 0.04161\n",
      "Epoch: 01 [ 8684/20158 ( 43%)], Train Loss: 0.04147\n",
      "Epoch: 01 [ 8724/20158 ( 43%)], Train Loss: 0.04137\n",
      "Epoch: 01 [ 8764/20158 ( 43%)], Train Loss: 0.04141\n",
      "Epoch: 01 [ 8804/20158 ( 44%)], Train Loss: 0.04132\n",
      "Epoch: 01 [ 8844/20158 ( 44%)], Train Loss: 0.04137\n",
      "Epoch: 01 [ 8884/20158 ( 44%)], Train Loss: 0.04124\n",
      "Epoch: 01 [ 8924/20158 ( 44%)], Train Loss: 0.04116\n",
      "Epoch: 01 [ 8964/20158 ( 44%)], Train Loss: 0.04109\n",
      "Epoch: 01 [ 9004/20158 ( 45%)], Train Loss: 0.04105\n",
      "Epoch: 01 [ 9044/20158 ( 45%)], Train Loss: 0.04097\n",
      "Epoch: 01 [ 9084/20158 ( 45%)], Train Loss: 0.04089\n",
      "Epoch: 01 [ 9124/20158 ( 45%)], Train Loss: 0.04076\n",
      "Epoch: 01 [ 9164/20158 ( 45%)], Train Loss: 0.04062\n",
      "Epoch: 01 [ 9204/20158 ( 46%)], Train Loss: 0.04056\n",
      "Epoch: 01 [ 9244/20158 ( 46%)], Train Loss: 0.04055\n",
      "Epoch: 01 [ 9284/20158 ( 46%)], Train Loss: 0.04053\n",
      "Epoch: 01 [ 9324/20158 ( 46%)], Train Loss: 0.04043\n",
      "Epoch: 01 [ 9364/20158 ( 46%)], Train Loss: 0.04031\n",
      "Epoch: 01 [ 9404/20158 ( 47%)], Train Loss: 0.04021\n",
      "Epoch: 01 [ 9444/20158 ( 47%)], Train Loss: 0.04013\n",
      "Epoch: 01 [ 9484/20158 ( 47%)], Train Loss: 0.04004\n",
      "Epoch: 01 [ 9524/20158 ( 47%)], Train Loss: 0.04006\n",
      "Epoch: 01 [ 9564/20158 ( 47%)], Train Loss: 0.04004\n",
      "Epoch: 01 [ 9604/20158 ( 48%)], Train Loss: 0.03998\n",
      "Epoch: 01 [ 9644/20158 ( 48%)], Train Loss: 0.04004\n",
      "Epoch: 01 [ 9684/20158 ( 48%)], Train Loss: 0.03991\n",
      "Epoch: 01 [ 9724/20158 ( 48%)], Train Loss: 0.03980\n",
      "Epoch: 01 [ 9764/20158 ( 48%)], Train Loss: 0.03970\n",
      "Epoch: 01 [ 9804/20158 ( 49%)], Train Loss: 0.03970\n",
      "Epoch: 01 [ 9844/20158 ( 49%)], Train Loss: 0.03967\n",
      "Epoch: 01 [ 9884/20158 ( 49%)], Train Loss: 0.03959\n",
      "Epoch: 01 [ 9924/20158 ( 49%)], Train Loss: 0.03950\n",
      "Epoch: 01 [ 9964/20158 ( 49%)], Train Loss: 0.03957\n",
      "Epoch: 01 [10004/20158 ( 50%)], Train Loss: 0.03967\n",
      "Epoch: 01 [10044/20158 ( 50%)], Train Loss: 0.03953\n",
      "Epoch: 01 [10084/20158 ( 50%)], Train Loss: 0.03945\n",
      "Epoch: 01 [10124/20158 ( 50%)], Train Loss: 0.03941\n",
      "Epoch: 01 [10164/20158 ( 50%)], Train Loss: 0.03933\n",
      "Epoch: 01 [10204/20158 ( 51%)], Train Loss: 0.03932\n",
      "Epoch: 01 [10244/20158 ( 51%)], Train Loss: 0.03926\n",
      "Epoch: 01 [10284/20158 ( 51%)], Train Loss: 0.03920\n",
      "Epoch: 01 [10324/20158 ( 51%)], Train Loss: 0.03911\n",
      "Epoch: 01 [10364/20158 ( 51%)], Train Loss: 0.03909\n",
      "Epoch: 01 [10404/20158 ( 52%)], Train Loss: 0.03900\n",
      "Epoch: 01 [10444/20158 ( 52%)], Train Loss: 0.03898\n",
      "Epoch: 01 [10484/20158 ( 52%)], Train Loss: 0.03905\n",
      "Epoch: 01 [10524/20158 ( 52%)], Train Loss: 0.03904\n",
      "Epoch: 01 [10564/20158 ( 52%)], Train Loss: 0.03907\n",
      "Epoch: 01 [10604/20158 ( 53%)], Train Loss: 0.03908\n",
      "Epoch: 01 [10644/20158 ( 53%)], Train Loss: 0.03904\n",
      "Epoch: 01 [10684/20158 ( 53%)], Train Loss: 0.03916\n",
      "Epoch: 01 [10724/20158 ( 53%)], Train Loss: 0.03932\n",
      "Epoch: 01 [10764/20158 ( 53%)], Train Loss: 0.03927\n",
      "Epoch: 01 [10804/20158 ( 54%)], Train Loss: 0.03932\n",
      "Epoch: 01 [10844/20158 ( 54%)], Train Loss: 0.03920\n",
      "Epoch: 01 [10884/20158 ( 54%)], Train Loss: 0.03908\n",
      "Epoch: 01 [10924/20158 ( 54%)], Train Loss: 0.03901\n",
      "Epoch: 01 [10964/20158 ( 54%)], Train Loss: 0.03893\n",
      "Epoch: 01 [11004/20158 ( 55%)], Train Loss: 0.03889\n",
      "Epoch: 01 [11044/20158 ( 55%)], Train Loss: 0.03884\n",
      "Epoch: 01 [11084/20158 ( 55%)], Train Loss: 0.03880\n",
      "Epoch: 01 [11124/20158 ( 55%)], Train Loss: 0.03872\n",
      "Epoch: 01 [11164/20158 ( 55%)], Train Loss: 0.03867\n",
      "Epoch: 01 [11204/20158 ( 56%)], Train Loss: 0.03862\n",
      "Epoch: 01 [11244/20158 ( 56%)], Train Loss: 0.03857\n",
      "Epoch: 01 [11284/20158 ( 56%)], Train Loss: 0.03856\n",
      "Epoch: 01 [11324/20158 ( 56%)], Train Loss: 0.03855\n",
      "Epoch: 01 [11364/20158 ( 56%)], Train Loss: 0.03849\n",
      "Epoch: 01 [11404/20158 ( 57%)], Train Loss: 0.03844\n",
      "Epoch: 01 [11444/20158 ( 57%)], Train Loss: 0.03839\n",
      "Epoch: 01 [11484/20158 ( 57%)], Train Loss: 0.03837\n",
      "Epoch: 01 [11524/20158 ( 57%)], Train Loss: 0.03837\n",
      "Epoch: 01 [11564/20158 ( 57%)], Train Loss: 0.03831\n",
      "Epoch: 01 [11604/20158 ( 58%)], Train Loss: 0.03832\n",
      "Epoch: 01 [11644/20158 ( 58%)], Train Loss: 0.03823\n",
      "Epoch: 01 [11684/20158 ( 58%)], Train Loss: 0.03821\n",
      "Epoch: 01 [11724/20158 ( 58%)], Train Loss: 0.03814\n",
      "Epoch: 01 [11764/20158 ( 58%)], Train Loss: 0.03817\n",
      "Epoch: 01 [11804/20158 ( 59%)], Train Loss: 0.03827\n",
      "Epoch: 01 [11844/20158 ( 59%)], Train Loss: 0.03823\n",
      "Epoch: 01 [11884/20158 ( 59%)], Train Loss: 0.03823\n",
      "Epoch: 01 [11924/20158 ( 59%)], Train Loss: 0.03819\n",
      "Epoch: 01 [11964/20158 ( 59%)], Train Loss: 0.03821\n",
      "Epoch: 01 [12004/20158 ( 60%)], Train Loss: 0.03820\n",
      "Epoch: 01 [12044/20158 ( 60%)], Train Loss: 0.03814\n",
      "Epoch: 01 [12084/20158 ( 60%)], Train Loss: 0.03815\n",
      "Epoch: 01 [12124/20158 ( 60%)], Train Loss: 0.03808\n",
      "Epoch: 01 [12164/20158 ( 60%)], Train Loss: 0.03802\n",
      "Epoch: 01 [12204/20158 ( 61%)], Train Loss: 0.03796\n",
      "Epoch: 01 [12244/20158 ( 61%)], Train Loss: 0.03799\n",
      "Epoch: 01 [12284/20158 ( 61%)], Train Loss: 0.03793\n",
      "Epoch: 01 [12324/20158 ( 61%)], Train Loss: 0.03784\n",
      "Epoch: 01 [12364/20158 ( 61%)], Train Loss: 0.03782\n",
      "Epoch: 01 [12404/20158 ( 62%)], Train Loss: 0.03780\n",
      "Epoch: 01 [12444/20158 ( 62%)], Train Loss: 0.03772\n",
      "Epoch: 01 [12484/20158 ( 62%)], Train Loss: 0.03781\n",
      "Epoch: 01 [12524/20158 ( 62%)], Train Loss: 0.03778\n",
      "Epoch: 01 [12564/20158 ( 62%)], Train Loss: 0.03780\n",
      "Epoch: 01 [12604/20158 ( 63%)], Train Loss: 0.03785\n",
      "Epoch: 01 [12644/20158 ( 63%)], Train Loss: 0.03790\n",
      "Epoch: 01 [12684/20158 ( 63%)], Train Loss: 0.03788\n",
      "Epoch: 01 [12724/20158 ( 63%)], Train Loss: 0.03780\n",
      "Epoch: 01 [12764/20158 ( 63%)], Train Loss: 0.03772\n",
      "Epoch: 01 [12804/20158 ( 64%)], Train Loss: 0.03774\n",
      "Epoch: 01 [12844/20158 ( 64%)], Train Loss: 0.03772\n",
      "Epoch: 01 [12884/20158 ( 64%)], Train Loss: 0.03773\n",
      "Epoch: 01 [12924/20158 ( 64%)], Train Loss: 0.03777\n",
      "Epoch: 01 [12964/20158 ( 64%)], Train Loss: 0.03776\n",
      "Epoch: 01 [13004/20158 ( 65%)], Train Loss: 0.03783\n",
      "Epoch: 01 [13044/20158 ( 65%)], Train Loss: 0.03784\n",
      "Epoch: 01 [13084/20158 ( 65%)], Train Loss: 0.03779\n",
      "Epoch: 01 [13124/20158 ( 65%)], Train Loss: 0.03778\n",
      "Epoch: 01 [13164/20158 ( 65%)], Train Loss: 0.03772\n",
      "Epoch: 01 [13204/20158 ( 66%)], Train Loss: 0.03785\n",
      "Epoch: 01 [13244/20158 ( 66%)], Train Loss: 0.03778\n",
      "Epoch: 01 [13284/20158 ( 66%)], Train Loss: 0.03770\n",
      "Epoch: 01 [13324/20158 ( 66%)], Train Loss: 0.03769\n",
      "Epoch: 01 [13364/20158 ( 66%)], Train Loss: 0.03778\n",
      "Epoch: 01 [13404/20158 ( 66%)], Train Loss: 0.03773\n",
      "Epoch: 01 [13444/20158 ( 67%)], Train Loss: 0.03768\n",
      "Epoch: 01 [13484/20158 ( 67%)], Train Loss: 0.03765\n",
      "Epoch: 01 [13524/20158 ( 67%)], Train Loss: 0.03762\n",
      "Epoch: 01 [13564/20158 ( 67%)], Train Loss: 0.03759\n",
      "Epoch: 01 [13604/20158 ( 67%)], Train Loss: 0.03759\n",
      "Epoch: 01 [13644/20158 ( 68%)], Train Loss: 0.03759\n",
      "Epoch: 01 [13684/20158 ( 68%)], Train Loss: 0.03753\n",
      "Epoch: 01 [13724/20158 ( 68%)], Train Loss: 0.03753\n",
      "Epoch: 01 [13764/20158 ( 68%)], Train Loss: 0.03752\n",
      "Epoch: 01 [13804/20158 ( 68%)], Train Loss: 0.03748\n",
      "Epoch: 01 [13844/20158 ( 69%)], Train Loss: 0.03745\n",
      "Epoch: 01 [13884/20158 ( 69%)], Train Loss: 0.03740\n",
      "Epoch: 01 [13924/20158 ( 69%)], Train Loss: 0.03741\n",
      "Epoch: 01 [13964/20158 ( 69%)], Train Loss: 0.03744\n",
      "Epoch: 01 [14004/20158 ( 69%)], Train Loss: 0.03745\n",
      "Epoch: 01 [14044/20158 ( 70%)], Train Loss: 0.03739\n",
      "Epoch: 01 [14084/20158 ( 70%)], Train Loss: 0.03735\n",
      "Epoch: 01 [14124/20158 ( 70%)], Train Loss: 0.03728\n",
      "Epoch: 01 [14164/20158 ( 70%)], Train Loss: 0.03727\n",
      "Epoch: 01 [14204/20158 ( 70%)], Train Loss: 0.03730\n",
      "Epoch: 01 [14244/20158 ( 71%)], Train Loss: 0.03721\n",
      "Epoch: 01 [14284/20158 ( 71%)], Train Loss: 0.03715\n",
      "Epoch: 01 [14324/20158 ( 71%)], Train Loss: 0.03712\n",
      "Epoch: 01 [14364/20158 ( 71%)], Train Loss: 0.03715\n",
      "Epoch: 01 [14404/20158 ( 71%)], Train Loss: 0.03722\n",
      "Epoch: 01 [14444/20158 ( 72%)], Train Loss: 0.03717\n",
      "Epoch: 01 [14484/20158 ( 72%)], Train Loss: 0.03712\n",
      "Epoch: 01 [14524/20158 ( 72%)], Train Loss: 0.03716\n",
      "Epoch: 01 [14564/20158 ( 72%)], Train Loss: 0.03712\n",
      "Epoch: 01 [14604/20158 ( 72%)], Train Loss: 0.03717\n",
      "Epoch: 01 [14644/20158 ( 73%)], Train Loss: 0.03711\n",
      "Epoch: 01 [14684/20158 ( 73%)], Train Loss: 0.03704\n",
      "Epoch: 01 [14724/20158 ( 73%)], Train Loss: 0.03699\n",
      "Epoch: 01 [14764/20158 ( 73%)], Train Loss: 0.03702\n",
      "Epoch: 01 [14804/20158 ( 73%)], Train Loss: 0.03704\n",
      "Epoch: 01 [14844/20158 ( 74%)], Train Loss: 0.03701\n",
      "Epoch: 01 [14884/20158 ( 74%)], Train Loss: 0.03700\n",
      "Epoch: 01 [14924/20158 ( 74%)], Train Loss: 0.03697\n",
      "Epoch: 01 [14964/20158 ( 74%)], Train Loss: 0.03690\n",
      "Epoch: 01 [15004/20158 ( 74%)], Train Loss: 0.03690\n",
      "Epoch: 01 [15044/20158 ( 75%)], Train Loss: 0.03687\n",
      "Epoch: 01 [15084/20158 ( 75%)], Train Loss: 0.03685\n",
      "Epoch: 01 [15124/20158 ( 75%)], Train Loss: 0.03683\n",
      "Epoch: 01 [15164/20158 ( 75%)], Train Loss: 0.03685\n",
      "Epoch: 01 [15204/20158 ( 75%)], Train Loss: 0.03680\n",
      "Epoch: 01 [15244/20158 ( 76%)], Train Loss: 0.03675\n",
      "Epoch: 01 [15284/20158 ( 76%)], Train Loss: 0.03675\n",
      "Epoch: 01 [15324/20158 ( 76%)], Train Loss: 0.03673\n",
      "Epoch: 01 [15364/20158 ( 76%)], Train Loss: 0.03671\n",
      "Epoch: 01 [15404/20158 ( 76%)], Train Loss: 0.03669\n",
      "Epoch: 01 [15444/20158 ( 77%)], Train Loss: 0.03662\n",
      "Epoch: 01 [15484/20158 ( 77%)], Train Loss: 0.03661\n",
      "Epoch: 01 [15524/20158 ( 77%)], Train Loss: 0.03659\n",
      "Epoch: 01 [15564/20158 ( 77%)], Train Loss: 0.03658\n",
      "Epoch: 01 [15604/20158 ( 77%)], Train Loss: 0.03657\n",
      "Epoch: 01 [15644/20158 ( 78%)], Train Loss: 0.03652\n",
      "Epoch: 01 [15684/20158 ( 78%)], Train Loss: 0.03651\n",
      "Epoch: 01 [15724/20158 ( 78%)], Train Loss: 0.03648\n",
      "Epoch: 01 [15764/20158 ( 78%)], Train Loss: 0.03642\n",
      "Epoch: 01 [15804/20158 ( 78%)], Train Loss: 0.03644\n",
      "Epoch: 01 [15844/20158 ( 79%)], Train Loss: 0.03652\n",
      "Epoch: 01 [15884/20158 ( 79%)], Train Loss: 0.03653\n",
      "Epoch: 01 [15924/20158 ( 79%)], Train Loss: 0.03654\n",
      "Epoch: 01 [15964/20158 ( 79%)], Train Loss: 0.03655\n",
      "Epoch: 01 [16004/20158 ( 79%)], Train Loss: 0.03655\n",
      "Epoch: 01 [16044/20158 ( 80%)], Train Loss: 0.03651\n",
      "Epoch: 01 [16084/20158 ( 80%)], Train Loss: 0.03650\n",
      "Epoch: 01 [16124/20158 ( 80%)], Train Loss: 0.03650\n",
      "Epoch: 01 [16164/20158 ( 80%)], Train Loss: 0.03650\n",
      "Epoch: 01 [16204/20158 ( 80%)], Train Loss: 0.03647\n",
      "Epoch: 01 [16244/20158 ( 81%)], Train Loss: 0.03645\n",
      "Epoch: 01 [16284/20158 ( 81%)], Train Loss: 0.03641\n",
      "Epoch: 01 [16324/20158 ( 81%)], Train Loss: 0.03638\n",
      "Epoch: 01 [16364/20158 ( 81%)], Train Loss: 0.03637\n",
      "Epoch: 01 [16404/20158 ( 81%)], Train Loss: 0.03635\n",
      "Epoch: 01 [16444/20158 ( 82%)], Train Loss: 0.03630\n",
      "Epoch: 01 [16484/20158 ( 82%)], Train Loss: 0.03639\n",
      "Epoch: 01 [16524/20158 ( 82%)], Train Loss: 0.03637\n",
      "Epoch: 01 [16564/20158 ( 82%)], Train Loss: 0.03632\n",
      "Epoch: 01 [16604/20158 ( 82%)], Train Loss: 0.03632\n",
      "Epoch: 01 [16644/20158 ( 83%)], Train Loss: 0.03628\n",
      "Epoch: 01 [16684/20158 ( 83%)], Train Loss: 0.03622\n",
      "Epoch: 01 [16724/20158 ( 83%)], Train Loss: 0.03618\n",
      "Epoch: 01 [16764/20158 ( 83%)], Train Loss: 0.03616\n",
      "Epoch: 01 [16804/20158 ( 83%)], Train Loss: 0.03621\n",
      "Epoch: 01 [16844/20158 ( 84%)], Train Loss: 0.03625\n",
      "Epoch: 01 [16884/20158 ( 84%)], Train Loss: 0.03621\n",
      "Epoch: 01 [16924/20158 ( 84%)], Train Loss: 0.03624\n",
      "Epoch: 01 [16964/20158 ( 84%)], Train Loss: 0.03620\n",
      "Epoch: 01 [17004/20158 ( 84%)], Train Loss: 0.03634\n",
      "Epoch: 01 [17044/20158 ( 85%)], Train Loss: 0.03631\n",
      "Epoch: 01 [17084/20158 ( 85%)], Train Loss: 0.03629\n",
      "Epoch: 01 [17124/20158 ( 85%)], Train Loss: 0.03625\n",
      "Epoch: 01 [17164/20158 ( 85%)], Train Loss: 0.03625\n",
      "Epoch: 01 [17204/20158 ( 85%)], Train Loss: 0.03621\n",
      "Epoch: 01 [17244/20158 ( 86%)], Train Loss: 0.03618\n",
      "Epoch: 01 [17284/20158 ( 86%)], Train Loss: 0.03615\n",
      "Epoch: 01 [17324/20158 ( 86%)], Train Loss: 0.03615\n",
      "Epoch: 01 [17364/20158 ( 86%)], Train Loss: 0.03613\n",
      "Epoch: 01 [17404/20158 ( 86%)], Train Loss: 0.03614\n",
      "Epoch: 01 [17444/20158 ( 87%)], Train Loss: 0.03615\n",
      "Epoch: 01 [17484/20158 ( 87%)], Train Loss: 0.03610\n",
      "Epoch: 01 [17524/20158 ( 87%)], Train Loss: 0.03609\n",
      "Epoch: 01 [17564/20158 ( 87%)], Train Loss: 0.03603\n",
      "Epoch: 01 [17604/20158 ( 87%)], Train Loss: 0.03600\n",
      "Epoch: 01 [17644/20158 ( 88%)], Train Loss: 0.03597\n",
      "Epoch: 01 [17684/20158 ( 88%)], Train Loss: 0.03599\n",
      "Epoch: 01 [17724/20158 ( 88%)], Train Loss: 0.03600\n",
      "Epoch: 01 [17764/20158 ( 88%)], Train Loss: 0.03606\n",
      "Epoch: 01 [17804/20158 ( 88%)], Train Loss: 0.03605\n",
      "Epoch: 01 [17844/20158 ( 89%)], Train Loss: 0.03612\n",
      "Epoch: 01 [17884/20158 ( 89%)], Train Loss: 0.03613\n",
      "Epoch: 01 [17924/20158 ( 89%)], Train Loss: 0.03613\n",
      "Epoch: 01 [17964/20158 ( 89%)], Train Loss: 0.03616\n",
      "Epoch: 01 [18004/20158 ( 89%)], Train Loss: 0.03614\n",
      "Epoch: 01 [18044/20158 ( 90%)], Train Loss: 0.03613\n",
      "Epoch: 01 [18084/20158 ( 90%)], Train Loss: 0.03617\n",
      "Epoch: 01 [18124/20158 ( 90%)], Train Loss: 0.03613\n",
      "Epoch: 01 [18164/20158 ( 90%)], Train Loss: 0.03607\n",
      "Epoch: 01 [18204/20158 ( 90%)], Train Loss: 0.03603\n",
      "Epoch: 01 [18244/20158 ( 91%)], Train Loss: 0.03602\n",
      "Epoch: 01 [18284/20158 ( 91%)], Train Loss: 0.03603\n",
      "Epoch: 01 [18324/20158 ( 91%)], Train Loss: 0.03605\n",
      "Epoch: 01 [18364/20158 ( 91%)], Train Loss: 0.03611\n",
      "Epoch: 01 [18404/20158 ( 91%)], Train Loss: 0.03609\n",
      "Epoch: 01 [18444/20158 ( 91%)], Train Loss: 0.03606\n",
      "Epoch: 01 [18484/20158 ( 92%)], Train Loss: 0.03606\n",
      "Epoch: 01 [18524/20158 ( 92%)], Train Loss: 0.03607\n",
      "Epoch: 01 [18564/20158 ( 92%)], Train Loss: 0.03618\n",
      "Epoch: 01 [18604/20158 ( 92%)], Train Loss: 0.03622\n",
      "Epoch: 01 [18644/20158 ( 92%)], Train Loss: 0.03621\n",
      "Epoch: 01 [18684/20158 ( 93%)], Train Loss: 0.03618\n",
      "Epoch: 01 [18724/20158 ( 93%)], Train Loss: 0.03619\n",
      "Epoch: 01 [18764/20158 ( 93%)], Train Loss: 0.03616\n",
      "Epoch: 01 [18804/20158 ( 93%)], Train Loss: 0.03619\n",
      "Epoch: 01 [18844/20158 ( 93%)], Train Loss: 0.03615\n",
      "Epoch: 01 [18884/20158 ( 94%)], Train Loss: 0.03611\n",
      "Epoch: 01 [18924/20158 ( 94%)], Train Loss: 0.03609\n",
      "Epoch: 01 [18964/20158 ( 94%)], Train Loss: 0.03606\n",
      "Epoch: 01 [19004/20158 ( 94%)], Train Loss: 0.03602\n",
      "Epoch: 01 [19044/20158 ( 94%)], Train Loss: 0.03599\n",
      "Epoch: 01 [19084/20158 ( 95%)], Train Loss: 0.03596\n",
      "Epoch: 01 [19124/20158 ( 95%)], Train Loss: 0.03594\n",
      "Epoch: 01 [19164/20158 ( 95%)], Train Loss: 0.03591\n",
      "Epoch: 01 [19204/20158 ( 95%)], Train Loss: 0.03590\n",
      "Epoch: 01 [19244/20158 ( 95%)], Train Loss: 0.03597\n",
      "Epoch: 01 [19284/20158 ( 96%)], Train Loss: 0.03595\n",
      "Epoch: 01 [19324/20158 ( 96%)], Train Loss: 0.03591\n",
      "Epoch: 01 [19364/20158 ( 96%)], Train Loss: 0.03590\n",
      "Epoch: 01 [19404/20158 ( 96%)], Train Loss: 0.03590\n",
      "Epoch: 01 [19444/20158 ( 96%)], Train Loss: 0.03589\n",
      "Epoch: 01 [19484/20158 ( 97%)], Train Loss: 0.03590\n",
      "Epoch: 01 [19524/20158 ( 97%)], Train Loss: 0.03587\n",
      "Epoch: 01 [19564/20158 ( 97%)], Train Loss: 0.03582\n",
      "Epoch: 01 [19604/20158 ( 97%)], Train Loss: 0.03580\n",
      "Epoch: 01 [19644/20158 ( 97%)], Train Loss: 0.03579\n",
      "Epoch: 01 [19684/20158 ( 98%)], Train Loss: 0.03580\n",
      "Epoch: 01 [19724/20158 ( 98%)], Train Loss: 0.03575\n",
      "Epoch: 01 [19764/20158 ( 98%)], Train Loss: 0.03569\n",
      "Epoch: 01 [19804/20158 ( 98%)], Train Loss: 0.03569\n",
      "Epoch: 01 [19844/20158 ( 98%)], Train Loss: 0.03569\n",
      "Epoch: 01 [19884/20158 ( 99%)], Train Loss: 0.03568\n",
      "Epoch: 01 [19924/20158 ( 99%)], Train Loss: 0.03570\n",
      "Epoch: 01 [19964/20158 ( 99%)], Train Loss: 0.03572\n",
      "Epoch: 01 [20004/20158 ( 99%)], Train Loss: 0.03575\n",
      "Epoch: 01 [20044/20158 ( 99%)], Train Loss: 0.03571\n",
      "Epoch: 01 [20084/20158 (100%)], Train Loss: 0.03568\n",
      "Epoch: 01 [20124/20158 (100%)], Train Loss: 0.03569\n",
      "Epoch: 01 [20158/20158 (100%)], Train Loss: 0.03569\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.30442\n",
      "\n",
      "Total Training Time: 5308.63679933548secs, Average Training Time per Epoch: 2654.31839966774secs.\n",
      "Total Validation Time: 254.64429903030396secs, Average Validation Time per Epoch: 127.32214951515198secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 4\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20209, Num examples Valid=2860\n",
      "Total Training Steps: 1264, Total Warmup Steps: 126\n",
      "Epoch: 00 [    4/20209 (  0%)], Train Loss: 0.74746\n",
      "Epoch: 00 [   44/20209 (  0%)], Train Loss: 0.73297\n",
      "Epoch: 00 [   84/20209 (  0%)], Train Loss: 0.72724\n",
      "Epoch: 00 [  124/20209 (  1%)], Train Loss: 0.72472\n",
      "Epoch: 00 [  164/20209 (  1%)], Train Loss: 0.72269\n",
      "Epoch: 00 [  204/20209 (  1%)], Train Loss: 0.72194\n",
      "Epoch: 00 [  244/20209 (  1%)], Train Loss: 0.71903\n",
      "Epoch: 00 [  284/20209 (  1%)], Train Loss: 0.71645\n",
      "Epoch: 00 [  324/20209 (  2%)], Train Loss: 0.71304\n",
      "Epoch: 00 [  364/20209 (  2%)], Train Loss: 0.70877\n",
      "Epoch: 00 [  404/20209 (  2%)], Train Loss: 0.70481\n",
      "Epoch: 00 [  444/20209 (  2%)], Train Loss: 0.69976\n",
      "Epoch: 00 [  484/20209 (  2%)], Train Loss: 0.69392\n",
      "Epoch: 00 [  524/20209 (  3%)], Train Loss: 0.68800\n",
      "Epoch: 00 [  564/20209 (  3%)], Train Loss: 0.68264\n",
      "Epoch: 00 [  604/20209 (  3%)], Train Loss: 0.67680\n",
      "Epoch: 00 [  644/20209 (  3%)], Train Loss: 0.67014\n",
      "Epoch: 00 [  684/20209 (  3%)], Train Loss: 0.66179\n",
      "Epoch: 00 [  724/20209 (  4%)], Train Loss: 0.65545\n",
      "Epoch: 00 [  764/20209 (  4%)], Train Loss: 0.64523\n",
      "Epoch: 00 [  804/20209 (  4%)], Train Loss: 0.63676\n",
      "Epoch: 00 [  844/20209 (  4%)], Train Loss: 0.62687\n",
      "Epoch: 00 [  884/20209 (  4%)], Train Loss: 0.61846\n",
      "Epoch: 00 [  924/20209 (  5%)], Train Loss: 0.60881\n",
      "Epoch: 00 [  964/20209 (  5%)], Train Loss: 0.59879\n",
      "Epoch: 00 [ 1004/20209 (  5%)], Train Loss: 0.58848\n",
      "Epoch: 00 [ 1044/20209 (  5%)], Train Loss: 0.57707\n",
      "Epoch: 00 [ 1084/20209 (  5%)], Train Loss: 0.56517\n",
      "Epoch: 00 [ 1124/20209 (  6%)], Train Loss: 0.55436\n",
      "Epoch: 00 [ 1164/20209 (  6%)], Train Loss: 0.54268\n",
      "Epoch: 00 [ 1204/20209 (  6%)], Train Loss: 0.53116\n",
      "Epoch: 00 [ 1244/20209 (  6%)], Train Loss: 0.51901\n",
      "Epoch: 00 [ 1284/20209 (  6%)], Train Loss: 0.50816\n",
      "Epoch: 00 [ 1324/20209 (  7%)], Train Loss: 0.49795\n",
      "Epoch: 00 [ 1364/20209 (  7%)], Train Loss: 0.48914\n",
      "Epoch: 00 [ 1404/20209 (  7%)], Train Loss: 0.48112\n",
      "Epoch: 00 [ 1444/20209 (  7%)], Train Loss: 0.47300\n",
      "Epoch: 00 [ 1484/20209 (  7%)], Train Loss: 0.46397\n",
      "Epoch: 00 [ 1524/20209 (  8%)], Train Loss: 0.45633\n",
      "Epoch: 00 [ 1564/20209 (  8%)], Train Loss: 0.44779\n",
      "Epoch: 00 [ 1604/20209 (  8%)], Train Loss: 0.44046\n",
      "Epoch: 00 [ 1644/20209 (  8%)], Train Loss: 0.43268\n",
      "Epoch: 00 [ 1684/20209 (  8%)], Train Loss: 0.42646\n",
      "Epoch: 00 [ 1724/20209 (  9%)], Train Loss: 0.41974\n",
      "Epoch: 00 [ 1764/20209 (  9%)], Train Loss: 0.41258\n",
      "Epoch: 00 [ 1804/20209 (  9%)], Train Loss: 0.40668\n",
      "Epoch: 00 [ 1844/20209 (  9%)], Train Loss: 0.40046\n",
      "Epoch: 00 [ 1884/20209 (  9%)], Train Loss: 0.39549\n",
      "Epoch: 00 [ 1924/20209 ( 10%)], Train Loss: 0.38932\n",
      "Epoch: 00 [ 1964/20209 ( 10%)], Train Loss: 0.38490\n",
      "Epoch: 00 [ 2004/20209 ( 10%)], Train Loss: 0.37909\n",
      "Epoch: 00 [ 2044/20209 ( 10%)], Train Loss: 0.37390\n",
      "Epoch: 00 [ 2084/20209 ( 10%)], Train Loss: 0.36930\n",
      "Epoch: 00 [ 2124/20209 ( 11%)], Train Loss: 0.36438\n",
      "Epoch: 00 [ 2164/20209 ( 11%)], Train Loss: 0.36023\n",
      "Epoch: 00 [ 2204/20209 ( 11%)], Train Loss: 0.35593\n",
      "Epoch: 00 [ 2244/20209 ( 11%)], Train Loss: 0.35192\n",
      "Epoch: 00 [ 2284/20209 ( 11%)], Train Loss: 0.34825\n",
      "Epoch: 00 [ 2324/20209 ( 11%)], Train Loss: 0.34392\n",
      "Epoch: 00 [ 2364/20209 ( 12%)], Train Loss: 0.34007\n",
      "Epoch: 00 [ 2404/20209 ( 12%)], Train Loss: 0.33655\n",
      "Epoch: 00 [ 2444/20209 ( 12%)], Train Loss: 0.33285\n",
      "Epoch: 00 [ 2484/20209 ( 12%)], Train Loss: 0.32970\n",
      "Epoch: 00 [ 2524/20209 ( 12%)], Train Loss: 0.32618\n",
      "Epoch: 00 [ 2564/20209 ( 13%)], Train Loss: 0.32304\n",
      "Epoch: 00 [ 2604/20209 ( 13%)], Train Loss: 0.31904\n",
      "Epoch: 00 [ 2644/20209 ( 13%)], Train Loss: 0.31506\n",
      "Epoch: 00 [ 2684/20209 ( 13%)], Train Loss: 0.31222\n",
      "Epoch: 00 [ 2724/20209 ( 13%)], Train Loss: 0.30898\n",
      "Epoch: 00 [ 2764/20209 ( 14%)], Train Loss: 0.30649\n",
      "Epoch: 00 [ 2804/20209 ( 14%)], Train Loss: 0.30362\n",
      "Epoch: 00 [ 2844/20209 ( 14%)], Train Loss: 0.30065\n",
      "Epoch: 00 [ 2884/20209 ( 14%)], Train Loss: 0.29822\n",
      "Epoch: 00 [ 2924/20209 ( 14%)], Train Loss: 0.29555\n",
      "Epoch: 00 [ 2964/20209 ( 15%)], Train Loss: 0.29291\n",
      "Epoch: 00 [ 3004/20209 ( 15%)], Train Loss: 0.29112\n",
      "Epoch: 00 [ 3044/20209 ( 15%)], Train Loss: 0.28861\n",
      "Epoch: 00 [ 3084/20209 ( 15%)], Train Loss: 0.28627\n",
      "Epoch: 00 [ 3124/20209 ( 15%)], Train Loss: 0.28356\n",
      "Epoch: 00 [ 3164/20209 ( 16%)], Train Loss: 0.28145\n",
      "Epoch: 00 [ 3204/20209 ( 16%)], Train Loss: 0.27919\n",
      "Epoch: 00 [ 3244/20209 ( 16%)], Train Loss: 0.27739\n",
      "Epoch: 00 [ 3284/20209 ( 16%)], Train Loss: 0.27526\n",
      "Epoch: 00 [ 3324/20209 ( 16%)], Train Loss: 0.27283\n",
      "Epoch: 00 [ 3364/20209 ( 17%)], Train Loss: 0.27064\n",
      "Epoch: 00 [ 3404/20209 ( 17%)], Train Loss: 0.26849\n",
      "Epoch: 00 [ 3444/20209 ( 17%)], Train Loss: 0.26651\n",
      "Epoch: 00 [ 3484/20209 ( 17%)], Train Loss: 0.26527\n",
      "Epoch: 00 [ 3524/20209 ( 17%)], Train Loss: 0.26341\n",
      "Epoch: 00 [ 3564/20209 ( 18%)], Train Loss: 0.26167\n",
      "Epoch: 00 [ 3604/20209 ( 18%)], Train Loss: 0.25980\n",
      "Epoch: 00 [ 3644/20209 ( 18%)], Train Loss: 0.25784\n",
      "Epoch: 00 [ 3684/20209 ( 18%)], Train Loss: 0.25668\n",
      "Epoch: 00 [ 3724/20209 ( 18%)], Train Loss: 0.25488\n",
      "Epoch: 00 [ 3764/20209 ( 19%)], Train Loss: 0.25347\n",
      "Epoch: 00 [ 3804/20209 ( 19%)], Train Loss: 0.25192\n",
      "Epoch: 00 [ 3844/20209 ( 19%)], Train Loss: 0.25045\n",
      "Epoch: 00 [ 3884/20209 ( 19%)], Train Loss: 0.24902\n",
      "Epoch: 00 [ 3924/20209 ( 19%)], Train Loss: 0.24742\n",
      "Epoch: 00 [ 3964/20209 ( 20%)], Train Loss: 0.24621\n",
      "Epoch: 00 [ 4004/20209 ( 20%)], Train Loss: 0.24421\n",
      "Epoch: 00 [ 4044/20209 ( 20%)], Train Loss: 0.24254\n",
      "Epoch: 00 [ 4084/20209 ( 20%)], Train Loss: 0.24145\n",
      "Epoch: 00 [ 4124/20209 ( 20%)], Train Loss: 0.23981\n",
      "Epoch: 00 [ 4164/20209 ( 21%)], Train Loss: 0.23863\n",
      "Epoch: 00 [ 4204/20209 ( 21%)], Train Loss: 0.23752\n",
      "Epoch: 00 [ 4244/20209 ( 21%)], Train Loss: 0.23574\n",
      "Epoch: 00 [ 4284/20209 ( 21%)], Train Loss: 0.23422\n",
      "Epoch: 00 [ 4324/20209 ( 21%)], Train Loss: 0.23272\n",
      "Epoch: 00 [ 4364/20209 ( 22%)], Train Loss: 0.23114\n",
      "Epoch: 00 [ 4404/20209 ( 22%)], Train Loss: 0.23008\n",
      "Epoch: 00 [ 4444/20209 ( 22%)], Train Loss: 0.22856\n",
      "Epoch: 00 [ 4484/20209 ( 22%)], Train Loss: 0.22731\n",
      "Epoch: 00 [ 4524/20209 ( 22%)], Train Loss: 0.22584\n",
      "Epoch: 00 [ 4564/20209 ( 23%)], Train Loss: 0.22471\n",
      "Epoch: 00 [ 4604/20209 ( 23%)], Train Loss: 0.22370\n",
      "Epoch: 00 [ 4644/20209 ( 23%)], Train Loss: 0.22262\n",
      "Epoch: 00 [ 4684/20209 ( 23%)], Train Loss: 0.22125\n",
      "Epoch: 00 [ 4724/20209 ( 23%)], Train Loss: 0.22024\n",
      "Epoch: 00 [ 4764/20209 ( 24%)], Train Loss: 0.21937\n",
      "Epoch: 00 [ 4804/20209 ( 24%)], Train Loss: 0.21830\n",
      "Epoch: 00 [ 4844/20209 ( 24%)], Train Loss: 0.21725\n",
      "Epoch: 00 [ 4884/20209 ( 24%)], Train Loss: 0.21653\n",
      "Epoch: 00 [ 4924/20209 ( 24%)], Train Loss: 0.21573\n",
      "Epoch: 00 [ 4964/20209 ( 25%)], Train Loss: 0.21485\n",
      "Epoch: 00 [ 5004/20209 ( 25%)], Train Loss: 0.21415\n",
      "Epoch: 00 [ 5044/20209 ( 25%)], Train Loss: 0.21345\n",
      "Epoch: 00 [ 5084/20209 ( 25%)], Train Loss: 0.21265\n",
      "Epoch: 00 [ 5124/20209 ( 25%)], Train Loss: 0.21173\n",
      "Epoch: 00 [ 5164/20209 ( 26%)], Train Loss: 0.21094\n",
      "Epoch: 00 [ 5204/20209 ( 26%)], Train Loss: 0.21032\n",
      "Epoch: 00 [ 5244/20209 ( 26%)], Train Loss: 0.20911\n",
      "Epoch: 00 [ 5284/20209 ( 26%)], Train Loss: 0.20794\n",
      "Epoch: 00 [ 5324/20209 ( 26%)], Train Loss: 0.20683\n",
      "Epoch: 00 [ 5364/20209 ( 27%)], Train Loss: 0.20620\n",
      "Epoch: 00 [ 5404/20209 ( 27%)], Train Loss: 0.20529\n",
      "Epoch: 00 [ 5444/20209 ( 27%)], Train Loss: 0.20452\n",
      "Epoch: 00 [ 5484/20209 ( 27%)], Train Loss: 0.20330\n",
      "Epoch: 00 [ 5524/20209 ( 27%)], Train Loss: 0.20219\n",
      "Epoch: 00 [ 5564/20209 ( 28%)], Train Loss: 0.20149\n",
      "Epoch: 00 [ 5604/20209 ( 28%)], Train Loss: 0.20054\n",
      "Epoch: 00 [ 5644/20209 ( 28%)], Train Loss: 0.19970\n",
      "Epoch: 00 [ 5684/20209 ( 28%)], Train Loss: 0.19890\n",
      "Epoch: 00 [ 5724/20209 ( 28%)], Train Loss: 0.19805\n",
      "Epoch: 00 [ 5764/20209 ( 29%)], Train Loss: 0.19695\n",
      "Epoch: 00 [ 5804/20209 ( 29%)], Train Loss: 0.19629\n",
      "Epoch: 00 [ 5844/20209 ( 29%)], Train Loss: 0.19551\n",
      "Epoch: 00 [ 5884/20209 ( 29%)], Train Loss: 0.19516\n",
      "Epoch: 00 [ 5924/20209 ( 29%)], Train Loss: 0.19461\n",
      "Epoch: 00 [ 5964/20209 ( 30%)], Train Loss: 0.19418\n",
      "Epoch: 00 [ 6004/20209 ( 30%)], Train Loss: 0.19346\n",
      "Epoch: 00 [ 6044/20209 ( 30%)], Train Loss: 0.19245\n",
      "Epoch: 00 [ 6084/20209 ( 30%)], Train Loss: 0.19195\n",
      "Epoch: 00 [ 6124/20209 ( 30%)], Train Loss: 0.19125\n",
      "Epoch: 00 [ 6164/20209 ( 31%)], Train Loss: 0.19083\n",
      "Epoch: 00 [ 6204/20209 ( 31%)], Train Loss: 0.19045\n",
      "Epoch: 00 [ 6244/20209 ( 31%)], Train Loss: 0.18979\n",
      "Epoch: 00 [ 6284/20209 ( 31%)], Train Loss: 0.18914\n",
      "Epoch: 00 [ 6324/20209 ( 31%)], Train Loss: 0.18844\n",
      "Epoch: 00 [ 6364/20209 ( 31%)], Train Loss: 0.18811\n",
      "Epoch: 00 [ 6404/20209 ( 32%)], Train Loss: 0.18762\n",
      "Epoch: 00 [ 6444/20209 ( 32%)], Train Loss: 0.18707\n",
      "Epoch: 00 [ 6484/20209 ( 32%)], Train Loss: 0.18627\n",
      "Epoch: 00 [ 6524/20209 ( 32%)], Train Loss: 0.18596\n",
      "Epoch: 00 [ 6564/20209 ( 32%)], Train Loss: 0.18573\n",
      "Epoch: 00 [ 6604/20209 ( 33%)], Train Loss: 0.18517\n",
      "Epoch: 00 [ 6644/20209 ( 33%)], Train Loss: 0.18480\n",
      "Epoch: 00 [ 6684/20209 ( 33%)], Train Loss: 0.18418\n",
      "Epoch: 00 [ 6724/20209 ( 33%)], Train Loss: 0.18358\n",
      "Epoch: 00 [ 6764/20209 ( 33%)], Train Loss: 0.18308\n",
      "Epoch: 00 [ 6804/20209 ( 34%)], Train Loss: 0.18254\n",
      "Epoch: 00 [ 6844/20209 ( 34%)], Train Loss: 0.18225\n",
      "Epoch: 00 [ 6884/20209 ( 34%)], Train Loss: 0.18185\n",
      "Epoch: 00 [ 6924/20209 ( 34%)], Train Loss: 0.18116\n",
      "Epoch: 00 [ 6964/20209 ( 34%)], Train Loss: 0.18061\n",
      "Epoch: 00 [ 7004/20209 ( 35%)], Train Loss: 0.18033\n",
      "Epoch: 00 [ 7044/20209 ( 35%)], Train Loss: 0.17954\n",
      "Epoch: 00 [ 7084/20209 ( 35%)], Train Loss: 0.17885\n",
      "Epoch: 00 [ 7124/20209 ( 35%)], Train Loss: 0.17798\n",
      "Epoch: 00 [ 7164/20209 ( 35%)], Train Loss: 0.17746\n",
      "Epoch: 00 [ 7204/20209 ( 36%)], Train Loss: 0.17713\n",
      "Epoch: 00 [ 7244/20209 ( 36%)], Train Loss: 0.17656\n",
      "Epoch: 00 [ 7284/20209 ( 36%)], Train Loss: 0.17591\n",
      "Epoch: 00 [ 7324/20209 ( 36%)], Train Loss: 0.17523\n",
      "Epoch: 00 [ 7364/20209 ( 36%)], Train Loss: 0.17476\n",
      "Epoch: 00 [ 7404/20209 ( 37%)], Train Loss: 0.17411\n",
      "Epoch: 00 [ 7444/20209 ( 37%)], Train Loss: 0.17357\n",
      "Epoch: 00 [ 7484/20209 ( 37%)], Train Loss: 0.17319\n",
      "Epoch: 00 [ 7524/20209 ( 37%)], Train Loss: 0.17263\n",
      "Epoch: 00 [ 7564/20209 ( 37%)], Train Loss: 0.17207\n",
      "Epoch: 00 [ 7604/20209 ( 38%)], Train Loss: 0.17160\n",
      "Epoch: 00 [ 7644/20209 ( 38%)], Train Loss: 0.17131\n",
      "Epoch: 00 [ 7684/20209 ( 38%)], Train Loss: 0.17109\n",
      "Epoch: 00 [ 7724/20209 ( 38%)], Train Loss: 0.17065\n",
      "Epoch: 00 [ 7764/20209 ( 38%)], Train Loss: 0.17022\n",
      "Epoch: 00 [ 7804/20209 ( 39%)], Train Loss: 0.16968\n",
      "Epoch: 00 [ 7844/20209 ( 39%)], Train Loss: 0.16909\n",
      "Epoch: 00 [ 7884/20209 ( 39%)], Train Loss: 0.16862\n",
      "Epoch: 00 [ 7924/20209 ( 39%)], Train Loss: 0.16825\n",
      "Epoch: 00 [ 7964/20209 ( 39%)], Train Loss: 0.16782\n",
      "Epoch: 00 [ 8004/20209 ( 40%)], Train Loss: 0.16743\n",
      "Epoch: 00 [ 8044/20209 ( 40%)], Train Loss: 0.16702\n",
      "Epoch: 00 [ 8084/20209 ( 40%)], Train Loss: 0.16672\n",
      "Epoch: 00 [ 8124/20209 ( 40%)], Train Loss: 0.16658\n",
      "Epoch: 00 [ 8164/20209 ( 40%)], Train Loss: 0.16637\n",
      "Epoch: 00 [ 8204/20209 ( 41%)], Train Loss: 0.16617\n",
      "Epoch: 00 [ 8244/20209 ( 41%)], Train Loss: 0.16575\n",
      "Epoch: 00 [ 8284/20209 ( 41%)], Train Loss: 0.16538\n",
      "Epoch: 00 [ 8324/20209 ( 41%)], Train Loss: 0.16524\n",
      "Epoch: 00 [ 8364/20209 ( 41%)], Train Loss: 0.16494\n",
      "Epoch: 00 [ 8404/20209 ( 42%)], Train Loss: 0.16472\n",
      "Epoch: 00 [ 8444/20209 ( 42%)], Train Loss: 0.16424\n",
      "Epoch: 00 [ 8484/20209 ( 42%)], Train Loss: 0.16365\n",
      "Epoch: 00 [ 8524/20209 ( 42%)], Train Loss: 0.16315\n",
      "Epoch: 00 [ 8564/20209 ( 42%)], Train Loss: 0.16307\n",
      "Epoch: 00 [ 8604/20209 ( 43%)], Train Loss: 0.16288\n",
      "Epoch: 00 [ 8644/20209 ( 43%)], Train Loss: 0.16267\n",
      "Epoch: 00 [ 8684/20209 ( 43%)], Train Loss: 0.16255\n",
      "Epoch: 00 [ 8724/20209 ( 43%)], Train Loss: 0.16243\n",
      "Epoch: 00 [ 8764/20209 ( 43%)], Train Loss: 0.16217\n",
      "Epoch: 00 [ 8804/20209 ( 44%)], Train Loss: 0.16190\n",
      "Epoch: 00 [ 8844/20209 ( 44%)], Train Loss: 0.16159\n",
      "Epoch: 00 [ 8884/20209 ( 44%)], Train Loss: 0.16128\n",
      "Epoch: 00 [ 8924/20209 ( 44%)], Train Loss: 0.16122\n",
      "Epoch: 00 [ 8964/20209 ( 44%)], Train Loss: 0.16085\n",
      "Epoch: 00 [ 9004/20209 ( 45%)], Train Loss: 0.16066\n",
      "Epoch: 00 [ 9044/20209 ( 45%)], Train Loss: 0.16029\n",
      "Epoch: 00 [ 9084/20209 ( 45%)], Train Loss: 0.15991\n",
      "Epoch: 00 [ 9124/20209 ( 45%)], Train Loss: 0.15950\n",
      "Epoch: 00 [ 9164/20209 ( 45%)], Train Loss: 0.15917\n",
      "Epoch: 00 [ 9204/20209 ( 46%)], Train Loss: 0.15880\n",
      "Epoch: 00 [ 9244/20209 ( 46%)], Train Loss: 0.15867\n",
      "Epoch: 00 [ 9284/20209 ( 46%)], Train Loss: 0.15843\n",
      "Epoch: 00 [ 9324/20209 ( 46%)], Train Loss: 0.15821\n",
      "Epoch: 00 [ 9364/20209 ( 46%)], Train Loss: 0.15791\n",
      "Epoch: 00 [ 9404/20209 ( 47%)], Train Loss: 0.15754\n",
      "Epoch: 00 [ 9444/20209 ( 47%)], Train Loss: 0.15756\n",
      "Epoch: 00 [ 9484/20209 ( 47%)], Train Loss: 0.15706\n",
      "Epoch: 00 [ 9524/20209 ( 47%)], Train Loss: 0.15678\n",
      "Epoch: 00 [ 9564/20209 ( 47%)], Train Loss: 0.15638\n",
      "Epoch: 00 [ 9604/20209 ( 48%)], Train Loss: 0.15612\n",
      "Epoch: 00 [ 9644/20209 ( 48%)], Train Loss: 0.15579\n",
      "Epoch: 00 [ 9684/20209 ( 48%)], Train Loss: 0.15558\n",
      "Epoch: 00 [ 9724/20209 ( 48%)], Train Loss: 0.15522\n",
      "Epoch: 00 [ 9764/20209 ( 48%)], Train Loss: 0.15501\n",
      "Epoch: 00 [ 9804/20209 ( 49%)], Train Loss: 0.15478\n",
      "Epoch: 00 [ 9844/20209 ( 49%)], Train Loss: 0.15447\n",
      "Epoch: 00 [ 9884/20209 ( 49%)], Train Loss: 0.15422\n",
      "Epoch: 00 [ 9924/20209 ( 49%)], Train Loss: 0.15400\n",
      "Epoch: 00 [ 9964/20209 ( 49%)], Train Loss: 0.15376\n",
      "Epoch: 00 [10004/20209 ( 50%)], Train Loss: 0.15349\n",
      "Epoch: 00 [10044/20209 ( 50%)], Train Loss: 0.15325\n",
      "Epoch: 00 [10084/20209 ( 50%)], Train Loss: 0.15301\n",
      "Epoch: 00 [10124/20209 ( 50%)], Train Loss: 0.15290\n",
      "Epoch: 00 [10164/20209 ( 50%)], Train Loss: 0.15252\n",
      "Epoch: 00 [10204/20209 ( 50%)], Train Loss: 0.15220\n",
      "Epoch: 00 [10244/20209 ( 51%)], Train Loss: 0.15190\n",
      "Epoch: 00 [10284/20209 ( 51%)], Train Loss: 0.15155\n",
      "Epoch: 00 [10324/20209 ( 51%)], Train Loss: 0.15134\n",
      "Epoch: 00 [10364/20209 ( 51%)], Train Loss: 0.15122\n",
      "Epoch: 00 [10404/20209 ( 51%)], Train Loss: 0.15112\n",
      "Epoch: 00 [10444/20209 ( 52%)], Train Loss: 0.15092\n",
      "Epoch: 00 [10484/20209 ( 52%)], Train Loss: 0.15064\n",
      "Epoch: 00 [10524/20209 ( 52%)], Train Loss: 0.15049\n",
      "Epoch: 00 [10564/20209 ( 52%)], Train Loss: 0.15025\n",
      "Epoch: 00 [10604/20209 ( 52%)], Train Loss: 0.14995\n",
      "Epoch: 00 [10644/20209 ( 53%)], Train Loss: 0.14990\n",
      "Epoch: 00 [10684/20209 ( 53%)], Train Loss: 0.14956\n",
      "Epoch: 00 [10724/20209 ( 53%)], Train Loss: 0.14951\n",
      "Epoch: 00 [10764/20209 ( 53%)], Train Loss: 0.14945\n",
      "Epoch: 00 [10804/20209 ( 53%)], Train Loss: 0.14912\n",
      "Epoch: 00 [10844/20209 ( 54%)], Train Loss: 0.14883\n",
      "Epoch: 00 [10884/20209 ( 54%)], Train Loss: 0.14855\n",
      "Epoch: 00 [10924/20209 ( 54%)], Train Loss: 0.14841\n",
      "Epoch: 00 [10964/20209 ( 54%)], Train Loss: 0.14821\n",
      "Epoch: 00 [11004/20209 ( 54%)], Train Loss: 0.14789\n",
      "Epoch: 00 [11044/20209 ( 55%)], Train Loss: 0.14786\n",
      "Epoch: 00 [11084/20209 ( 55%)], Train Loss: 0.14762\n",
      "Epoch: 00 [11124/20209 ( 55%)], Train Loss: 0.14737\n",
      "Epoch: 00 [11164/20209 ( 55%)], Train Loss: 0.14720\n",
      "Epoch: 00 [11204/20209 ( 55%)], Train Loss: 0.14716\n",
      "Epoch: 00 [11244/20209 ( 56%)], Train Loss: 0.14698\n",
      "Epoch: 00 [11284/20209 ( 56%)], Train Loss: 0.14701\n",
      "Epoch: 00 [11324/20209 ( 56%)], Train Loss: 0.14671\n",
      "Epoch: 00 [11364/20209 ( 56%)], Train Loss: 0.14654\n",
      "Epoch: 00 [11404/20209 ( 56%)], Train Loss: 0.14626\n",
      "Epoch: 00 [11444/20209 ( 57%)], Train Loss: 0.14613\n",
      "Epoch: 00 [11484/20209 ( 57%)], Train Loss: 0.14602\n",
      "Epoch: 00 [11524/20209 ( 57%)], Train Loss: 0.14590\n",
      "Epoch: 00 [11564/20209 ( 57%)], Train Loss: 0.14562\n",
      "Epoch: 00 [11604/20209 ( 57%)], Train Loss: 0.14555\n",
      "Epoch: 00 [11644/20209 ( 58%)], Train Loss: 0.14529\n",
      "Epoch: 00 [11684/20209 ( 58%)], Train Loss: 0.14498\n",
      "Epoch: 00 [11724/20209 ( 58%)], Train Loss: 0.14477\n",
      "Epoch: 00 [11764/20209 ( 58%)], Train Loss: 0.14448\n",
      "Epoch: 00 [11804/20209 ( 58%)], Train Loss: 0.14418\n",
      "Epoch: 00 [11844/20209 ( 59%)], Train Loss: 0.14402\n",
      "Epoch: 00 [11884/20209 ( 59%)], Train Loss: 0.14374\n",
      "Epoch: 00 [11924/20209 ( 59%)], Train Loss: 0.14342\n",
      "Epoch: 00 [11964/20209 ( 59%)], Train Loss: 0.14324\n",
      "Epoch: 00 [12004/20209 ( 59%)], Train Loss: 0.14312\n",
      "Epoch: 00 [12044/20209 ( 60%)], Train Loss: 0.14296\n",
      "Epoch: 00 [12084/20209 ( 60%)], Train Loss: 0.14268\n",
      "Epoch: 00 [12124/20209 ( 60%)], Train Loss: 0.14246\n",
      "Epoch: 00 [12164/20209 ( 60%)], Train Loss: 0.14214\n",
      "Epoch: 00 [12204/20209 ( 60%)], Train Loss: 0.14195\n",
      "Epoch: 00 [12244/20209 ( 61%)], Train Loss: 0.14182\n",
      "Epoch: 00 [12284/20209 ( 61%)], Train Loss: 0.14154\n",
      "Epoch: 00 [12324/20209 ( 61%)], Train Loss: 0.14132\n",
      "Epoch: 00 [12364/20209 ( 61%)], Train Loss: 0.14112\n",
      "Epoch: 00 [12404/20209 ( 61%)], Train Loss: 0.14099\n",
      "Epoch: 00 [12444/20209 ( 62%)], Train Loss: 0.14088\n",
      "Epoch: 00 [12484/20209 ( 62%)], Train Loss: 0.14059\n",
      "Epoch: 00 [12524/20209 ( 62%)], Train Loss: 0.14036\n",
      "Epoch: 00 [12564/20209 ( 62%)], Train Loss: 0.14005\n",
      "Epoch: 00 [12604/20209 ( 62%)], Train Loss: 0.13995\n",
      "Epoch: 00 [12644/20209 ( 63%)], Train Loss: 0.13989\n",
      "Epoch: 00 [12684/20209 ( 63%)], Train Loss: 0.13964\n",
      "Epoch: 00 [12724/20209 ( 63%)], Train Loss: 0.13941\n",
      "Epoch: 00 [12764/20209 ( 63%)], Train Loss: 0.13939\n",
      "Epoch: 00 [12804/20209 ( 63%)], Train Loss: 0.13923\n",
      "Epoch: 00 [12844/20209 ( 64%)], Train Loss: 0.13913\n",
      "Epoch: 00 [12884/20209 ( 64%)], Train Loss: 0.13891\n",
      "Epoch: 00 [12924/20209 ( 64%)], Train Loss: 0.13865\n",
      "Epoch: 00 [12964/20209 ( 64%)], Train Loss: 0.13861\n",
      "Epoch: 00 [13004/20209 ( 64%)], Train Loss: 0.13844\n",
      "Epoch: 00 [13044/20209 ( 65%)], Train Loss: 0.13830\n",
      "Epoch: 00 [13084/20209 ( 65%)], Train Loss: 0.13819\n",
      "Epoch: 00 [13124/20209 ( 65%)], Train Loss: 0.13793\n",
      "Epoch: 00 [13164/20209 ( 65%)], Train Loss: 0.13781\n",
      "Epoch: 00 [13204/20209 ( 65%)], Train Loss: 0.13763\n",
      "Epoch: 00 [13244/20209 ( 66%)], Train Loss: 0.13750\n",
      "Epoch: 00 [13284/20209 ( 66%)], Train Loss: 0.13743\n",
      "Epoch: 00 [13324/20209 ( 66%)], Train Loss: 0.13715\n",
      "Epoch: 00 [13364/20209 ( 66%)], Train Loss: 0.13710\n",
      "Epoch: 00 [13404/20209 ( 66%)], Train Loss: 0.13693\n",
      "Epoch: 00 [13444/20209 ( 67%)], Train Loss: 0.13680\n",
      "Epoch: 00 [13484/20209 ( 67%)], Train Loss: 0.13662\n",
      "Epoch: 00 [13524/20209 ( 67%)], Train Loss: 0.13646\n",
      "Epoch: 00 [13564/20209 ( 67%)], Train Loss: 0.13637\n",
      "Epoch: 00 [13604/20209 ( 67%)], Train Loss: 0.13618\n",
      "Epoch: 00 [13644/20209 ( 68%)], Train Loss: 0.13601\n",
      "Epoch: 00 [13684/20209 ( 68%)], Train Loss: 0.13594\n",
      "Epoch: 00 [13724/20209 ( 68%)], Train Loss: 0.13571\n",
      "Epoch: 00 [13764/20209 ( 68%)], Train Loss: 0.13579\n",
      "Epoch: 00 [13804/20209 ( 68%)], Train Loss: 0.13560\n",
      "Epoch: 00 [13844/20209 ( 69%)], Train Loss: 0.13550\n",
      "Epoch: 00 [13884/20209 ( 69%)], Train Loss: 0.13530\n",
      "Epoch: 00 [13924/20209 ( 69%)], Train Loss: 0.13521\n",
      "Epoch: 00 [13964/20209 ( 69%)], Train Loss: 0.13508\n",
      "Epoch: 00 [14004/20209 ( 69%)], Train Loss: 0.13507\n",
      "Epoch: 00 [14044/20209 ( 69%)], Train Loss: 0.13486\n",
      "Epoch: 00 [14084/20209 ( 70%)], Train Loss: 0.13476\n",
      "Epoch: 00 [14124/20209 ( 70%)], Train Loss: 0.13460\n",
      "Epoch: 00 [14164/20209 ( 70%)], Train Loss: 0.13448\n",
      "Epoch: 00 [14204/20209 ( 70%)], Train Loss: 0.13437\n",
      "Epoch: 00 [14244/20209 ( 70%)], Train Loss: 0.13423\n",
      "Epoch: 00 [14284/20209 ( 71%)], Train Loss: 0.13407\n",
      "Epoch: 00 [14324/20209 ( 71%)], Train Loss: 0.13400\n",
      "Epoch: 00 [14364/20209 ( 71%)], Train Loss: 0.13383\n",
      "Epoch: 00 [14404/20209 ( 71%)], Train Loss: 0.13373\n",
      "Epoch: 00 [14444/20209 ( 71%)], Train Loss: 0.13362\n",
      "Epoch: 00 [14484/20209 ( 72%)], Train Loss: 0.13364\n",
      "Epoch: 00 [14524/20209 ( 72%)], Train Loss: 0.13350\n",
      "Epoch: 00 [14564/20209 ( 72%)], Train Loss: 0.13353\n",
      "Epoch: 00 [14604/20209 ( 72%)], Train Loss: 0.13339\n",
      "Epoch: 00 [14644/20209 ( 72%)], Train Loss: 0.13328\n",
      "Epoch: 00 [14684/20209 ( 73%)], Train Loss: 0.13302\n",
      "Epoch: 00 [14724/20209 ( 73%)], Train Loss: 0.13288\n",
      "Epoch: 00 [14764/20209 ( 73%)], Train Loss: 0.13270\n",
      "Epoch: 00 [14804/20209 ( 73%)], Train Loss: 0.13261\n",
      "Epoch: 00 [14844/20209 ( 73%)], Train Loss: 0.13252\n",
      "Epoch: 00 [14884/20209 ( 74%)], Train Loss: 0.13235\n",
      "Epoch: 00 [14924/20209 ( 74%)], Train Loss: 0.13221\n",
      "Epoch: 00 [14964/20209 ( 74%)], Train Loss: 0.13202\n",
      "Epoch: 00 [15004/20209 ( 74%)], Train Loss: 0.13189\n",
      "Epoch: 00 [15044/20209 ( 74%)], Train Loss: 0.13180\n",
      "Epoch: 00 [15084/20209 ( 75%)], Train Loss: 0.13168\n",
      "Epoch: 00 [15124/20209 ( 75%)], Train Loss: 0.13153\n",
      "Epoch: 00 [15164/20209 ( 75%)], Train Loss: 0.13140\n",
      "Epoch: 00 [15204/20209 ( 75%)], Train Loss: 0.13139\n",
      "Epoch: 00 [15244/20209 ( 75%)], Train Loss: 0.13135\n",
      "Epoch: 00 [15284/20209 ( 76%)], Train Loss: 0.13113\n",
      "Epoch: 00 [15324/20209 ( 76%)], Train Loss: 0.13098\n",
      "Epoch: 00 [15364/20209 ( 76%)], Train Loss: 0.13092\n",
      "Epoch: 00 [15404/20209 ( 76%)], Train Loss: 0.13080\n",
      "Epoch: 00 [15444/20209 ( 76%)], Train Loss: 0.13064\n",
      "Epoch: 00 [15484/20209 ( 77%)], Train Loss: 0.13045\n",
      "Epoch: 00 [15524/20209 ( 77%)], Train Loss: 0.13035\n",
      "Epoch: 00 [15564/20209 ( 77%)], Train Loss: 0.13011\n",
      "Epoch: 00 [15604/20209 ( 77%)], Train Loss: 0.13000\n",
      "Epoch: 00 [15644/20209 ( 77%)], Train Loss: 0.12999\n",
      "Epoch: 00 [15684/20209 ( 78%)], Train Loss: 0.12982\n",
      "Epoch: 00 [15724/20209 ( 78%)], Train Loss: 0.12975\n",
      "Epoch: 00 [15764/20209 ( 78%)], Train Loss: 0.12972\n",
      "Epoch: 00 [15804/20209 ( 78%)], Train Loss: 0.12969\n",
      "Epoch: 00 [15844/20209 ( 78%)], Train Loss: 0.12959\n",
      "Epoch: 00 [15884/20209 ( 79%)], Train Loss: 0.12955\n",
      "Epoch: 00 [15924/20209 ( 79%)], Train Loss: 0.12948\n",
      "Epoch: 00 [15964/20209 ( 79%)], Train Loss: 0.12942\n",
      "Epoch: 00 [16004/20209 ( 79%)], Train Loss: 0.12925\n",
      "Epoch: 00 [16044/20209 ( 79%)], Train Loss: 0.12915\n",
      "Epoch: 00 [16084/20209 ( 80%)], Train Loss: 0.12897\n",
      "Epoch: 00 [16124/20209 ( 80%)], Train Loss: 0.12882\n",
      "Epoch: 00 [16164/20209 ( 80%)], Train Loss: 0.12866\n",
      "Epoch: 00 [16204/20209 ( 80%)], Train Loss: 0.12849\n",
      "Epoch: 00 [16244/20209 ( 80%)], Train Loss: 0.12843\n",
      "Epoch: 00 [16284/20209 ( 81%)], Train Loss: 0.12835\n",
      "Epoch: 00 [16324/20209 ( 81%)], Train Loss: 0.12824\n",
      "Epoch: 00 [16364/20209 ( 81%)], Train Loss: 0.12820\n",
      "Epoch: 00 [16404/20209 ( 81%)], Train Loss: 0.12807\n",
      "Epoch: 00 [16444/20209 ( 81%)], Train Loss: 0.12793\n",
      "Epoch: 00 [16484/20209 ( 82%)], Train Loss: 0.12785\n",
      "Epoch: 00 [16524/20209 ( 82%)], Train Loss: 0.12779\n",
      "Epoch: 00 [16564/20209 ( 82%)], Train Loss: 0.12766\n",
      "Epoch: 00 [16604/20209 ( 82%)], Train Loss: 0.12756\n",
      "Epoch: 00 [16644/20209 ( 82%)], Train Loss: 0.12749\n",
      "Epoch: 00 [16684/20209 ( 83%)], Train Loss: 0.12740\n",
      "Epoch: 00 [16724/20209 ( 83%)], Train Loss: 0.12724\n",
      "Epoch: 00 [16764/20209 ( 83%)], Train Loss: 0.12722\n",
      "Epoch: 00 [16804/20209 ( 83%)], Train Loss: 0.12713\n",
      "Epoch: 00 [16844/20209 ( 83%)], Train Loss: 0.12723\n",
      "Epoch: 00 [16884/20209 ( 84%)], Train Loss: 0.12705\n",
      "Epoch: 00 [16924/20209 ( 84%)], Train Loss: 0.12696\n",
      "Epoch: 00 [16964/20209 ( 84%)], Train Loss: 0.12680\n",
      "Epoch: 00 [17004/20209 ( 84%)], Train Loss: 0.12671\n",
      "Epoch: 00 [17044/20209 ( 84%)], Train Loss: 0.12665\n",
      "Epoch: 00 [17084/20209 ( 85%)], Train Loss: 0.12659\n",
      "Epoch: 00 [17124/20209 ( 85%)], Train Loss: 0.12646\n",
      "Epoch: 00 [17164/20209 ( 85%)], Train Loss: 0.12639\n",
      "Epoch: 00 [17204/20209 ( 85%)], Train Loss: 0.12624\n",
      "Epoch: 00 [17244/20209 ( 85%)], Train Loss: 0.12625\n",
      "Epoch: 00 [17284/20209 ( 86%)], Train Loss: 0.12609\n",
      "Epoch: 00 [17324/20209 ( 86%)], Train Loss: 0.12600\n",
      "Epoch: 00 [17364/20209 ( 86%)], Train Loss: 0.12598\n",
      "Epoch: 00 [17404/20209 ( 86%)], Train Loss: 0.12586\n",
      "Epoch: 00 [17444/20209 ( 86%)], Train Loss: 0.12576\n",
      "Epoch: 00 [17484/20209 ( 87%)], Train Loss: 0.12565\n",
      "Epoch: 00 [17524/20209 ( 87%)], Train Loss: 0.12549\n",
      "Epoch: 00 [17564/20209 ( 87%)], Train Loss: 0.12535\n",
      "Epoch: 00 [17604/20209 ( 87%)], Train Loss: 0.12521\n",
      "Epoch: 00 [17644/20209 ( 87%)], Train Loss: 0.12507\n",
      "Epoch: 00 [17684/20209 ( 88%)], Train Loss: 0.12493\n",
      "Epoch: 00 [17724/20209 ( 88%)], Train Loss: 0.12480\n",
      "Epoch: 00 [17764/20209 ( 88%)], Train Loss: 0.12469\n",
      "Epoch: 00 [17804/20209 ( 88%)], Train Loss: 0.12468\n",
      "Epoch: 00 [17844/20209 ( 88%)], Train Loss: 0.12463\n",
      "Epoch: 00 [17884/20209 ( 88%)], Train Loss: 0.12452\n",
      "Epoch: 00 [17924/20209 ( 89%)], Train Loss: 0.12438\n",
      "Epoch: 00 [17964/20209 ( 89%)], Train Loss: 0.12431\n",
      "Epoch: 00 [18004/20209 ( 89%)], Train Loss: 0.12420\n",
      "Epoch: 00 [18044/20209 ( 89%)], Train Loss: 0.12411\n",
      "Epoch: 00 [18084/20209 ( 89%)], Train Loss: 0.12409\n",
      "Epoch: 00 [18124/20209 ( 90%)], Train Loss: 0.12402\n",
      "Epoch: 00 [18164/20209 ( 90%)], Train Loss: 0.12410\n",
      "Epoch: 00 [18204/20209 ( 90%)], Train Loss: 0.12400\n",
      "Epoch: 00 [18244/20209 ( 90%)], Train Loss: 0.12405\n",
      "Epoch: 00 [18284/20209 ( 90%)], Train Loss: 0.12399\n",
      "Epoch: 00 [18324/20209 ( 91%)], Train Loss: 0.12395\n",
      "Epoch: 00 [18364/20209 ( 91%)], Train Loss: 0.12393\n",
      "Epoch: 00 [18404/20209 ( 91%)], Train Loss: 0.12387\n",
      "Epoch: 00 [18444/20209 ( 91%)], Train Loss: 0.12376\n",
      "Epoch: 00 [18484/20209 ( 91%)], Train Loss: 0.12364\n",
      "Epoch: 00 [18524/20209 ( 92%)], Train Loss: 0.12359\n",
      "Epoch: 00 [18564/20209 ( 92%)], Train Loss: 0.12350\n",
      "Epoch: 00 [18604/20209 ( 92%)], Train Loss: 0.12359\n",
      "Epoch: 00 [18644/20209 ( 92%)], Train Loss: 0.12344\n",
      "Epoch: 00 [18684/20209 ( 92%)], Train Loss: 0.12335\n",
      "Epoch: 00 [18724/20209 ( 93%)], Train Loss: 0.12335\n",
      "Epoch: 00 [18764/20209 ( 93%)], Train Loss: 0.12325\n",
      "Epoch: 00 [18804/20209 ( 93%)], Train Loss: 0.12315\n",
      "Epoch: 00 [18844/20209 ( 93%)], Train Loss: 0.12300\n",
      "Epoch: 00 [18884/20209 ( 93%)], Train Loss: 0.12286\n",
      "Epoch: 00 [18924/20209 ( 94%)], Train Loss: 0.12266\n",
      "Epoch: 00 [18964/20209 ( 94%)], Train Loss: 0.12259\n",
      "Epoch: 00 [19004/20209 ( 94%)], Train Loss: 0.12244\n",
      "Epoch: 00 [19044/20209 ( 94%)], Train Loss: 0.12231\n",
      "Epoch: 00 [19084/20209 ( 94%)], Train Loss: 0.12220\n",
      "Epoch: 00 [19124/20209 ( 95%)], Train Loss: 0.12210\n",
      "Epoch: 00 [19164/20209 ( 95%)], Train Loss: 0.12211\n",
      "Epoch: 00 [19204/20209 ( 95%)], Train Loss: 0.12198\n",
      "Epoch: 00 [19244/20209 ( 95%)], Train Loss: 0.12187\n",
      "Epoch: 00 [19284/20209 ( 95%)], Train Loss: 0.12176\n",
      "Epoch: 00 [19324/20209 ( 96%)], Train Loss: 0.12174\n",
      "Epoch: 00 [19364/20209 ( 96%)], Train Loss: 0.12173\n",
      "Epoch: 00 [19404/20209 ( 96%)], Train Loss: 0.12162\n",
      "Epoch: 00 [19444/20209 ( 96%)], Train Loss: 0.12151\n",
      "Epoch: 00 [19484/20209 ( 96%)], Train Loss: 0.12152\n",
      "Epoch: 00 [19524/20209 ( 97%)], Train Loss: 0.12151\n",
      "Epoch: 00 [19564/20209 ( 97%)], Train Loss: 0.12143\n",
      "Epoch: 00 [19604/20209 ( 97%)], Train Loss: 0.12136\n",
      "Epoch: 00 [19644/20209 ( 97%)], Train Loss: 0.12119\n",
      "Epoch: 00 [19684/20209 ( 97%)], Train Loss: 0.12112\n",
      "Epoch: 00 [19724/20209 ( 98%)], Train Loss: 0.12109\n",
      "Epoch: 00 [19764/20209 ( 98%)], Train Loss: 0.12100\n",
      "Epoch: 00 [19804/20209 ( 98%)], Train Loss: 0.12098\n",
      "Epoch: 00 [19844/20209 ( 98%)], Train Loss: 0.12085\n",
      "Epoch: 00 [19884/20209 ( 98%)], Train Loss: 0.12079\n",
      "Epoch: 00 [19924/20209 ( 99%)], Train Loss: 0.12081\n",
      "Epoch: 00 [19964/20209 ( 99%)], Train Loss: 0.12071\n",
      "Epoch: 00 [20004/20209 ( 99%)], Train Loss: 0.12064\n",
      "Epoch: 00 [20044/20209 ( 99%)], Train Loss: 0.12067\n",
      "Epoch: 00 [20084/20209 ( 99%)], Train Loss: 0.12059\n",
      "Epoch: 00 [20124/20209 (100%)], Train Loss: 0.12054\n",
      "Epoch: 00 [20164/20209 (100%)], Train Loss: 0.12049\n",
      "Epoch: 00 [20204/20209 (100%)], Train Loss: 0.12036\n",
      "Epoch: 00 [20209/20209 (100%)], Train Loss: 0.12034\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.22871\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.22871\n",
      "Saving model checkpoint to output/checkpoint-fold-4.\n",
      "\n",
      "Epoch: 01 [    4/20209 (  0%)], Train Loss: 0.05773\n",
      "Epoch: 01 [   44/20209 (  0%)], Train Loss: 0.05561\n",
      "Epoch: 01 [   84/20209 (  0%)], Train Loss: 0.05804\n",
      "Epoch: 01 [  124/20209 (  1%)], Train Loss: 0.08676\n",
      "Epoch: 01 [  164/20209 (  1%)], Train Loss: 0.09061\n",
      "Epoch: 01 [  204/20209 (  1%)], Train Loss: 0.09197\n",
      "Epoch: 01 [  244/20209 (  1%)], Train Loss: 0.08516\n",
      "Epoch: 01 [  284/20209 (  1%)], Train Loss: 0.08102\n",
      "Epoch: 01 [  324/20209 (  2%)], Train Loss: 0.08096\n",
      "Epoch: 01 [  364/20209 (  2%)], Train Loss: 0.08034\n",
      "Epoch: 01 [  404/20209 (  2%)], Train Loss: 0.08146\n",
      "Epoch: 01 [  444/20209 (  2%)], Train Loss: 0.08160\n",
      "Epoch: 01 [  484/20209 (  2%)], Train Loss: 0.07955\n",
      "Epoch: 01 [  524/20209 (  3%)], Train Loss: 0.07914\n",
      "Epoch: 01 [  564/20209 (  3%)], Train Loss: 0.08083\n",
      "Epoch: 01 [  604/20209 (  3%)], Train Loss: 0.07983\n",
      "Epoch: 01 [  644/20209 (  3%)], Train Loss: 0.08138\n",
      "Epoch: 01 [  684/20209 (  3%)], Train Loss: 0.07934\n",
      "Epoch: 01 [  724/20209 (  4%)], Train Loss: 0.08053\n",
      "Epoch: 01 [  764/20209 (  4%)], Train Loss: 0.07761\n",
      "Epoch: 01 [  804/20209 (  4%)], Train Loss: 0.07756\n",
      "Epoch: 01 [  844/20209 (  4%)], Train Loss: 0.07576\n",
      "Epoch: 01 [  884/20209 (  4%)], Train Loss: 0.07597\n",
      "Epoch: 01 [  924/20209 (  5%)], Train Loss: 0.07685\n",
      "Epoch: 01 [  964/20209 (  5%)], Train Loss: 0.07650\n",
      "Epoch: 01 [ 1004/20209 (  5%)], Train Loss: 0.07693\n",
      "Epoch: 01 [ 1044/20209 (  5%)], Train Loss: 0.07617\n",
      "Epoch: 01 [ 1084/20209 (  5%)], Train Loss: 0.07501\n",
      "Epoch: 01 [ 1124/20209 (  6%)], Train Loss: 0.07570\n",
      "Epoch: 01 [ 1164/20209 (  6%)], Train Loss: 0.07436\n",
      "Epoch: 01 [ 1204/20209 (  6%)], Train Loss: 0.07357\n",
      "Epoch: 01 [ 1244/20209 (  6%)], Train Loss: 0.07248\n",
      "Epoch: 01 [ 1284/20209 (  6%)], Train Loss: 0.07219\n",
      "Epoch: 01 [ 1324/20209 (  7%)], Train Loss: 0.07125\n",
      "Epoch: 01 [ 1364/20209 (  7%)], Train Loss: 0.07149\n",
      "Epoch: 01 [ 1404/20209 (  7%)], Train Loss: 0.07141\n",
      "Epoch: 01 [ 1444/20209 (  7%)], Train Loss: 0.07132\n",
      "Epoch: 01 [ 1484/20209 (  7%)], Train Loss: 0.07015\n",
      "Epoch: 01 [ 1524/20209 (  8%)], Train Loss: 0.06980\n",
      "Epoch: 01 [ 1564/20209 (  8%)], Train Loss: 0.06907\n",
      "Epoch: 01 [ 1604/20209 (  8%)], Train Loss: 0.06864\n",
      "Epoch: 01 [ 1644/20209 (  8%)], Train Loss: 0.06846\n",
      "Epoch: 01 [ 1684/20209 (  8%)], Train Loss: 0.06897\n",
      "Epoch: 01 [ 1724/20209 (  9%)], Train Loss: 0.06834\n",
      "Epoch: 01 [ 1764/20209 (  9%)], Train Loss: 0.06779\n",
      "Epoch: 01 [ 1804/20209 (  9%)], Train Loss: 0.06729\n",
      "Epoch: 01 [ 1844/20209 (  9%)], Train Loss: 0.06678\n",
      "Epoch: 01 [ 1884/20209 (  9%)], Train Loss: 0.06680\n",
      "Epoch: 01 [ 1924/20209 ( 10%)], Train Loss: 0.06635\n",
      "Epoch: 01 [ 1964/20209 ( 10%)], Train Loss: 0.06655\n",
      "Epoch: 01 [ 2004/20209 ( 10%)], Train Loss: 0.06599\n",
      "Epoch: 01 [ 2044/20209 ( 10%)], Train Loss: 0.06596\n",
      "Epoch: 01 [ 2084/20209 ( 10%)], Train Loss: 0.06607\n",
      "Epoch: 01 [ 2124/20209 ( 11%)], Train Loss: 0.06565\n",
      "Epoch: 01 [ 2164/20209 ( 11%)], Train Loss: 0.06525\n",
      "Epoch: 01 [ 2204/20209 ( 11%)], Train Loss: 0.06491\n",
      "Epoch: 01 [ 2244/20209 ( 11%)], Train Loss: 0.06488\n",
      "Epoch: 01 [ 2284/20209 ( 11%)], Train Loss: 0.06534\n",
      "Epoch: 01 [ 2324/20209 ( 11%)], Train Loss: 0.06485\n",
      "Epoch: 01 [ 2364/20209 ( 12%)], Train Loss: 0.06454\n",
      "Epoch: 01 [ 2404/20209 ( 12%)], Train Loss: 0.06424\n",
      "Epoch: 01 [ 2444/20209 ( 12%)], Train Loss: 0.06429\n",
      "Epoch: 01 [ 2484/20209 ( 12%)], Train Loss: 0.06409\n",
      "Epoch: 01 [ 2524/20209 ( 12%)], Train Loss: 0.06395\n",
      "Epoch: 01 [ 2564/20209 ( 13%)], Train Loss: 0.06422\n",
      "Epoch: 01 [ 2604/20209 ( 13%)], Train Loss: 0.06355\n",
      "Epoch: 01 [ 2644/20209 ( 13%)], Train Loss: 0.06289\n",
      "Epoch: 01 [ 2684/20209 ( 13%)], Train Loss: 0.06296\n",
      "Epoch: 01 [ 2724/20209 ( 13%)], Train Loss: 0.06253\n",
      "Epoch: 01 [ 2764/20209 ( 14%)], Train Loss: 0.06232\n",
      "Epoch: 01 [ 2804/20209 ( 14%)], Train Loss: 0.06215\n",
      "Epoch: 01 [ 2844/20209 ( 14%)], Train Loss: 0.06197\n",
      "Epoch: 01 [ 2884/20209 ( 14%)], Train Loss: 0.06187\n",
      "Epoch: 01 [ 2924/20209 ( 14%)], Train Loss: 0.06143\n",
      "Epoch: 01 [ 2964/20209 ( 15%)], Train Loss: 0.06123\n",
      "Epoch: 01 [ 3004/20209 ( 15%)], Train Loss: 0.06140\n",
      "Epoch: 01 [ 3044/20209 ( 15%)], Train Loss: 0.06131\n",
      "Epoch: 01 [ 3084/20209 ( 15%)], Train Loss: 0.06103\n",
      "Epoch: 01 [ 3124/20209 ( 15%)], Train Loss: 0.06053\n",
      "Epoch: 01 [ 3164/20209 ( 16%)], Train Loss: 0.06035\n",
      "Epoch: 01 [ 3204/20209 ( 16%)], Train Loss: 0.06009\n",
      "Epoch: 01 [ 3244/20209 ( 16%)], Train Loss: 0.06002\n",
      "Epoch: 01 [ 3284/20209 ( 16%)], Train Loss: 0.05968\n",
      "Epoch: 01 [ 3324/20209 ( 16%)], Train Loss: 0.05923\n",
      "Epoch: 01 [ 3364/20209 ( 17%)], Train Loss: 0.05891\n",
      "Epoch: 01 [ 3404/20209 ( 17%)], Train Loss: 0.05858\n",
      "Epoch: 01 [ 3444/20209 ( 17%)], Train Loss: 0.05819\n",
      "Epoch: 01 [ 3484/20209 ( 17%)], Train Loss: 0.05846\n",
      "Epoch: 01 [ 3524/20209 ( 17%)], Train Loss: 0.05806\n",
      "Epoch: 01 [ 3564/20209 ( 18%)], Train Loss: 0.05779\n",
      "Epoch: 01 [ 3604/20209 ( 18%)], Train Loss: 0.05757\n",
      "Epoch: 01 [ 3644/20209 ( 18%)], Train Loss: 0.05725\n",
      "Epoch: 01 [ 3684/20209 ( 18%)], Train Loss: 0.05730\n",
      "Epoch: 01 [ 3724/20209 ( 18%)], Train Loss: 0.05695\n",
      "Epoch: 01 [ 3764/20209 ( 19%)], Train Loss: 0.05695\n",
      "Epoch: 01 [ 3804/20209 ( 19%)], Train Loss: 0.05670\n",
      "Epoch: 01 [ 3844/20209 ( 19%)], Train Loss: 0.05646\n",
      "Epoch: 01 [ 3884/20209 ( 19%)], Train Loss: 0.05639\n",
      "Epoch: 01 [ 3924/20209 ( 19%)], Train Loss: 0.05613\n",
      "Epoch: 01 [ 3964/20209 ( 20%)], Train Loss: 0.05605\n",
      "Epoch: 01 [ 4004/20209 ( 20%)], Train Loss: 0.05563\n",
      "Epoch: 01 [ 4044/20209 ( 20%)], Train Loss: 0.05528\n",
      "Epoch: 01 [ 4084/20209 ( 20%)], Train Loss: 0.05529\n",
      "Epoch: 01 [ 4124/20209 ( 20%)], Train Loss: 0.05504\n",
      "Epoch: 01 [ 4164/20209 ( 21%)], Train Loss: 0.05486\n",
      "Epoch: 01 [ 4204/20209 ( 21%)], Train Loss: 0.05477\n",
      "Epoch: 01 [ 4244/20209 ( 21%)], Train Loss: 0.05440\n",
      "Epoch: 01 [ 4284/20209 ( 21%)], Train Loss: 0.05411\n",
      "Epoch: 01 [ 4324/20209 ( 21%)], Train Loss: 0.05387\n",
      "Epoch: 01 [ 4364/20209 ( 22%)], Train Loss: 0.05349\n",
      "Epoch: 01 [ 4404/20209 ( 22%)], Train Loss: 0.05337\n",
      "Epoch: 01 [ 4444/20209 ( 22%)], Train Loss: 0.05313\n",
      "Epoch: 01 [ 4484/20209 ( 22%)], Train Loss: 0.05313\n",
      "Epoch: 01 [ 4524/20209 ( 22%)], Train Loss: 0.05280\n",
      "Epoch: 01 [ 4564/20209 ( 23%)], Train Loss: 0.05258\n",
      "Epoch: 01 [ 4604/20209 ( 23%)], Train Loss: 0.05238\n",
      "Epoch: 01 [ 4644/20209 ( 23%)], Train Loss: 0.05215\n",
      "Epoch: 01 [ 4684/20209 ( 23%)], Train Loss: 0.05187\n",
      "Epoch: 01 [ 4724/20209 ( 23%)], Train Loss: 0.05180\n",
      "Epoch: 01 [ 4764/20209 ( 24%)], Train Loss: 0.05177\n",
      "Epoch: 01 [ 4804/20209 ( 24%)], Train Loss: 0.05156\n",
      "Epoch: 01 [ 4844/20209 ( 24%)], Train Loss: 0.05152\n",
      "Epoch: 01 [ 4884/20209 ( 24%)], Train Loss: 0.05151\n",
      "Epoch: 01 [ 4924/20209 ( 24%)], Train Loss: 0.05153\n",
      "Epoch: 01 [ 4964/20209 ( 25%)], Train Loss: 0.05144\n",
      "Epoch: 01 [ 5004/20209 ( 25%)], Train Loss: 0.05133\n",
      "Epoch: 01 [ 5044/20209 ( 25%)], Train Loss: 0.05114\n",
      "Epoch: 01 [ 5084/20209 ( 25%)], Train Loss: 0.05113\n",
      "Epoch: 01 [ 5124/20209 ( 25%)], Train Loss: 0.05096\n",
      "Epoch: 01 [ 5164/20209 ( 26%)], Train Loss: 0.05086\n",
      "Epoch: 01 [ 5204/20209 ( 26%)], Train Loss: 0.05080\n",
      "Epoch: 01 [ 5244/20209 ( 26%)], Train Loss: 0.05056\n",
      "Epoch: 01 [ 5284/20209 ( 26%)], Train Loss: 0.05037\n",
      "Epoch: 01 [ 5324/20209 ( 26%)], Train Loss: 0.05027\n",
      "Epoch: 01 [ 5364/20209 ( 27%)], Train Loss: 0.05019\n",
      "Epoch: 01 [ 5404/20209 ( 27%)], Train Loss: 0.05004\n",
      "Epoch: 01 [ 5444/20209 ( 27%)], Train Loss: 0.04992\n",
      "Epoch: 01 [ 5484/20209 ( 27%)], Train Loss: 0.04965\n",
      "Epoch: 01 [ 5524/20209 ( 27%)], Train Loss: 0.04936\n",
      "Epoch: 01 [ 5564/20209 ( 28%)], Train Loss: 0.04926\n",
      "Epoch: 01 [ 5604/20209 ( 28%)], Train Loss: 0.04904\n",
      "Epoch: 01 [ 5644/20209 ( 28%)], Train Loss: 0.04884\n",
      "Epoch: 01 [ 5684/20209 ( 28%)], Train Loss: 0.04870\n",
      "Epoch: 01 [ 5724/20209 ( 28%)], Train Loss: 0.04855\n",
      "Epoch: 01 [ 5764/20209 ( 29%)], Train Loss: 0.04828\n",
      "Epoch: 01 [ 5804/20209 ( 29%)], Train Loss: 0.04827\n",
      "Epoch: 01 [ 5844/20209 ( 29%)], Train Loss: 0.04811\n",
      "Epoch: 01 [ 5884/20209 ( 29%)], Train Loss: 0.04812\n",
      "Epoch: 01 [ 5924/20209 ( 29%)], Train Loss: 0.04804\n",
      "Epoch: 01 [ 5964/20209 ( 30%)], Train Loss: 0.04810\n",
      "Epoch: 01 [ 6004/20209 ( 30%)], Train Loss: 0.04800\n",
      "Epoch: 01 [ 6044/20209 ( 30%)], Train Loss: 0.04776\n",
      "Epoch: 01 [ 6084/20209 ( 30%)], Train Loss: 0.04775\n",
      "Epoch: 01 [ 6124/20209 ( 30%)], Train Loss: 0.04769\n",
      "Epoch: 01 [ 6164/20209 ( 31%)], Train Loss: 0.04773\n",
      "Epoch: 01 [ 6204/20209 ( 31%)], Train Loss: 0.04776\n",
      "Epoch: 01 [ 6244/20209 ( 31%)], Train Loss: 0.04760\n",
      "Epoch: 01 [ 6284/20209 ( 31%)], Train Loss: 0.04757\n",
      "Epoch: 01 [ 6324/20209 ( 31%)], Train Loss: 0.04738\n",
      "Epoch: 01 [ 6364/20209 ( 31%)], Train Loss: 0.04755\n",
      "Epoch: 01 [ 6404/20209 ( 32%)], Train Loss: 0.04755\n",
      "Epoch: 01 [ 6444/20209 ( 32%)], Train Loss: 0.04748\n",
      "Epoch: 01 [ 6484/20209 ( 32%)], Train Loss: 0.04725\n",
      "Epoch: 01 [ 6524/20209 ( 32%)], Train Loss: 0.04727\n",
      "Epoch: 01 [ 6564/20209 ( 32%)], Train Loss: 0.04744\n",
      "Epoch: 01 [ 6604/20209 ( 33%)], Train Loss: 0.04732\n",
      "Epoch: 01 [ 6644/20209 ( 33%)], Train Loss: 0.04722\n",
      "Epoch: 01 [ 6684/20209 ( 33%)], Train Loss: 0.04718\n",
      "Epoch: 01 [ 6724/20209 ( 33%)], Train Loss: 0.04704\n",
      "Epoch: 01 [ 6764/20209 ( 33%)], Train Loss: 0.04700\n",
      "Epoch: 01 [ 6804/20209 ( 34%)], Train Loss: 0.04692\n",
      "Epoch: 01 [ 6844/20209 ( 34%)], Train Loss: 0.04695\n",
      "Epoch: 01 [ 6884/20209 ( 34%)], Train Loss: 0.04688\n",
      "Epoch: 01 [ 6924/20209 ( 34%)], Train Loss: 0.04668\n",
      "Epoch: 01 [ 6964/20209 ( 34%)], Train Loss: 0.04656\n",
      "Epoch: 01 [ 7004/20209 ( 35%)], Train Loss: 0.04649\n",
      "Epoch: 01 [ 7044/20209 ( 35%)], Train Loss: 0.04633\n",
      "Epoch: 01 [ 7084/20209 ( 35%)], Train Loss: 0.04615\n",
      "Epoch: 01 [ 7124/20209 ( 35%)], Train Loss: 0.04595\n",
      "Epoch: 01 [ 7164/20209 ( 35%)], Train Loss: 0.04582\n",
      "Epoch: 01 [ 7204/20209 ( 36%)], Train Loss: 0.04583\n",
      "Epoch: 01 [ 7244/20209 ( 36%)], Train Loss: 0.04565\n",
      "Epoch: 01 [ 7284/20209 ( 36%)], Train Loss: 0.04550\n",
      "Epoch: 01 [ 7324/20209 ( 36%)], Train Loss: 0.04532\n",
      "Epoch: 01 [ 7364/20209 ( 36%)], Train Loss: 0.04520\n",
      "Epoch: 01 [ 7404/20209 ( 37%)], Train Loss: 0.04509\n",
      "Epoch: 01 [ 7444/20209 ( 37%)], Train Loss: 0.04499\n",
      "Epoch: 01 [ 7484/20209 ( 37%)], Train Loss: 0.04487\n",
      "Epoch: 01 [ 7524/20209 ( 37%)], Train Loss: 0.04473\n",
      "Epoch: 01 [ 7564/20209 ( 37%)], Train Loss: 0.04462\n",
      "Epoch: 01 [ 7604/20209 ( 38%)], Train Loss: 0.04451\n",
      "Epoch: 01 [ 7644/20209 ( 38%)], Train Loss: 0.04455\n",
      "Epoch: 01 [ 7684/20209 ( 38%)], Train Loss: 0.04459\n",
      "Epoch: 01 [ 7724/20209 ( 38%)], Train Loss: 0.04452\n",
      "Epoch: 01 [ 7764/20209 ( 38%)], Train Loss: 0.04448\n",
      "Epoch: 01 [ 7804/20209 ( 39%)], Train Loss: 0.04432\n",
      "Epoch: 01 [ 7844/20209 ( 39%)], Train Loss: 0.04416\n",
      "Epoch: 01 [ 7884/20209 ( 39%)], Train Loss: 0.04406\n",
      "Epoch: 01 [ 7924/20209 ( 39%)], Train Loss: 0.04402\n",
      "Epoch: 01 [ 7964/20209 ( 39%)], Train Loss: 0.04396\n",
      "Epoch: 01 [ 8004/20209 ( 40%)], Train Loss: 0.04393\n",
      "Epoch: 01 [ 8044/20209 ( 40%)], Train Loss: 0.04382\n",
      "Epoch: 01 [ 8084/20209 ( 40%)], Train Loss: 0.04382\n",
      "Epoch: 01 [ 8124/20209 ( 40%)], Train Loss: 0.04384\n",
      "Epoch: 01 [ 8164/20209 ( 40%)], Train Loss: 0.04381\n",
      "Epoch: 01 [ 8204/20209 ( 41%)], Train Loss: 0.04388\n",
      "Epoch: 01 [ 8244/20209 ( 41%)], Train Loss: 0.04384\n",
      "Epoch: 01 [ 8284/20209 ( 41%)], Train Loss: 0.04375\n",
      "Epoch: 01 [ 8324/20209 ( 41%)], Train Loss: 0.04380\n",
      "Epoch: 01 [ 8364/20209 ( 41%)], Train Loss: 0.04374\n",
      "Epoch: 01 [ 8404/20209 ( 42%)], Train Loss: 0.04372\n",
      "Epoch: 01 [ 8444/20209 ( 42%)], Train Loss: 0.04362\n",
      "Epoch: 01 [ 8484/20209 ( 42%)], Train Loss: 0.04348\n",
      "Epoch: 01 [ 8524/20209 ( 42%)], Train Loss: 0.04333\n",
      "Epoch: 01 [ 8564/20209 ( 42%)], Train Loss: 0.04337\n",
      "Epoch: 01 [ 8604/20209 ( 43%)], Train Loss: 0.04340\n",
      "Epoch: 01 [ 8644/20209 ( 43%)], Train Loss: 0.04336\n",
      "Epoch: 01 [ 8684/20209 ( 43%)], Train Loss: 0.04340\n",
      "Epoch: 01 [ 8724/20209 ( 43%)], Train Loss: 0.04338\n",
      "Epoch: 01 [ 8764/20209 ( 43%)], Train Loss: 0.04335\n",
      "Epoch: 01 [ 8804/20209 ( 44%)], Train Loss: 0.04334\n",
      "Epoch: 01 [ 8844/20209 ( 44%)], Train Loss: 0.04331\n",
      "Epoch: 01 [ 8884/20209 ( 44%)], Train Loss: 0.04326\n",
      "Epoch: 01 [ 8924/20209 ( 44%)], Train Loss: 0.04336\n",
      "Epoch: 01 [ 8964/20209 ( 44%)], Train Loss: 0.04324\n",
      "Epoch: 01 [ 9004/20209 ( 45%)], Train Loss: 0.04319\n",
      "Epoch: 01 [ 9044/20209 ( 45%)], Train Loss: 0.04310\n",
      "Epoch: 01 [ 9084/20209 ( 45%)], Train Loss: 0.04298\n",
      "Epoch: 01 [ 9124/20209 ( 45%)], Train Loss: 0.04289\n",
      "Epoch: 01 [ 9164/20209 ( 45%)], Train Loss: 0.04279\n",
      "Epoch: 01 [ 9204/20209 ( 46%)], Train Loss: 0.04272\n",
      "Epoch: 01 [ 9244/20209 ( 46%)], Train Loss: 0.04266\n",
      "Epoch: 01 [ 9284/20209 ( 46%)], Train Loss: 0.04258\n",
      "Epoch: 01 [ 9324/20209 ( 46%)], Train Loss: 0.04261\n",
      "Epoch: 01 [ 9364/20209 ( 46%)], Train Loss: 0.04252\n",
      "Epoch: 01 [ 9404/20209 ( 47%)], Train Loss: 0.04246\n",
      "Epoch: 01 [ 9444/20209 ( 47%)], Train Loss: 0.04266\n",
      "Epoch: 01 [ 9484/20209 ( 47%)], Train Loss: 0.04252\n",
      "Epoch: 01 [ 9524/20209 ( 47%)], Train Loss: 0.04253\n",
      "Epoch: 01 [ 9564/20209 ( 47%)], Train Loss: 0.04243\n",
      "Epoch: 01 [ 9604/20209 ( 48%)], Train Loss: 0.04236\n",
      "Epoch: 01 [ 9644/20209 ( 48%)], Train Loss: 0.04225\n",
      "Epoch: 01 [ 9684/20209 ( 48%)], Train Loss: 0.04231\n",
      "Epoch: 01 [ 9724/20209 ( 48%)], Train Loss: 0.04222\n",
      "Epoch: 01 [ 9764/20209 ( 48%)], Train Loss: 0.04222\n",
      "Epoch: 01 [ 9804/20209 ( 49%)], Train Loss: 0.04216\n",
      "Epoch: 01 [ 9844/20209 ( 49%)], Train Loss: 0.04212\n",
      "Epoch: 01 [ 9884/20209 ( 49%)], Train Loss: 0.04204\n",
      "Epoch: 01 [ 9924/20209 ( 49%)], Train Loss: 0.04205\n",
      "Epoch: 01 [ 9964/20209 ( 49%)], Train Loss: 0.04201\n",
      "Epoch: 01 [10004/20209 ( 50%)], Train Loss: 0.04199\n",
      "Epoch: 01 [10044/20209 ( 50%)], Train Loss: 0.04194\n",
      "Epoch: 01 [10084/20209 ( 50%)], Train Loss: 0.04189\n",
      "Epoch: 01 [10124/20209 ( 50%)], Train Loss: 0.04200\n",
      "Epoch: 01 [10164/20209 ( 50%)], Train Loss: 0.04191\n",
      "Epoch: 01 [10204/20209 ( 50%)], Train Loss: 0.04184\n",
      "Epoch: 01 [10244/20209 ( 51%)], Train Loss: 0.04174\n",
      "Epoch: 01 [10284/20209 ( 51%)], Train Loss: 0.04166\n",
      "Epoch: 01 [10324/20209 ( 51%)], Train Loss: 0.04170\n",
      "Epoch: 01 [10364/20209 ( 51%)], Train Loss: 0.04169\n",
      "Epoch: 01 [10404/20209 ( 51%)], Train Loss: 0.04185\n",
      "Epoch: 01 [10444/20209 ( 52%)], Train Loss: 0.04181\n",
      "Epoch: 01 [10484/20209 ( 52%)], Train Loss: 0.04170\n",
      "Epoch: 01 [10524/20209 ( 52%)], Train Loss: 0.04168\n",
      "Epoch: 01 [10564/20209 ( 52%)], Train Loss: 0.04167\n",
      "Epoch: 01 [10604/20209 ( 52%)], Train Loss: 0.04157\n",
      "Epoch: 01 [10644/20209 ( 53%)], Train Loss: 0.04159\n",
      "Epoch: 01 [10684/20209 ( 53%)], Train Loss: 0.04151\n",
      "Epoch: 01 [10724/20209 ( 53%)], Train Loss: 0.04150\n",
      "Epoch: 01 [10764/20209 ( 53%)], Train Loss: 0.04152\n",
      "Epoch: 01 [10804/20209 ( 53%)], Train Loss: 0.04145\n",
      "Epoch: 01 [10844/20209 ( 54%)], Train Loss: 0.04140\n",
      "Epoch: 01 [10884/20209 ( 54%)], Train Loss: 0.04135\n",
      "Epoch: 01 [10924/20209 ( 54%)], Train Loss: 0.04131\n",
      "Epoch: 01 [10964/20209 ( 54%)], Train Loss: 0.04126\n",
      "Epoch: 01 [11004/20209 ( 54%)], Train Loss: 0.04116\n",
      "Epoch: 01 [11044/20209 ( 55%)], Train Loss: 0.04124\n",
      "Epoch: 01 [11084/20209 ( 55%)], Train Loss: 0.04119\n",
      "Epoch: 01 [11124/20209 ( 55%)], Train Loss: 0.04115\n",
      "Epoch: 01 [11164/20209 ( 55%)], Train Loss: 0.04105\n",
      "Epoch: 01 [11204/20209 ( 55%)], Train Loss: 0.04106\n",
      "Epoch: 01 [11244/20209 ( 56%)], Train Loss: 0.04098\n",
      "Epoch: 01 [11284/20209 ( 56%)], Train Loss: 0.04104\n",
      "Epoch: 01 [11324/20209 ( 56%)], Train Loss: 0.04097\n",
      "Epoch: 01 [11364/20209 ( 56%)], Train Loss: 0.04092\n",
      "Epoch: 01 [11404/20209 ( 56%)], Train Loss: 0.04087\n",
      "Epoch: 01 [11444/20209 ( 57%)], Train Loss: 0.04080\n",
      "Epoch: 01 [11484/20209 ( 57%)], Train Loss: 0.04085\n",
      "Epoch: 01 [11524/20209 ( 57%)], Train Loss: 0.04089\n",
      "Epoch: 01 [11564/20209 ( 57%)], Train Loss: 0.04080\n",
      "Epoch: 01 [11604/20209 ( 57%)], Train Loss: 0.04086\n",
      "Epoch: 01 [11644/20209 ( 58%)], Train Loss: 0.04077\n",
      "Epoch: 01 [11684/20209 ( 58%)], Train Loss: 0.04072\n",
      "Epoch: 01 [11724/20209 ( 58%)], Train Loss: 0.04064\n",
      "Epoch: 01 [11764/20209 ( 58%)], Train Loss: 0.04055\n",
      "Epoch: 01 [11804/20209 ( 58%)], Train Loss: 0.04047\n",
      "Epoch: 01 [11844/20209 ( 59%)], Train Loss: 0.04048\n",
      "Epoch: 01 [11884/20209 ( 59%)], Train Loss: 0.04039\n",
      "Epoch: 01 [11924/20209 ( 59%)], Train Loss: 0.04032\n",
      "Epoch: 01 [11964/20209 ( 59%)], Train Loss: 0.04026\n",
      "Epoch: 01 [12004/20209 ( 59%)], Train Loss: 0.04021\n",
      "Epoch: 01 [12044/20209 ( 60%)], Train Loss: 0.04019\n",
      "Epoch: 01 [12084/20209 ( 60%)], Train Loss: 0.04012\n",
      "Epoch: 01 [12124/20209 ( 60%)], Train Loss: 0.04007\n",
      "Epoch: 01 [12164/20209 ( 60%)], Train Loss: 0.03999\n",
      "Epoch: 01 [12204/20209 ( 60%)], Train Loss: 0.03998\n",
      "Epoch: 01 [12244/20209 ( 61%)], Train Loss: 0.03994\n",
      "Epoch: 01 [12284/20209 ( 61%)], Train Loss: 0.03983\n",
      "Epoch: 01 [12324/20209 ( 61%)], Train Loss: 0.03973\n",
      "Epoch: 01 [12364/20209 ( 61%)], Train Loss: 0.03970\n",
      "Epoch: 01 [12404/20209 ( 61%)], Train Loss: 0.03962\n",
      "Epoch: 01 [12444/20209 ( 62%)], Train Loss: 0.03977\n",
      "Epoch: 01 [12484/20209 ( 62%)], Train Loss: 0.03968\n",
      "Epoch: 01 [12524/20209 ( 62%)], Train Loss: 0.03964\n",
      "Epoch: 01 [12564/20209 ( 62%)], Train Loss: 0.03957\n",
      "Epoch: 01 [12604/20209 ( 62%)], Train Loss: 0.03958\n",
      "Epoch: 01 [12644/20209 ( 63%)], Train Loss: 0.03959\n",
      "Epoch: 01 [12684/20209 ( 63%)], Train Loss: 0.03954\n",
      "Epoch: 01 [12724/20209 ( 63%)], Train Loss: 0.03950\n",
      "Epoch: 01 [12764/20209 ( 63%)], Train Loss: 0.03953\n",
      "Epoch: 01 [12804/20209 ( 63%)], Train Loss: 0.03951\n",
      "Epoch: 01 [12844/20209 ( 64%)], Train Loss: 0.03951\n",
      "Epoch: 01 [12884/20209 ( 64%)], Train Loss: 0.03947\n",
      "Epoch: 01 [12924/20209 ( 64%)], Train Loss: 0.03938\n",
      "Epoch: 01 [12964/20209 ( 64%)], Train Loss: 0.03936\n",
      "Epoch: 01 [13004/20209 ( 64%)], Train Loss: 0.03935\n",
      "Epoch: 01 [13044/20209 ( 65%)], Train Loss: 0.03936\n",
      "Epoch: 01 [13084/20209 ( 65%)], Train Loss: 0.03933\n",
      "Epoch: 01 [13124/20209 ( 65%)], Train Loss: 0.03929\n",
      "Epoch: 01 [13164/20209 ( 65%)], Train Loss: 0.03927\n",
      "Epoch: 01 [13204/20209 ( 65%)], Train Loss: 0.03925\n",
      "Epoch: 01 [13244/20209 ( 66%)], Train Loss: 0.03923\n",
      "Epoch: 01 [13284/20209 ( 66%)], Train Loss: 0.03922\n",
      "Epoch: 01 [13324/20209 ( 66%)], Train Loss: 0.03915\n",
      "Epoch: 01 [13364/20209 ( 66%)], Train Loss: 0.03918\n",
      "Epoch: 01 [13404/20209 ( 66%)], Train Loss: 0.03913\n",
      "Epoch: 01 [13444/20209 ( 67%)], Train Loss: 0.03910\n",
      "Epoch: 01 [13484/20209 ( 67%)], Train Loss: 0.03907\n",
      "Epoch: 01 [13524/20209 ( 67%)], Train Loss: 0.03900\n",
      "Epoch: 01 [13564/20209 ( 67%)], Train Loss: 0.03899\n",
      "Epoch: 01 [13604/20209 ( 67%)], Train Loss: 0.03894\n",
      "Epoch: 01 [13644/20209 ( 68%)], Train Loss: 0.03886\n",
      "Epoch: 01 [13684/20209 ( 68%)], Train Loss: 0.03887\n",
      "Epoch: 01 [13724/20209 ( 68%)], Train Loss: 0.03880\n",
      "Epoch: 01 [13764/20209 ( 68%)], Train Loss: 0.03890\n",
      "Epoch: 01 [13804/20209 ( 68%)], Train Loss: 0.03888\n",
      "Epoch: 01 [13844/20209 ( 69%)], Train Loss: 0.03886\n",
      "Epoch: 01 [13884/20209 ( 69%)], Train Loss: 0.03885\n",
      "Epoch: 01 [13924/20209 ( 69%)], Train Loss: 0.03883\n",
      "Epoch: 01 [13964/20209 ( 69%)], Train Loss: 0.03885\n",
      "Epoch: 01 [14004/20209 ( 69%)], Train Loss: 0.03890\n",
      "Epoch: 01 [14044/20209 ( 69%)], Train Loss: 0.03884\n",
      "Epoch: 01 [14084/20209 ( 70%)], Train Loss: 0.03888\n",
      "Epoch: 01 [14124/20209 ( 70%)], Train Loss: 0.03887\n",
      "Epoch: 01 [14164/20209 ( 70%)], Train Loss: 0.03886\n",
      "Epoch: 01 [14204/20209 ( 70%)], Train Loss: 0.03888\n",
      "Epoch: 01 [14244/20209 ( 70%)], Train Loss: 0.03889\n",
      "Epoch: 01 [14284/20209 ( 71%)], Train Loss: 0.03889\n",
      "Epoch: 01 [14324/20209 ( 71%)], Train Loss: 0.03891\n",
      "Epoch: 01 [14364/20209 ( 71%)], Train Loss: 0.03884\n",
      "Epoch: 01 [14404/20209 ( 71%)], Train Loss: 0.03883\n",
      "Epoch: 01 [14444/20209 ( 71%)], Train Loss: 0.03881\n",
      "Epoch: 01 [14484/20209 ( 72%)], Train Loss: 0.03888\n",
      "Epoch: 01 [14524/20209 ( 72%)], Train Loss: 0.03884\n",
      "Epoch: 01 [14564/20209 ( 72%)], Train Loss: 0.03888\n",
      "Epoch: 01 [14604/20209 ( 72%)], Train Loss: 0.03888\n",
      "Epoch: 01 [14644/20209 ( 72%)], Train Loss: 0.03889\n",
      "Epoch: 01 [14684/20209 ( 73%)], Train Loss: 0.03880\n",
      "Epoch: 01 [14724/20209 ( 73%)], Train Loss: 0.03880\n",
      "Epoch: 01 [14764/20209 ( 73%)], Train Loss: 0.03874\n",
      "Epoch: 01 [14804/20209 ( 73%)], Train Loss: 0.03874\n",
      "Epoch: 01 [14844/20209 ( 73%)], Train Loss: 0.03875\n",
      "Epoch: 01 [14884/20209 ( 74%)], Train Loss: 0.03872\n",
      "Epoch: 01 [14924/20209 ( 74%)], Train Loss: 0.03873\n",
      "Epoch: 01 [14964/20209 ( 74%)], Train Loss: 0.03868\n",
      "Epoch: 01 [15004/20209 ( 74%)], Train Loss: 0.03865\n",
      "Epoch: 01 [15044/20209 ( 74%)], Train Loss: 0.03865\n",
      "Epoch: 01 [15084/20209 ( 75%)], Train Loss: 0.03867\n",
      "Epoch: 01 [15124/20209 ( 75%)], Train Loss: 0.03862\n",
      "Epoch: 01 [15164/20209 ( 75%)], Train Loss: 0.03863\n",
      "Epoch: 01 [15204/20209 ( 75%)], Train Loss: 0.03861\n",
      "Epoch: 01 [15244/20209 ( 75%)], Train Loss: 0.03857\n",
      "Epoch: 01 [15284/20209 ( 76%)], Train Loss: 0.03851\n",
      "Epoch: 01 [15324/20209 ( 76%)], Train Loss: 0.03848\n",
      "Epoch: 01 [15364/20209 ( 76%)], Train Loss: 0.03847\n",
      "Epoch: 01 [15404/20209 ( 76%)], Train Loss: 0.03844\n",
      "Epoch: 01 [15444/20209 ( 76%)], Train Loss: 0.03841\n",
      "Epoch: 01 [15484/20209 ( 77%)], Train Loss: 0.03836\n",
      "Epoch: 01 [15524/20209 ( 77%)], Train Loss: 0.03831\n",
      "Epoch: 01 [15564/20209 ( 77%)], Train Loss: 0.03822\n",
      "Epoch: 01 [15604/20209 ( 77%)], Train Loss: 0.03821\n",
      "Epoch: 01 [15644/20209 ( 77%)], Train Loss: 0.03819\n",
      "Epoch: 01 [15684/20209 ( 78%)], Train Loss: 0.03816\n",
      "Epoch: 01 [15724/20209 ( 78%)], Train Loss: 0.03815\n",
      "Epoch: 01 [15764/20209 ( 78%)], Train Loss: 0.03820\n",
      "Epoch: 01 [15804/20209 ( 78%)], Train Loss: 0.03821\n",
      "Epoch: 01 [15844/20209 ( 78%)], Train Loss: 0.03818\n",
      "Epoch: 01 [15884/20209 ( 79%)], Train Loss: 0.03816\n",
      "Epoch: 01 [15924/20209 ( 79%)], Train Loss: 0.03814\n",
      "Epoch: 01 [15964/20209 ( 79%)], Train Loss: 0.03811\n",
      "Epoch: 01 [16004/20209 ( 79%)], Train Loss: 0.03808\n",
      "Epoch: 01 [16044/20209 ( 79%)], Train Loss: 0.03806\n",
      "Epoch: 01 [16084/20209 ( 80%)], Train Loss: 0.03803\n",
      "Epoch: 01 [16124/20209 ( 80%)], Train Loss: 0.03800\n",
      "Epoch: 01 [16164/20209 ( 80%)], Train Loss: 0.03797\n",
      "Epoch: 01 [16204/20209 ( 80%)], Train Loss: 0.03790\n",
      "Epoch: 01 [16244/20209 ( 80%)], Train Loss: 0.03792\n",
      "Epoch: 01 [16284/20209 ( 81%)], Train Loss: 0.03791\n",
      "Epoch: 01 [16324/20209 ( 81%)], Train Loss: 0.03794\n",
      "Epoch: 01 [16364/20209 ( 81%)], Train Loss: 0.03794\n",
      "Epoch: 01 [16404/20209 ( 81%)], Train Loss: 0.03792\n",
      "Epoch: 01 [16444/20209 ( 81%)], Train Loss: 0.03791\n",
      "Epoch: 01 [16484/20209 ( 82%)], Train Loss: 0.03791\n",
      "Epoch: 01 [16524/20209 ( 82%)], Train Loss: 0.03793\n",
      "Epoch: 01 [16564/20209 ( 82%)], Train Loss: 0.03792\n",
      "Epoch: 01 [16604/20209 ( 82%)], Train Loss: 0.03788\n",
      "Epoch: 01 [16644/20209 ( 82%)], Train Loss: 0.03785\n",
      "Epoch: 01 [16684/20209 ( 83%)], Train Loss: 0.03789\n",
      "Epoch: 01 [16724/20209 ( 83%)], Train Loss: 0.03782\n",
      "Epoch: 01 [16764/20209 ( 83%)], Train Loss: 0.03782\n",
      "Epoch: 01 [16804/20209 ( 83%)], Train Loss: 0.03783\n",
      "Epoch: 01 [16844/20209 ( 83%)], Train Loss: 0.03789\n",
      "Epoch: 01 [16884/20209 ( 84%)], Train Loss: 0.03784\n",
      "Epoch: 01 [16924/20209 ( 84%)], Train Loss: 0.03785\n",
      "Epoch: 01 [16964/20209 ( 84%)], Train Loss: 0.03781\n",
      "Epoch: 01 [17004/20209 ( 84%)], Train Loss: 0.03782\n",
      "Epoch: 01 [17044/20209 ( 84%)], Train Loss: 0.03782\n",
      "Epoch: 01 [17084/20209 ( 85%)], Train Loss: 0.03780\n",
      "Epoch: 01 [17124/20209 ( 85%)], Train Loss: 0.03779\n",
      "Epoch: 01 [17164/20209 ( 85%)], Train Loss: 0.03778\n",
      "Epoch: 01 [17204/20209 ( 85%)], Train Loss: 0.03774\n",
      "Epoch: 01 [17244/20209 ( 85%)], Train Loss: 0.03777\n",
      "Epoch: 01 [17284/20209 ( 86%)], Train Loss: 0.03774\n",
      "Epoch: 01 [17324/20209 ( 86%)], Train Loss: 0.03772\n",
      "Epoch: 01 [17364/20209 ( 86%)], Train Loss: 0.03773\n",
      "Epoch: 01 [17404/20209 ( 86%)], Train Loss: 0.03770\n",
      "Epoch: 01 [17444/20209 ( 86%)], Train Loss: 0.03768\n",
      "Epoch: 01 [17484/20209 ( 87%)], Train Loss: 0.03762\n",
      "Epoch: 01 [17524/20209 ( 87%)], Train Loss: 0.03759\n",
      "Epoch: 01 [17564/20209 ( 87%)], Train Loss: 0.03754\n",
      "Epoch: 01 [17604/20209 ( 87%)], Train Loss: 0.03752\n",
      "Epoch: 01 [17644/20209 ( 87%)], Train Loss: 0.03747\n",
      "Epoch: 01 [17684/20209 ( 88%)], Train Loss: 0.03744\n",
      "Epoch: 01 [17724/20209 ( 88%)], Train Loss: 0.03738\n",
      "Epoch: 01 [17764/20209 ( 88%)], Train Loss: 0.03737\n",
      "Epoch: 01 [17804/20209 ( 88%)], Train Loss: 0.03737\n",
      "Epoch: 01 [17844/20209 ( 88%)], Train Loss: 0.03738\n",
      "Epoch: 01 [17884/20209 ( 88%)], Train Loss: 0.03736\n",
      "Epoch: 01 [17924/20209 ( 89%)], Train Loss: 0.03735\n",
      "Epoch: 01 [17964/20209 ( 89%)], Train Loss: 0.03734\n",
      "Epoch: 01 [18004/20209 ( 89%)], Train Loss: 0.03730\n",
      "Epoch: 01 [18044/20209 ( 89%)], Train Loss: 0.03731\n",
      "Epoch: 01 [18084/20209 ( 89%)], Train Loss: 0.03736\n",
      "Epoch: 01 [18124/20209 ( 90%)], Train Loss: 0.03740\n",
      "Epoch: 01 [18164/20209 ( 90%)], Train Loss: 0.03746\n",
      "Epoch: 01 [18204/20209 ( 90%)], Train Loss: 0.03747\n",
      "Epoch: 01 [18244/20209 ( 90%)], Train Loss: 0.03755\n",
      "Epoch: 01 [18284/20209 ( 90%)], Train Loss: 0.03755\n",
      "Epoch: 01 [18324/20209 ( 91%)], Train Loss: 0.03758\n",
      "Epoch: 01 [18364/20209 ( 91%)], Train Loss: 0.03763\n",
      "Epoch: 01 [18404/20209 ( 91%)], Train Loss: 0.03764\n",
      "Epoch: 01 [18444/20209 ( 91%)], Train Loss: 0.03762\n",
      "Epoch: 01 [18484/20209 ( 91%)], Train Loss: 0.03759\n",
      "Epoch: 01 [18524/20209 ( 92%)], Train Loss: 0.03764\n",
      "Epoch: 01 [18564/20209 ( 92%)], Train Loss: 0.03762\n",
      "Epoch: 01 [18604/20209 ( 92%)], Train Loss: 0.03771\n",
      "Epoch: 01 [18644/20209 ( 92%)], Train Loss: 0.03767\n",
      "Epoch: 01 [18684/20209 ( 92%)], Train Loss: 0.03765\n",
      "Epoch: 01 [18724/20209 ( 93%)], Train Loss: 0.03767\n",
      "Epoch: 01 [18764/20209 ( 93%)], Train Loss: 0.03767\n",
      "Epoch: 01 [18804/20209 ( 93%)], Train Loss: 0.03767\n",
      "Epoch: 01 [18844/20209 ( 93%)], Train Loss: 0.03765\n",
      "Epoch: 01 [18884/20209 ( 93%)], Train Loss: 0.03763\n",
      "Epoch: 01 [18924/20209 ( 94%)], Train Loss: 0.03756\n",
      "Epoch: 01 [18964/20209 ( 94%)], Train Loss: 0.03755\n",
      "Epoch: 01 [19004/20209 ( 94%)], Train Loss: 0.03751\n",
      "Epoch: 01 [19044/20209 ( 94%)], Train Loss: 0.03747\n",
      "Epoch: 01 [19084/20209 ( 94%)], Train Loss: 0.03746\n",
      "Epoch: 01 [19124/20209 ( 95%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19164/20209 ( 95%)], Train Loss: 0.03751\n",
      "Epoch: 01 [19204/20209 ( 95%)], Train Loss: 0.03750\n",
      "Epoch: 01 [19244/20209 ( 95%)], Train Loss: 0.03745\n",
      "Epoch: 01 [19284/20209 ( 95%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19324/20209 ( 96%)], Train Loss: 0.03748\n",
      "Epoch: 01 [19364/20209 ( 96%)], Train Loss: 0.03752\n",
      "Epoch: 01 [19404/20209 ( 96%)], Train Loss: 0.03751\n",
      "Epoch: 01 [19444/20209 ( 96%)], Train Loss: 0.03748\n",
      "Epoch: 01 [19484/20209 ( 96%)], Train Loss: 0.03750\n",
      "Epoch: 01 [19524/20209 ( 97%)], Train Loss: 0.03751\n",
      "Epoch: 01 [19564/20209 ( 97%)], Train Loss: 0.03750\n",
      "Epoch: 01 [19604/20209 ( 97%)], Train Loss: 0.03751\n",
      "Epoch: 01 [19644/20209 ( 97%)], Train Loss: 0.03745\n",
      "Epoch: 01 [19684/20209 ( 97%)], Train Loss: 0.03745\n",
      "Epoch: 01 [19724/20209 ( 98%)], Train Loss: 0.03747\n",
      "Epoch: 01 [19764/20209 ( 98%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19804/20209 ( 98%)], Train Loss: 0.03747\n",
      "Epoch: 01 [19844/20209 ( 98%)], Train Loss: 0.03744\n",
      "Epoch: 01 [19884/20209 ( 98%)], Train Loss: 0.03746\n",
      "Epoch: 01 [19924/20209 ( 99%)], Train Loss: 0.03750\n",
      "Epoch: 01 [19964/20209 ( 99%)], Train Loss: 0.03745\n",
      "Epoch: 01 [20004/20209 ( 99%)], Train Loss: 0.03747\n",
      "Epoch: 01 [20044/20209 ( 99%)], Train Loss: 0.03753\n",
      "Epoch: 01 [20084/20209 ( 99%)], Train Loss: 0.03755\n",
      "Epoch: 01 [20124/20209 (100%)], Train Loss: 0.03753\n",
      "Epoch: 01 [20164/20209 (100%)], Train Loss: 0.03757\n",
      "Epoch: 01 [20204/20209 (100%)], Train Loss: 0.03753\n",
      "Epoch: 01 [20209/20209 (100%)], Train Loss: 0.03752\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.22187\n",
      "1 Epoch, Best epoch was updated! Valid Loss: 0.22187\n",
      "Saving model checkpoint to output/checkpoint-fold-4.\n",
      "\n",
      "Total Training Time: 5321.793386936188secs, Average Training Time per Epoch: 2660.896693468094secs.\n",
      "Total Validation Time: 250.0760622024536secs, Average Validation Time per Epoch: 125.0380311012268secs.\n"
     ]
    }
   ],
   "source": [
    "# for fold in range(1):\n",
    "for fold in range(5):\n",
    "    print();print()\n",
    "    print('-'*50)\n",
    "    print(f'FOLD: {fold}')\n",
    "    print('-'*50)\n",
    "    run(train, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bf330f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-06T17:41:55.365380Z",
     "iopub.status.busy": "2021-09-06T17:41:55.364424Z",
     "iopub.status.idle": "2021-09-06T17:41:55.369203Z",
     "shell.execute_reply": "2021-09-06T17:41:55.368752Z"
    },
    "id": "DkjRIhdbwjHx",
    "outputId": "c62d2d45-cd44-4ca9-a775-3814971090d7",
    "papermill": {
     "duration": 10.924087,
     "end_time": "2021-09-06T17:41:55.369316",
     "exception": false,
     "start_time": "2021-09-06T17:41:44.445229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example for training second fold\n",
    "\n",
    "# for fold in range(1, 2):\n",
    "#     print();print()\n",
    "#     print('-'*50)\n",
    "#     print(f'FOLD: {fold}')\n",
    "#     print('-'*50)\n",
    "#     run(train, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398e4f2",
   "metadata": {
    "papermill": {
     "duration": 1.294012,
     "end_time": "2021-09-06T17:41:58.023577",
     "exception": false,
     "start_time": "2021-09-06T17:41:56.729565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Thanks and please do Upvote!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6e147",
   "metadata": {
    "papermill": {
     "duration": 1.607387,
     "end_time": "2021-09-06T17:42:00.935753",
     "exception": false,
     "start_time": "2021-09-06T17:41:59.328366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bdaae",
   "metadata": {
    "papermill": {
     "duration": 1.294517,
     "end_time": "2021-09-06T17:42:03.543816",
     "exception": false,
     "start_time": "2021-09-06T17:42:02.249299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28309.321792,
   "end_time": "2021-09-06T17:42:07.916939",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-06T09:50:18.595147",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04697190dd2147d288bca765f8f15a6c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a9d3d14047f445dbd7b841655f8d49f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_953257ee80f2428e97eaaa0ffda67018",
       "placeholder": "",
       "style": "IPY_MODEL_bf4a6c49ae414835a4879f90f8e4f547",
       "value": " 2.24G/2.24G [00:53&lt;00:00, 27.0MB/s]"
      }
     },
     "1030e4e518a44a0a92909870e1c95232": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "15d00af1d58a497f84b6182ebbbbef28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_675adbcf6ccf41c399cb7847f190ae63",
       "placeholder": "",
       "style": "IPY_MODEL_e8e143a192f24a12827924d371dee83d",
       "value": "Downloading: 100%"
      }
     },
     "1c3708245c4a4455ae4d96d0afd909df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_84e37506ad06430d8665d2c0302694d1",
       "placeholder": "",
       "style": "IPY_MODEL_88762b97f5754dfc9645d38c5efc4c85",
       "value": " 5.07M/5.07M [00:00&lt;00:00, 11.2MB/s]"
      }
     },
     "2ca2861b1a55409c95a25f96c2fb45a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6ab1123c86b343d3b544b462ae7399fa",
       "max": 150.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7bd03e0f9a8d45d6b86453ccb03c8c74",
       "value": 150.0
      }
     },
     "2d3fa4990d694c5cb58c1c0e98d7dbbc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40ff7a8e35644ca4bf28249a9f11e321": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "53d4dcb233f14eb998027b17b00392c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "57a4fbbbd9f34b89a68a4a9568dd72a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b23452d94f514145a863a0356f6196c3",
        "IPY_MODEL_2ca2861b1a55409c95a25f96c2fb45a6",
        "IPY_MODEL_82e81d4215144b6191b1b69b0eae2b07"
       ],
       "layout": "IPY_MODEL_9b9495206a494745ae69414bfe9a9b56"
      }
     },
     "5d5402dcac1e4c4497b9b483656c5e3f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f23c848341e44ff804ae46b1f74466b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f3411fa838542218aee015bef731a9a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "61621c4a825741fc9462ad8e46865338": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "630128bbeda948a79500a8eda0637ee8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_04697190dd2147d288bca765f8f15a6c",
       "max": 2239666418.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_63f54f9956fd4e3c910aab4b3a42dce9",
       "value": 2239666418.0
      }
     },
     "63f54f9956fd4e3c910aab4b3a42dce9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "675adbcf6ccf41c399cb7847f190ae63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68ee74c6ad864af9961b5a8072d5fbc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e17123d970dd4dc29369f74a0bf4f1ca",
       "placeholder": "",
       "style": "IPY_MODEL_ce624fb5ca3c41a6b0073708cd9d022a",
       "value": " 179/179 [00:00&lt;00:00, 6.24kB/s]"
      }
     },
     "6ab1123c86b343d3b544b462ae7399fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c6ab2386d3046f7a969dcadf5b06970": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7bd03e0f9a8d45d6b86453ccb03c8c74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "815c2ae3d845423b929ab776b51a336b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82e81d4215144b6191b1b69b0eae2b07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5d5402dcac1e4c4497b9b483656c5e3f",
       "placeholder": "",
       "style": "IPY_MODEL_97e8224700c2435c9a89a55f95e438d9",
       "value": " 150/150 [00:00&lt;00:00, 2.95kB/s]"
      }
     },
     "84ca40f61d7f4a8f9591163369656e4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "84e37506ad06430d8665d2c0302694d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "86396a473dc041b5ac12e7db963a8f88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bdfc6d7b60f44335b2acd7b76e9a2bef",
       "max": 606.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6c6ab2386d3046f7a969dcadf5b06970",
       "value": 606.0
      }
     },
     "88762b97f5754dfc9645d38c5efc4c85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8ddb5fb9f17b4ea98cf7543238edcd9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b941cd23978a4e8ab9ca6e626a262c71",
       "placeholder": "",
       "style": "IPY_MODEL_61621c4a825741fc9462ad8e46865338",
       "value": "Downloading: 100%"
      }
     },
     "953257ee80f2428e97eaaa0ffda67018": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96ee4d8cf6fc4e589bd2749822a56198": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "97e8224700c2435c9a89a55f95e438d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9b9495206a494745ae69414bfe9a9b56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9fb8fe18f00b40d39ab4f5aaf2a3115f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a9bb62c958b5442abf6ddc1f6e41d11d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c6b33a9f0a944d2a84cda509e815291c",
       "max": 5069051.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_53d4dcb233f14eb998027b17b00392c7",
       "value": 5069051.0
      }
     },
     "ac38b1f6e44f434cb9892ee16cdeb630": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "aec977248a41409d8a8baed2ab39ae1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b23452d94f514145a863a0356f6196c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f23c848341e44ff804ae46b1f74466b",
       "placeholder": "",
       "style": "IPY_MODEL_40ff7a8e35644ca4bf28249a9f11e321",
       "value": "Downloading: 100%"
      }
     },
     "b252cd83c11044e994719c5d17c8dc83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ff81f380a67d4222a787f8d6045bde80",
       "max": 179.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ac38b1f6e44f434cb9892ee16cdeb630",
       "value": 179.0
      }
     },
     "b7c522e101864bb497b3299e298787c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f3411fa838542218aee015bef731a9a",
       "placeholder": "",
       "style": "IPY_MODEL_1030e4e518a44a0a92909870e1c95232",
       "value": "Downloading: 100%"
      }
     },
     "b941cd23978a4e8ab9ca6e626a262c71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bdfc6d7b60f44335b2acd7b76e9a2bef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf4a6c49ae414835a4879f90f8e4f547": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c6b33a9f0a944d2a84cda509e815291c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c96a1305868449a8984939218043ac2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dbbf8e1a40944586bea18367c690c2ab",
        "IPY_MODEL_a9bb62c958b5442abf6ddc1f6e41d11d",
        "IPY_MODEL_1c3708245c4a4455ae4d96d0afd909df"
       ],
       "layout": "IPY_MODEL_aec977248a41409d8a8baed2ab39ae1d"
      }
     },
     "ce624fb5ca3c41a6b0073708cd9d022a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "db9f9842df6a4d6f83c2217f44eb09f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_15d00af1d58a497f84b6182ebbbbef28",
        "IPY_MODEL_b252cd83c11044e994719c5d17c8dc83",
        "IPY_MODEL_68ee74c6ad864af9961b5a8072d5fbc5"
       ],
       "layout": "IPY_MODEL_815c2ae3d845423b929ab776b51a336b"
      }
     },
     "dbbf8e1a40944586bea18367c690c2ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2d3fa4990d694c5cb58c1c0e98d7dbbc",
       "placeholder": "",
       "style": "IPY_MODEL_dcb3f315b4284837ba656d1f4ac2ba14",
       "value": "Downloading: 100%"
      }
     },
     "dcb3f315b4284837ba656d1f4ac2ba14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e17123d970dd4dc29369f74a0bf4f1ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e505aed8fa20441b940c2b6de2496272": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9fb8fe18f00b40d39ab4f5aaf2a3115f",
       "placeholder": "",
       "style": "IPY_MODEL_84ca40f61d7f4a8f9591163369656e4f",
       "value": " 606/606 [00:00&lt;00:00, 21.2kB/s]"
      }
     },
     "e8e143a192f24a12827924d371dee83d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ec00fb8fbf1f4691a047de77d3c5755e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "efab4e4c18324bd4901026885c4411ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b7c522e101864bb497b3299e298787c9",
        "IPY_MODEL_630128bbeda948a79500a8eda0637ee8",
        "IPY_MODEL_0a9d3d14047f445dbd7b841655f8d49f"
       ],
       "layout": "IPY_MODEL_ec00fb8fbf1f4691a047de77d3c5755e"
      }
     },
     "efbf7f51507f43bdadddb71078e39305": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8ddb5fb9f17b4ea98cf7543238edcd9c",
        "IPY_MODEL_86396a473dc041b5ac12e7db963a8f88",
        "IPY_MODEL_e505aed8fa20441b940c2b6de2496272"
       ],
       "layout": "IPY_MODEL_96ee4d8cf6fc4e589bd2749822a56198"
      }
     },
     "ff81f380a67d4222a787f8d6045bde80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
