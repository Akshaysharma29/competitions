{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535c2e4d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-08T04:36:56.320751Z",
     "iopub.status.busy": "2021-10-08T04:36:56.320099Z",
     "iopub.status.idle": "2021-10-08T04:36:56.321710Z",
     "shell.execute_reply": "2021-10-08T04:36:56.321230Z",
     "shell.execute_reply.started": "2021-10-07T04:07:15.175299Z"
    },
    "papermill": {
     "duration": 0.041778,
     "end_time": "2021-10-08T04:36:56.321845",
     "exception": false,
     "start_time": "2021-10-08T04:36:56.280067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e324741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:36:56.474142Z",
     "iopub.status.busy": "2021-10-08T04:36:56.463864Z",
     "iopub.status.idle": "2021-10-08T04:37:06.357949Z",
     "shell.execute_reply": "2021-10-08T04:37:06.357442Z",
     "shell.execute_reply.started": "2021-10-07T04:07:15.184444Z"
    },
    "papermill": {
     "duration": 10.004344,
     "end_time": "2021-10-08T04:37:06.358080",
     "exception": false,
     "start_time": "2021-10-08T04:36:56.353736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-cloud 0.1.13 requires tensorflow<3.0,>=1.15.0, which is not installed.\r\n",
      "dask-cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda112, which is not installed.\r\n",
      "cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda110, which is not installed.\r\n",
      "s3fs 2021.6.1 requires fsspec==2021.06.1, but you have fsspec 2021.6.0 which is incompatible.\r\n",
      "pytorch-lightning 1.3.8 requires fsspec[http]!=2021.06.0,>=2021.05.0, but you have fsspec 2021.6.0 which is incompatible.\r\n",
      "dask-cudf 21.6.1+2.g101fc0fda4 requires dask<=2021.5.1,>=2021.4.0, but you have dask 2021.6.2 which is incompatible.\r\n",
      "dask-cudf 21.6.1+2.g101fc0fda4 requires distributed<=2021.5.1,>=2.22.0, but you have distributed 2021.6.2 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# ! pip install datasets transformers\n",
    "!pip uninstall fsspec -qq -y\n",
    "!pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f48050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:06.429086Z",
     "iopub.status.busy": "2021-10-08T04:37:06.428314Z",
     "iopub.status.idle": "2021-10-08T04:37:06.849512Z",
     "shell.execute_reply": "2021-10-08T04:37:06.848983Z",
     "shell.execute_reply.started": "2021-10-07T04:07:24.76849Z"
    },
    "papermill": {
     "duration": 0.457414,
     "end_time": "2021-10-08T04:37:06.849638",
     "exception": false,
     "start_time": "2021-10-08T04:37:06.392224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db940b92",
   "metadata": {
    "papermill": {
     "duration": 0.034218,
     "end_time": "2021-10-08T04:37:06.918516",
     "exception": false,
     "start_time": "2021-10-08T04:37:06.884298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c85b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:06.989777Z",
     "iopub.status.busy": "2021-10-08T04:37:06.989076Z",
     "iopub.status.idle": "2021-10-08T04:37:13.645989Z",
     "shell.execute_reply": "2021-10-08T04:37:13.645468Z",
     "shell.execute_reply.started": "2021-10-07T04:07:25.473167Z"
    },
    "papermill": {
     "duration": 6.693447,
     "end_time": "2021-10-08T04:37:13.646138",
     "exception": false,
     "start_time": "2021-10-08T04:37:06.952691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_checkpoint = \"distilbert-base-uncased\"\n",
    "# model_checkpoint = \"deepset/xlm-roberta-large-squad2\"\n",
    "# model_checkpoint = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-base-squad2\"\n",
    "model_checkpoint = \"../input/pipeline-for-qa-train-with-5-folds-400-135/chaii-trained-model-0\"\n",
    "\n",
    "from transformers import XLMTokenizer,AutoTokenizer\n",
    "# tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19ac9bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:13.718160Z",
     "iopub.status.busy": "2021-10-08T04:37:13.717561Z",
     "iopub.status.idle": "2021-10-08T04:37:13.721106Z",
     "shell.execute_reply": "2021-10-08T04:37:13.721504Z",
     "shell.execute_reply.started": "2021-10-07T04:07:32.615678Z"
    },
    "papermill": {
     "duration": 0.041129,
     "end_time": "2021-10-08T04:37:13.721626",
     "exception": false,
     "start_time": "2021-10-08T04:37:13.680497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898245bd",
   "metadata": {
    "papermill": {
     "duration": 0.033032,
     "end_time": "2021-10-08T04:37:13.787297",
     "exception": false,
     "start_time": "2021-10-08T04:37:13.754265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Doc_stride is used to handle large text: tokens>512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00de4485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:13.854673Z",
     "iopub.status.busy": "2021-10-08T04:37:13.854159Z",
     "iopub.status.idle": "2021-10-08T04:37:13.857982Z",
     "shell.execute_reply": "2021-10-08T04:37:13.857540Z",
     "shell.execute_reply.started": "2021-10-07T04:07:32.622828Z"
    },
    "papermill": {
     "duration": 0.038538,
     "end_time": "2021-10-08T04:37:13.858087",
     "exception": false,
     "start_time": "2021-10-08T04:37:13.819549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 400#384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 135#128 # The authorized overlap between two part of the context when splitting it is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435db41e",
   "metadata": {
    "papermill": {
     "duration": 0.033829,
     "end_time": "2021-10-08T04:37:13.923812",
     "exception": false,
     "start_time": "2021-10-08T04:37:13.889983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2722a80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:13.994131Z",
     "iopub.status.busy": "2021-10-08T04:37:13.993593Z",
     "iopub.status.idle": "2021-10-08T04:37:14.033005Z",
     "shell.execute_reply": "2021-10-08T04:37:14.033379Z",
     "shell.execute_reply.started": "2021-10-07T04:07:45.318908Z"
    },
    "papermill": {
     "duration": 0.0768,
     "end_time": "2021-10-08T04:37:14.033501",
     "exception": false,
     "start_time": "2021-10-08T04:37:13.956701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
    "test_dataset = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7b67f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:14.101369Z",
     "iopub.status.busy": "2021-10-08T04:37:14.100875Z",
     "iopub.status.idle": "2021-10-08T04:37:14.104754Z",
     "shell.execute_reply": "2021-10-08T04:37:14.104328Z",
     "shell.execute_reply.started": "2021-10-07T04:07:46.229554Z"
    },
    "papermill": {
     "duration": 0.038992,
     "end_time": "2021-10-08T04:37:14.104902",
     "exception": false,
     "start_time": "2021-10-08T04:37:14.065910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87831c",
   "metadata": {
    "papermill": {
     "duration": 0.032391,
     "end_time": "2021-10-08T04:37:14.170236",
     "exception": false,
     "start_time": "2021-10-08T04:37:14.137845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e169fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:14.244359Z",
     "iopub.status.busy": "2021-10-08T04:37:14.243641Z",
     "iopub.status.idle": "2021-10-08T04:37:14.246994Z",
     "shell.execute_reply": "2021-10-08T04:37:14.246545Z",
     "shell.execute_reply.started": "2021-10-07T04:07:48.582848Z"
    },
    "papermill": {
     "duration": 0.043437,
     "end_time": "2021-10-08T04:37:14.247102",
     "exception": false,
     "start_time": "2021-10-08T04:37:14.203665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In some padding required on left side\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feff5a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:14.314533Z",
     "iopub.status.busy": "2021-10-08T04:37:14.313985Z",
     "iopub.status.idle": "2021-10-08T04:37:14.317805Z",
     "shell.execute_reply": "2021-10-08T04:37:14.317361Z",
     "shell.execute_reply.started": "2021-10-07T04:07:49.640841Z"
    },
    "papermill": {
     "duration": 0.038634,
     "end_time": "2021-10-08T04:37:14.317909",
     "exception": false,
     "start_time": "2021-10-08T04:37:14.279275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features = prepare_validation_features(test_dataset[:5])\n",
    "# print(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2b4894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:14.415132Z",
     "iopub.status.busy": "2021-10-08T04:37:14.414341Z",
     "iopub.status.idle": "2021-10-08T04:37:14.800149Z",
     "shell.execute_reply": "2021-10-08T04:37:14.800678Z",
     "shell.execute_reply.started": "2021-10-07T04:07:51.217934Z"
    },
    "papermill": {
     "duration": 0.450538,
     "end_time": "2021-10-08T04:37:14.800895",
     "exception": false,
     "start_time": "2021-10-08T04:37:14.350357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8904123efb6420d91ded8a593e7894d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " validation_features = test_dataset.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        remove_columns=test_dataset.column_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d42e4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:14.873146Z",
     "iopub.status.busy": "2021-10-08T04:37:14.871856Z",
     "iopub.status.idle": "2021-10-08T04:37:14.874268Z",
     "shell.execute_reply": "2021-10-08T04:37:14.874714Z",
     "shell.execute_reply.started": "2021-10-07T04:07:51.671956Z"
    },
    "papermill": {
     "duration": 0.04022,
     "end_time": "2021-10-08T04:37:14.874860",
     "exception": false,
     "start_time": "2021-10-08T04:37:14.834640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# import collections\n",
    "\n",
    "\n",
    "# def postprocess_qa_predictions(examples, features, start_logits_cons, end_logits_cons, n_best_size = 20, \n",
    "#                                max_answer_length = 30):\n",
    "#     all_start_logits = start_logits_cons\n",
    "#     all_end_logits = end_logits_cons\n",
    "# #     all_start_logits, all_end_logits = raw_predictions\n",
    "#     # Build a map example to its corresponding features.\n",
    "#     example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "#     features_per_example = collections.defaultdict(list)\n",
    "#     for i, feature in enumerate(features):\n",
    "#         features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "#     # The dictionaries we have to fill.\n",
    "#     predictions = collections.OrderedDict()\n",
    "\n",
    "#     # Logging.\n",
    "#     print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "#     # Let's loop over all the examples!\n",
    "#     for example_index, example in enumerate(tqdm(examples)):\n",
    "#         # Those are the indices of the features associated to the current example.\n",
    "#         feature_indices = features_per_example[example_index]\n",
    "\n",
    "#         min_null_score = None # Only used if squad_v2 is True.\n",
    "#         valid_answers = []\n",
    "        \n",
    "#         context = example[\"context\"]\n",
    "#         # Looping through all the features associated to the current example.\n",
    "#         for feature_index in feature_indices:\n",
    "#             # We grab the predictions of the model for this feature.\n",
    "#             start_logits = all_start_logits[feature_index]\n",
    "#             end_logits = all_end_logits[feature_index]\n",
    "#             # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "#             # context.\n",
    "#             offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "#             # Update minimum null prediction.\n",
    "#             cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "#             feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "#             if min_null_score is None or min_null_score < feature_null_score:\n",
    "#                 min_null_score = feature_null_score\n",
    "\n",
    "#             # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "#             start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "#             end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "#             for start_index in start_indexes:\n",
    "#                 for end_index in end_indexes:\n",
    "#                     # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "#                     # to part of the input_ids that are not in the context.\n",
    "#                     if (\n",
    "#                         start_index >= len(offset_mapping)\n",
    "#                         or end_index >= len(offset_mapping)\n",
    "#                         or offset_mapping[start_index] is None\n",
    "#                         or offset_mapping[end_index] is None\n",
    "#                     ):\n",
    "#                         continue\n",
    "#                     # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "#                     if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "#                         continue\n",
    "\n",
    "#                     start_char = offset_mapping[start_index][0]\n",
    "#                     end_char = offset_mapping[end_index][1]\n",
    "#                     valid_answers.append(\n",
    "#                         {\n",
    "#                             \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "#                             \"text\": context[start_char: end_char]\n",
    "#                         }\n",
    "#                     )\n",
    "        \n",
    "#         if len(valid_answers) > 0:\n",
    "#             best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "#         else:\n",
    "#             # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "#             # failure.\n",
    "#             best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "#         # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "# #         if not squad_v2:\n",
    "#         predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "# #         else:\n",
    "# #             answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "# #             predictions[example[\"id\"]] = answer\n",
    "\n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef27d73",
   "metadata": {
    "papermill": {
     "duration": 0.032223,
     "end_time": "2021-10-08T04:37:14.939581",
     "exception": false,
     "start_time": "2021-10-08T04:37:14.907358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "824b3c59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:15.007719Z",
     "iopub.status.busy": "2021-10-08T04:37:15.007233Z",
     "iopub.status.idle": "2021-10-08T04:37:15.832637Z",
     "shell.execute_reply": "2021-10-08T04:37:15.832123Z",
     "shell.execute_reply.started": "2021-10-07T04:07:56.022685Z"
    },
    "papermill": {
     "duration": 0.861045,
     "end_time": "2021-10-08T04:37:15.832799",
     "exception": false,
     "start_time": "2021-10-08T04:37:14.971754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "gc.collect()\n",
    "folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b582dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:37:15.903509Z",
     "iopub.status.busy": "2021-10-08T04:37:15.902977Z",
     "iopub.status.idle": "2021-10-08T04:39:43.503300Z",
     "shell.execute_reply": "2021-10-08T04:39:43.503964Z",
     "shell.execute_reply.started": "2021-10-07T04:08:33.101178Z"
    },
    "papermill": {
     "duration": 147.637893,
     "end_time": "2021-10-08T04:39:43.504180",
     "exception": false,
     "start_time": "2021-10-08T04:37:15.866287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting using model 0 start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of model 0 complete\n",
      "-----------------------------------\n",
      "predicting using model 1 start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of model 1 complete\n",
      "-----------------------------------\n",
      "predicting using model 2 start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of model 2 complete\n",
      "-----------------------------------\n",
      "predicting using model 3 start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of model 3 complete\n",
      "-----------------------------------\n",
      "predicting using model 4 start\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of model 4 complete\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = []\n",
    "for i in range(folds):\n",
    "    print(f'predicting using model {i} start')\n",
    "#     model_checkpoint = f\"../input/pipeline-for-qa-train-with-5-folds/chaii-trained-model-{i}\"\n",
    "    model_checkpoint = f\"../input/pipeline-for-qa-train-with-5-folds-400-135/chaii-trained-model-{i}\"\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "    trainer = Trainer(model)\n",
    "    raw_predictions.append(trainer.predict(validation_features))\n",
    "    print(f'prediction of model {i} complete')\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72c7d881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:43.582788Z",
     "iopub.status.busy": "2021-10-08T04:39:43.582285Z",
     "iopub.status.idle": "2021-10-08T04:39:43.586430Z",
     "shell.execute_reply": "2021-10-08T04:39:43.586030Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.077292Z"
    },
    "papermill": {
     "duration": 0.043914,
     "end_time": "2021-10-08T04:39:43.586532",
     "exception": false,
     "start_time": "2021-10-08T04:39:43.542618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(raw_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8857340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:43.664254Z",
     "iopub.status.busy": "2021-10-08T04:39:43.663695Z",
     "iopub.status.idle": "2021-10-08T04:39:43.667446Z",
     "shell.execute_reply": "2021-10-08T04:39:43.667950Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.08418Z"
    },
    "papermill": {
     "duration": 0.045008,
     "end_time": "2021-10-08T04:39:43.668132",
     "exception": false,
     "start_time": "2021-10-08T04:39:43.623124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82962e6b",
   "metadata": {
    "papermill": {
     "duration": 0.036441,
     "end_time": "2021-10-08T04:39:43.741388",
     "exception": false,
     "start_time": "2021-10-08T04:39:43.704947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87f3704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:43.822071Z",
     "iopub.status.busy": "2021-10-08T04:39:43.821542Z",
     "iopub.status.idle": "2021-10-08T04:39:43.827026Z",
     "shell.execute_reply": "2021-10-08T04:39:43.827554Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.100049Z"
    },
    "papermill": {
     "duration": 0.049288,
     "end_time": "2021-10-08T04:39:43.827710",
     "exception": false,
     "start_time": "2021-10-08T04:39:43.778422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "start_logits_cons = 0\n",
    "end_logits_cons = 0\n",
    "for raw_prediction in raw_predictions:\n",
    "    sc, ec = raw_prediction.predictions\n",
    "    sc = sc/folds\n",
    "    ec = ec/folds\n",
    "    start_logits_cons += sc\n",
    "    end_logits_cons += ec\n",
    "print(len(start_logits_cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40f9964a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:43.905302Z",
     "iopub.status.busy": "2021-10-08T04:39:43.904753Z",
     "iopub.status.idle": "2021-10-08T04:39:43.908818Z",
     "shell.execute_reply": "2021-10-08T04:39:43.908388Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.112243Z"
    },
    "papermill": {
     "duration": 0.043872,
     "end_time": "2021-10-08T04:39:43.908927",
     "exception": false,
     "start_time": "2021-10-08T04:39:43.865055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start_logits_cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b20f9b7",
   "metadata": {
    "papermill": {
     "duration": 0.036754,
     "end_time": "2021-10-08T04:39:43.983072",
     "exception": false,
     "start_time": "2021-10-08T04:39:43.946318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be362bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:44.068504Z",
     "iopub.status.busy": "2021-10-08T04:39:44.067863Z",
     "iopub.status.idle": "2021-10-08T04:39:44.101909Z",
     "shell.execute_reply": "2021-10-08T04:39:44.102329Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.120155Z"
    },
    "papermill": {
     "duration": 0.082465,
     "end_time": "2021-10-08T04:39:44.102450",
     "exception": false,
     "start_time": "2021-10-08T04:39:44.019985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apex AMP Installed :: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import gc\n",
    "gc.enable()\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn import model_selection\n",
    "from string import punctuation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader,\n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_INSTALLED = True\n",
    "except ImportError:\n",
    "    APEX_INSTALLED = False\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging,\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    ")\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def optimal_num_of_loader_workers():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    return optimal_value\n",
    "\n",
    "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c4a818a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:44.184129Z",
     "iopub.status.busy": "2021-10-08T04:39:44.182901Z",
     "iopub.status.idle": "2021-10-08T04:39:44.185404Z",
     "shell.execute_reply": "2021-10-08T04:39:44.185832Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.165118Z"
    },
    "papermill": {
     "duration": 0.045239,
     "end_time": "2021-10-08T04:39:44.185950",
     "exception": false,
     "start_time": "2021-10-08T04:39:44.140711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # model\n",
    "    model_type = 'xlm_roberta'\n",
    "    model_name_or_path = \"../input/xlm-roberta-large-squad-v2\"\n",
    "    config_name = \"../input/xlm-roberta-large-squad-v2\"\n",
    "    fp16 = True if APEX_INSTALLED else False\n",
    "    fp16_opt_level = \"O1\"\n",
    "    gradient_accumulation_steps = 2\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer_name = \"../input/xlm-roberta-large-squad-v2\"\n",
    "    max_seq_length = 400\n",
    "    doc_stride = 135\n",
    "\n",
    "    # train\n",
    "    epochs = 1\n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 128\n",
    "\n",
    "    # optimzer\n",
    "    optimizer_type = 'AdamW'\n",
    "    learning_rate = 1e-5\n",
    "    weight_decay = 1e-2\n",
    "    epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # scheduler\n",
    "    decay_name = 'linear-warmup'\n",
    "    warmup_ratio = 0.1\n",
    "\n",
    "    # logging\n",
    "    logging_steps = 10\n",
    "\n",
    "    # evaluate\n",
    "    output_dir = 'output'\n",
    "    seed = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e900e5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:44.269314Z",
     "iopub.status.busy": "2021-10-08T04:39:44.268084Z",
     "iopub.status.idle": "2021-10-08T04:39:44.270746Z",
     "shell.execute_reply": "2021-10-08T04:39:44.270363Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.175498Z"
    },
    "papermill": {
     "duration": 0.047745,
     "end_time": "2021-10-08T04:39:44.270870",
     "exception": false,
     "start_time": "2021-10-08T04:39:44.223125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, features, mode='train'):\n",
    "        super(DatasetRetriever, self).__init__()\n",
    "        self.features = features\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, item):   \n",
    "        feature = self.features[item]\n",
    "        if self.mode == 'train':\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
    "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
    "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':feature['offset_mapping'],\n",
    "                'sequence_ids':feature['sequence_ids'],\n",
    "                'id':feature['example_id'],\n",
    "                'context': feature['context'],\n",
    "                'question': feature['question']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a80e5c7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:44.352726Z",
     "iopub.status.busy": "2021-10-08T04:39:44.352227Z",
     "iopub.status.idle": "2021-10-08T04:39:44.355578Z",
     "shell.execute_reply": "2021-10-08T04:39:44.356359Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.188998Z"
    },
    "papermill": {
     "duration": 0.048516,
     "end_time": "2021-10-08T04:39:44.356481",
     "exception": false,
     "start_time": "2021-10-08T04:39:44.307965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, modelname_or_path, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self._init_weights(self.qa_outputs)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "    ):\n",
    "        outputs = self.xlm_roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        # sequence_output = self.dropout(sequence_output)\n",
    "        qa_logits = self.qa_outputs(sequence_output)\n",
    "        \n",
    "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "    \n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a692331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:44.435724Z",
     "iopub.status.busy": "2021-10-08T04:39:44.434356Z",
     "iopub.status.idle": "2021-10-08T04:39:44.437359Z",
     "shell.execute_reply": "2021-10-08T04:39:44.436882Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.201901Z"
    },
    "papermill": {
     "duration": 0.043858,
     "end_time": "2021-10-08T04:39:44.437453",
     "exception": false,
     "start_time": "2021-10-08T04:39:44.393595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    model = Model(args.model_name_or_path, config=config)\n",
    "    return config, tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f8097e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:44.519396Z",
     "iopub.status.busy": "2021-10-08T04:39:44.518743Z",
     "iopub.status.idle": "2021-10-08T04:39:44.522113Z",
     "shell.execute_reply": "2021-10-08T04:39:44.521659Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.21339Z"
    },
    "papermill": {
     "duration": 0.047779,
     "end_time": "2021-10-08T04:39:44.522215",
     "exception": false,
     "start_time": "2021-10-08T04:39:44.474436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_test_features(args, example, tokenizer):\n",
    "    example[\"question\"] = example[\"question\"].lstrip()\n",
    "    \n",
    "    tokenized_example = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=args.max_seq_length,\n",
    "        stride=args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(tokenized_example[\"input_ids\"])):\n",
    "        feature = {}\n",
    "        feature[\"example_id\"] = example['id']\n",
    "        feature['context'] = example['context']\n",
    "        feature['question'] = example['question']\n",
    "        feature['input_ids'] = tokenized_example['input_ids'][i]\n",
    "        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n",
    "        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n",
    "        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ef5fd35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:44.602444Z",
     "iopub.status.busy": "2021-10-08T04:39:44.601965Z",
     "iopub.status.idle": "2021-10-08T04:39:45.905293Z",
     "shell.execute_reply": "2021-10-08T04:39:45.904687Z",
     "shell.execute_reply.started": "2021-10-07T04:11:38.227994Z"
    },
    "papermill": {
     "duration": 1.346247,
     "end_time": "2021-10-08T04:39:45.905421",
     "exception": false,
     "start_time": "2021-10-08T04:39:44.559174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
    "\n",
    "test['context'] = test['context'].apply(lambda x: ' '.join(x.split()))\n",
    "test['question'] = test['question'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "#base_model_path = '../input/chaii-qa-5-fold-xlmroberta-torch-fit'\n",
    "# test=test[:10]\n",
    "#test=test[:10]\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config().tokenizer_name)\n",
    "\n",
    "test_features = []\n",
    "for i, row in test.iterrows():\n",
    "    test_features += prepare_test_features(Config(), row, tokenizer)\n",
    "\n",
    "args = Config()\n",
    "test_dataset = DatasetRetriever(test_features, mode='test')\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.eval_batch_size, \n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    num_workers=optimal_num_of_loader_workers(),\n",
    "    pin_memory=True, \n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "303b9600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:45.985724Z",
     "iopub.status.busy": "2021-10-08T04:39:45.985214Z",
     "iopub.status.idle": "2021-10-08T04:39:45.989089Z",
     "shell.execute_reply": "2021-10-08T04:39:45.988671Z",
     "shell.execute_reply.started": "2021-10-07T04:11:39.552009Z"
    },
    "papermill": {
     "duration": 0.044041,
     "end_time": "2021-10-08T04:39:45.989196",
     "exception": false,
     "start_time": "2021-10-08T04:39:45.945155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = '../input/chaii-xlmr-5-fold/output/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d1b00fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:46.069560Z",
     "iopub.status.busy": "2021-10-08T04:39:46.069065Z",
     "iopub.status.idle": "2021-10-08T04:39:46.072740Z",
     "shell.execute_reply": "2021-10-08T04:39:46.072351Z",
     "shell.execute_reply.started": "2021-10-07T04:11:39.557684Z"
    },
    "papermill": {
     "duration": 0.046762,
     "end_time": "2021-10-08T04:39:46.072861",
     "exception": false,
     "start_time": "2021-10-08T04:39:46.026099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(checkpoint_path):\n",
    "    config, tokenizer, model = make_model(Config())\n",
    "    model.cuda();\n",
    "    model.load_state_dict(\n",
    "        torch.load(base_model + checkpoint_path)\n",
    "    );\n",
    "    \n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs_start, outputs_end = model(batch['input_ids'].cuda(), batch['attention_mask'].cuda())\n",
    "            start_logits.append(outputs_start.cpu().numpy().tolist())\n",
    "            end_logits.append(outputs_end.cpu().numpy().tolist())\n",
    "            del outputs_start, outputs_end\n",
    "    del model, tokenizer, config\n",
    "    gc.collect()\n",
    "    return np.vstack(start_logits), np.vstack(end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5848828b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:39:46.189526Z",
     "iopub.status.busy": "2021-10-08T04:39:46.188710Z",
     "iopub.status.idle": "2021-10-08T04:42:50.381997Z",
     "shell.execute_reply": "2021-10-08T04:42:50.382429Z",
     "shell.execute_reply.started": "2021-10-07T04:11:39.568693Z"
    },
    "papermill": {
     "duration": 184.271813,
     "end_time": "2021-10-08T04:42:50.382600",
     "exception": false,
     "start_time": "2021-10-08T04:39:46.110787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_logits1, end_logits1 = get_predictions('checkpoint-fold-0/pytorch_model.bin')\n",
    "start_logits2, end_logits2 = get_predictions('checkpoint-fold-1/pytorch_model.bin')\n",
    "start_logits3, end_logits3 = get_predictions('checkpoint-fold-2/pytorch_model.bin')\n",
    "start_logits4, end_logits4 = get_predictions('checkpoint-fold-3/pytorch_model.bin')\n",
    "start_logits5, end_logits5 = get_predictions('checkpoint-fold-4/pytorch_model.bin')\n",
    "\n",
    "\n",
    "\n",
    "start_logits = (start_logits1 + start_logits2 + start_logits3 + start_logits4+ start_logits5)/5\n",
    "end_logits = (end_logits1 + end_logits2 + end_logits3 + end_logits4 + end_logits5 )/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77257411",
   "metadata": {
    "papermill": {
     "duration": 0.0367,
     "end_time": "2021-10-08T04:42:50.457504",
     "exception": false,
     "start_time": "2021-10-08T04:42:50.420804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensembled logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daeaf565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:50.537359Z",
     "iopub.status.busy": "2021-10-08T04:42:50.536688Z",
     "iopub.status.idle": "2021-10-08T04:42:50.545187Z",
     "shell.execute_reply": "2021-10-08T04:42:50.545615Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.58382Z"
    },
    "papermill": {
     "duration": 0.050903,
     "end_time": "2021-10-08T04:42:50.545817",
     "exception": false,
     "start_time": "2021-10-08T04:42:50.494914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "normalizer = torch.nn.Softmax(dim=1)\n",
    "\n",
    "start_logits = normalizer(torch.from_numpy(start_logits))\n",
    "end_logits = normalizer(torch.from_numpy(end_logits))\n",
    "\n",
    "start_logits = start_logits.numpy()\n",
    "end_logits = end_logits.numpy()\n",
    "\n",
    "\n",
    "start_logits_cons = normalizer(torch.from_numpy(start_logits_cons))\n",
    "end_logits_cons = normalizer(torch.from_numpy(end_logits_cons))\n",
    "\n",
    "start_logits_cons = start_logits_cons.numpy()\n",
    "end_logits_cons = end_logits_cons.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac5380bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:50.623996Z",
     "iopub.status.busy": "2021-10-08T04:42:50.623381Z",
     "iopub.status.idle": "2021-10-08T04:42:50.628134Z",
     "shell.execute_reply": "2021-10-08T04:42:50.629162Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.599422Z"
    },
    "papermill": {
     "duration": 0.046238,
     "end_time": "2021-10-08T04:42:50.629318",
     "exception": false,
     "start_time": "2021-10-08T04:42:50.583080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 400)\n",
      "(63, 400)\n"
     ]
    }
   ],
   "source": [
    "print(start_logits.shape)\n",
    "print(start_logits_cons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "517fc4ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:50.708477Z",
     "iopub.status.busy": "2021-10-08T04:42:50.707808Z",
     "iopub.status.idle": "2021-10-08T04:42:50.710281Z",
     "shell.execute_reply": "2021-10-08T04:42:50.710644Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.606509Z"
    },
    "papermill": {
     "duration": 0.04398,
     "end_time": "2021-10-08T04:42:50.710778",
     "exception": false,
     "start_time": "2021-10-08T04:42:50.666798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = 0.5#0.3#0.2#0.1#0.05#0.4#0.5\n",
    "model2 = 0.5#0.7#0.8#0.9#0.95#0.6#0.5\n",
    "\n",
    "final_start_logits = model1*start_logits_cons + model2*start_logits\n",
    "final_end_logits = model1*end_logits_cons + model2*end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123e959",
   "metadata": {
    "papermill": {
     "duration": 0.036845,
     "end_time": "2021-10-08T04:42:50.784854",
     "exception": false,
     "start_time": "2021-10-08T04:42:50.748009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparing final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e322a29a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:50.872580Z",
     "iopub.status.busy": "2021-10-08T04:42:50.872032Z",
     "iopub.status.idle": "2021-10-08T04:42:50.875091Z",
     "shell.execute_reply": "2021-10-08T04:42:50.874650Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.616547Z"
    },
    "papermill": {
     "duration": 0.05319,
     "end_time": "2021-10-08T04:42:50.875190",
     "exception": false,
     "start_time": "2021-10-08T04:42:50.822000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    \n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    for example_index, example in examples.iterrows():\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "\n",
    "            sequence_ids = features[feature_index][\"sequence_ids\"]\n",
    "            context_index = 1\n",
    "\n",
    "            features[feature_index][\"offset_mapping\"] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n",
    "            ]\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        \n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d83ff722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:50.954602Z",
     "iopub.status.busy": "2021-10-08T04:42:50.954064Z",
     "iopub.status.idle": "2021-10-08T04:42:50.984706Z",
     "shell.execute_reply": "2021-10-08T04:42:50.985393Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.636015Z"
    },
    "papermill": {
     "duration": 0.073503,
     "end_time": "2021-10-08T04:42:50.985554",
     "exception": false,
     "start_time": "2021-10-08T04:42:50.912051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 5 example predictions split into 63 features.\n"
     ]
    }
   ],
   "source": [
    "# final_predictions = postprocess_qa_predictions(test_dataset, tokenizer, validation_features, (start_logits, end_logits))\n",
    "# final_predictions = postprocess_qa_predictions(test_dataset, tokenizer, validation_features, (start_logits_cons, end_logits_cons))\n",
    "# final_predictions = postprocess_qa_predictions(test_dataset,  tokenizer, test_features, (final_start_logits, final_end_logits))\n",
    "final_predictions = postprocess_qa_predictions(test, test_features, (final_start_logits, final_end_logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b65078ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:51.064854Z",
     "iopub.status.busy": "2021-10-08T04:42:51.064333Z",
     "iopub.status.idle": "2021-10-08T04:42:51.067706Z",
     "shell.execute_reply": "2021-10-08T04:42:51.068135Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.678649Z"
    },
    "papermill": {
     "duration": 0.045103,
     "end_time": "2021-10-08T04:42:51.068248",
     "exception": false,
     "start_time": "2021-10-08T04:42:51.023145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = []\n",
    "for p1, p2 in final_predictions.items():\n",
    "    p2 = \" \".join(p2.split())\n",
    "    p2 = p2.strip(punctuation)\n",
    "#     submission.append((p1, p2))\n",
    "    submission.append(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b49ecfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:51.150091Z",
     "iopub.status.busy": "2021-10-08T04:42:51.149476Z",
     "iopub.status.idle": "2021-10-08T04:42:51.170954Z",
     "shell.execute_reply": "2021-10-08T04:42:51.170546Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.687122Z"
    },
    "papermill": {
     "duration": 0.065575,
     "end_time": "2021-10-08T04:42:51.171057",
     "exception": false,
     "start_time": "2021-10-08T04:42:51.105482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>language</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22bff3dec</td>\n",
       "      <td>  (: 7  1983; , ...</td>\n",
       "      <td>       </td>\n",
       "      <td>hindi</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>282758170</td>\n",
       "      <td>  (Google Maps) (   ...</td>\n",
       "      <td>      ?</td>\n",
       "      <td>hindi</td>\n",
       "      <td>20  2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d60987e0e</td>\n",
       "      <td>   (   -  ...</td>\n",
       "      <td>      ?</td>\n",
       "      <td>hindi</td>\n",
       "      <td>  </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f99c770dc</td>\n",
       "      <td> (: ;   ...</td>\n",
       "      <td>   ?</td>\n",
       "      <td>tamil</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40dec1964</td>\n",
       "      <td>  ,   ...</td>\n",
       "      <td>     ...</td>\n",
       "      <td>tamil</td>\n",
       "      <td>   </td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            context  \\\n",
       "0  22bff3dec    (: 7  1983; , ...   \n",
       "1  282758170    (Google Maps) (   ...   \n",
       "2  d60987e0e     (   -  ...   \n",
       "3  f99c770dc   (: ;   ...   \n",
       "4  40dec1964    ,   ...   \n",
       "\n",
       "                                            question language  \\\n",
       "0                           hindi   \n",
       "1                         ?    hindi   \n",
       "2                        ?    hindi   \n",
       "3                          ?    tamil   \n",
       "4       ...    tamil   \n",
       "\n",
       "                      PredictionString  \n",
       "0                                   \n",
       "1                       20  2010  \n",
       "2                            \n",
       "3                                   13  \n",
       "4       "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"PredictionString\"] = submission\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6deac",
   "metadata": {
    "papermill": {
     "duration": 0.037887,
     "end_time": "2021-10-08T04:42:51.246554",
     "exception": false,
     "start_time": "2021-10-08T04:42:51.208667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Custom Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ee544f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:51.338751Z",
     "iopub.status.busy": "2021-10-08T04:42:51.334647Z",
     "iopub.status.idle": "2021-10-08T04:42:51.341512Z",
     "shell.execute_reply": "2021-10-08T04:42:51.341885Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.714549Z"
    },
    "papermill": {
     "duration": 0.057885,
     "end_time": "2021-10-08T04:42:51.342001",
     "exception": false,
     "start_time": "2021-10-08T04:42:51.284116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"\",  \",\", \";\"]\n",
    "bad_endings = [\"...\", \"-\", \"(\", \")\", \"\", \",\", \";\"]\n",
    "\n",
    "tamil_ad = \".\"\n",
    "tamil_bc = \".\"\n",
    "tamil_km = \".\"\n",
    "hindi_ad = \"\"\n",
    "hindi_bc = \".\"\n",
    "\n",
    "\n",
    "cleaned_preds = []\n",
    "for pred, context in test[[\"PredictionString\", \"context\"]].to_numpy():\n",
    "    if pred == \"\":\n",
    "        cleaned_preds.append(pred)\n",
    "        continue\n",
    "    while any([pred.startswith(y) for y in bad_starts]):\n",
    "        pred = pred[1:]\n",
    "    while any([pred.endswith(y) for y in bad_endings]):\n",
    "        if pred.endswith(\"...\"):\n",
    "            pred = pred[:-3]\n",
    "        else:\n",
    "            pred = pred[:-1]\n",
    "    \n",
    "    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n",
    "        pred = pred+\".\"\n",
    "\n",
    "    cleaned_preds.append(pred)\n",
    "\n",
    "test[\"PredictionString\"] = cleaned_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5ede1bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:51.427361Z",
     "iopub.status.busy": "2021-10-08T04:42:51.426614Z",
     "iopub.status.idle": "2021-10-08T04:42:51.431656Z",
     "shell.execute_reply": "2021-10-08T04:42:51.431087Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.732367Z"
    },
    "papermill": {
     "duration": 0.051831,
     "end_time": "2021-10-08T04:42:51.431778",
     "exception": false,
     "start_time": "2021-10-08T04:42:51.379947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>language</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22bff3dec</td>\n",
       "      <td>  (: 7  1983; , ...</td>\n",
       "      <td>       </td>\n",
       "      <td>hindi</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>282758170</td>\n",
       "      <td>  (Google Maps) (   ...</td>\n",
       "      <td>      ?</td>\n",
       "      <td>hindi</td>\n",
       "      <td>20  2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d60987e0e</td>\n",
       "      <td>   (   -  ...</td>\n",
       "      <td>      ?</td>\n",
       "      <td>hindi</td>\n",
       "      <td>  </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f99c770dc</td>\n",
       "      <td> (: ;   ...</td>\n",
       "      <td>   ?</td>\n",
       "      <td>tamil</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40dec1964</td>\n",
       "      <td>  ,   ...</td>\n",
       "      <td>     ...</td>\n",
       "      <td>tamil</td>\n",
       "      <td>   </td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            context  \\\n",
       "0  22bff3dec    (: 7  1983; , ...   \n",
       "1  282758170    (Google Maps) (   ...   \n",
       "2  d60987e0e     (   -  ...   \n",
       "3  f99c770dc   (: ;   ...   \n",
       "4  40dec1964    ,   ...   \n",
       "\n",
       "                                            question language  \\\n",
       "0                           hindi   \n",
       "1                         ?    hindi   \n",
       "2                        ?    hindi   \n",
       "3                          ?    tamil   \n",
       "4       ...    tamil   \n",
       "\n",
       "                      PredictionString  \n",
       "0                                   \n",
       "1                       20  2010  \n",
       "2                            \n",
       "3                                   13  \n",
       "4       "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e40f3eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T04:42:51.513214Z",
     "iopub.status.busy": "2021-10-08T04:42:51.512438Z",
     "iopub.status.idle": "2021-10-08T04:42:51.518396Z",
     "shell.execute_reply": "2021-10-08T04:42:51.517971Z",
     "shell.execute_reply.started": "2021-10-07T04:14:34.751196Z"
    },
    "papermill": {
     "duration": 0.048206,
     "end_time": "2021-10-08T04:42:51.518491",
     "exception": false,
     "start_time": "2021-10-08T04:42:51.470285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[['id','PredictionString']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fef8f",
   "metadata": {
    "papermill": {
     "duration": 0.037784,
     "end_time": "2021-10-08T04:42:51.594624",
     "exception": false,
     "start_time": "2021-10-08T04:42:51.556840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8eed9c",
   "metadata": {
    "papermill": {
     "duration": 0.037849,
     "end_time": "2021-10-08T04:42:51.670546",
     "exception": false,
     "start_time": "2021-10-08T04:42:51.632697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310107d7",
   "metadata": {
    "papermill": {
     "duration": 0.037887,
     "end_time": "2021-10-08T04:42:51.746450",
     "exception": false,
     "start_time": "2021-10-08T04:42:51.708563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 364.817222,
   "end_time": "2021-10-08T04:42:54.558267",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-08T04:36:49.741045",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0620c320707f4f7dab56d8b9d3c77d56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4dfb44f06ec94789a3429abc59a750c7",
       "placeholder": "",
       "style": "IPY_MODEL_d928b5322c39498d9d800d7a5e574308",
       "value": "&lt;tqdm.auto.tqdm object at 0x7fb377a34e50&gt;"
      }
     },
     "0b59b7cb9fd343eb98487914e96c36a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46183be15f0144bc9a82f8921a29a008": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4dfb44f06ec94789a3429abc59a750c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d9a2bc81b6e4b78bf17729797d13041": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_46183be15f0144bc9a82f8921a29a008",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_de1a023d988247baaee70fa9454c58e8",
       "value": 1.0
      }
     },
     "d8904123efb6420d91ded8a593e7894d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6d9a2bc81b6e4b78bf17729797d13041",
        "IPY_MODEL_0620c320707f4f7dab56d8b9d3c77d56"
       ],
       "layout": "IPY_MODEL_0b59b7cb9fd343eb98487914e96c36a8"
      }
     },
     "d928b5322c39498d9d800d7a5e574308": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "de1a023d988247baaee70fa9454c58e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
