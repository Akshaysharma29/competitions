{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heavy-passion",
   "metadata": {
    "papermill": {
     "duration": 0.017307,
     "end_time": "2021-08-01T06:44:58.845991",
     "exception": false,
     "start_time": "2021-08-01T06:44:58.828684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "This is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https://www.kaggle.com/maunish/clrp-roberta-base).\n",
    "\n",
    "Acknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bigger-source",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:44:58.883387Z",
     "iopub.status.busy": "2021-08-01T06:44:58.881892Z",
     "iopub.status.idle": "2021-08-01T06:45:44.511726Z",
     "shell.execute_reply": "2021-08-01T06:45:44.511082Z",
     "shell.execute_reply.started": "2021-07-30T08:41:34.856918Z"
    },
    "papermill": {
     "duration": 45.649704,
     "end_time": "2021-08-01T06:45:44.511942",
     "exception": false,
     "start_time": "2021-08-01T06:44:58.862238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in /kaggle/working/.git/\r\n",
      "Detected operating system as Ubuntu/bionic.\r\n",
      "Checking for curl...\r\n",
      "Detected curl...\r\n",
      "Checking for gpg...\r\n",
      "Detected gpg...\r\n",
      "Running apt-get update... done.\r\n",
      "Installing apt-transport-https... done.\r\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\r\n",
      "Importing packagecloud gpg key... done.\r\n",
      "Running apt-get update... done.\r\n",
      "\r\n",
      "The repository is setup! You can now install packages.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following NEW packages will be installed:\r\n",
      "  git-lfs\r\n",
      "0 upgraded, 1 newly installed, 0 to remove and 52 not upgraded.\r\n",
      "Need to get 2129 kB of archives.\r\n",
      "After this operation, 7662 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2129 kB]\r\n",
      "Fetched 2129 kB in 0s (10.2 MB/s)\r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "Selecting previously unselected package git-lfs.\r\n",
      "(Reading database ... 100757 files and directories currently installed.)\r\n",
      "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\r\n",
      "Unpacking git-lfs (2.3.4-1) ...\r\n",
      "Setting up git-lfs (2.3.4-1) ...\r\n",
      "Updated git hooks.\r\n",
      "Git LFS initialized.\r\n",
      "Cloning into 'roberta-base'...\r\n",
      "remote: Enumerating objects: 74, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (74/74), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\r\n",
      "remote: Total 74 (delta 32), reused 0 (delta 0)\u001b[K\r\n",
      "Unpacking objects: 100% (74/74), done.\r\n",
      "Filtering content: 100% (4/4), 2.15 GiB | 83.54 MiB/s, done.\r\n"
     ]
    }
   ],
   "source": [
    "!git init\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n",
    "!apt-get install git-lfs\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/roberta-base\n",
    "\n",
    "# !git clone https://huggingface.co/roberta-large\n",
    "# !git clone https://huggingface.co/facebook/bart-base\n",
    "# !git clone https://huggingface.co/bert-base-uncased\n",
    "# !git clone https://huggingface.co/microsoft/deberta-base\n",
    "# !git clone https://huggingface.co/distilroberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alert-master",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:01.867554Z",
     "iopub.status.busy": "2021-08-01T06:46:01.866401Z",
     "iopub.status.idle": "2021-08-01T06:46:11.105257Z",
     "shell.execute_reply": "2021-08-01T06:46:11.104218Z",
     "shell.execute_reply.started": "2021-07-30T08:43:02.217613Z"
    },
    "papermill": {
     "duration": 9.524527,
     "end_time": "2021-08-01T06:46:11.105408",
     "exception": false,
     "start_time": "2021-08-01T06:46:01.580881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impressed-court",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:11.209788Z",
     "iopub.status.busy": "2021-08-01T06:46:11.209225Z",
     "iopub.status.idle": "2021-08-01T06:46:11.213673Z",
     "shell.execute_reply": "2021-08-01T06:46:11.213218Z",
     "shell.execute_reply.started": "2021-07-30T08:43:11.497549Z"
    },
    "papermill": {
     "duration": 0.078356,
     "end_time": "2021-08-01T06:46:11.213802",
     "exception": false,
     "start_time": "2021-08-01T06:46:11.135446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_FOLDS = 6#10#9#8#7#6#5\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 16#24#32#16#1\n",
    "MAX_LEN = 300#248\n",
    "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "ROBERTA_PATH = \"../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin\"#\"../input/clrp-roberta-base/clrp_roberta_base\"\n",
    "TOKENIZER_PATH = \"./roberta-base\"#\"../input/clrp-roberta-base/clrp_roberta_base\"\n",
    "CONFIG_PATH = \"../input/commonlit-readability-prize-roberta-torch-itpt/output/config.json\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "killing-current",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:11.277123Z",
     "iopub.status.busy": "2021-08-01T06:46:11.276505Z",
     "iopub.status.idle": "2021-08-01T06:46:11.280485Z",
     "shell.execute_reply": "2021-08-01T06:46:11.280036Z",
     "shell.execute_reply.started": "2021-07-30T08:43:11.80933Z"
    },
    "papermill": {
     "duration": 0.037779,
     "end_time": "2021-08-01T06:46:11.280596",
     "exception": false,
     "start_time": "2021-08-01T06:46:11.242817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "usual-seller",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:11.345904Z",
     "iopub.status.busy": "2021-08-01T06:46:11.345345Z",
     "iopub.status.idle": "2021-08-01T06:46:11.464758Z",
     "shell.execute_reply": "2021-08-01T06:46:11.463862Z",
     "shell.execute_reply.started": "2021-07-30T08:43:14.841605Z"
    },
    "papermill": {
     "duration": 0.155238,
     "end_time": "2021-08-01T06:46:11.464901",
     "exception": false,
     "start_time": "2021-08-01T06:46:11.309663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n",
    "\n",
    "# Remove incomplete entries if any.\n",
    "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
    "              inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pleasant-philosophy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:11.526324Z",
     "iopub.status.busy": "2021-08-01T06:46:11.524860Z",
     "iopub.status.idle": "2021-08-01T06:46:11.734589Z",
     "shell.execute_reply": "2021-08-01T06:46:11.735812Z",
     "shell.execute_reply.started": "2021-07-30T08:43:15.147745Z"
    },
    "papermill": {
     "duration": 0.242591,
     "end_time": "2021-08-01T06:46:11.736091",
     "exception": false,
     "start_time": "2021-08-01T06:46:11.493500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-chapel",
   "metadata": {
    "papermill": {
     "duration": 0.079873,
     "end_time": "2021-08-01T06:46:11.871237",
     "exception": false,
     "start_time": "2021-08-01T06:46:11.791364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alpine-wallace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:12.061808Z",
     "iopub.status.busy": "2021-08-01T06:46:12.060954Z",
     "iopub.status.idle": "2021-08-01T06:46:12.067232Z",
     "shell.execute_reply": "2021-08-01T06:46:12.067857Z",
     "shell.execute_reply.started": "2021-07-30T08:43:16.796974Z"
    },
    "papermill": {
     "duration": 0.115135,
     "end_time": "2021-08-01T06:46:12.068102",
     "exception": false,
     "start_time": "2021-08-01T06:46:11.952967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
    "    \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-tunisia",
   "metadata": {
    "papermill": {
     "duration": 0.036259,
     "end_time": "2021-08-01T06:46:12.151904",
     "exception": false,
     "start_time": "2021-08-01T06:46:12.115645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surrounded-massage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:12.216904Z",
     "iopub.status.busy": "2021-08-01T06:46:12.215897Z",
     "iopub.status.idle": "2021-08-01T06:46:12.218614Z",
     "shell.execute_reply": "2021-08-01T06:46:12.218180Z",
     "shell.execute_reply.started": "2021-07-30T08:43:16.818616Z"
    },
    "papermill": {
     "duration": 0.036789,
     "end_time": "2021-08-01T06:46:12.218745",
     "exception": false,
     "start_time": "2021-08-01T06:46:12.181956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class LitModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         config = AutoConfig.from_pretrained(CONFIG_PATH)\n",
    "#         config.update({\"output_hidden_states\":True, \n",
    "#                        \"hidden_dropout_prob\": 0.0,\n",
    "# #                        \"attention_probs_dropout_prob\":0.0,\n",
    "#                        \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "#         self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "#         self.attention = nn.Sequential(            \n",
    "#             nn.Linear(768, 512),            \n",
    "#             nn.Tanh(),  \n",
    "#             nn.Linear(512, 1),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )        \n",
    "\n",
    "#         self.regressor = nn.Sequential(      \n",
    "# #             nn.LayerNorm(768),\n",
    "#             nn.Linear(768, 1),          \n",
    "#         )\n",
    "        \n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         roberta_output = self.roberta(input_ids=input_ids,\n",
    "#                                       attention_mask=attention_mask)        \n",
    "\n",
    "#         # There are a total of 13 layers of hidden states.\n",
    "#         # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "#         # We take the hidden states from the last Roberta layer.\n",
    "#         last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "#         # The number of cells is MAX_LEN.\n",
    "#         # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "#         # In order to condense hidden states of all cells to a context vector,\n",
    "#         # we compute a weighted average of the hidden states of all cells.\n",
    "#         # We compute the weight of each cell, using the attention neural network.\n",
    "#         weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "#         # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "#         # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "#         # Now we compute context_vector as the weighted average.\n",
    "#         # context_vector.shape is BATCH_SIZE x 768\n",
    "#         context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "#         # Now we reduce the context vector to the prediction score.\n",
    "#         return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "norman-commodity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:12.284542Z",
     "iopub.status.busy": "2021-08-01T06:46:12.283710Z",
     "iopub.status.idle": "2021-08-01T06:46:12.287538Z",
     "shell.execute_reply": "2021-08-01T06:46:12.287116Z",
     "shell.execute_reply.started": "2021-07-30T08:43:16.830893Z"
    },
    "papermill": {
     "duration": 0.038791,
     "end_time": "2021-08-01T06:46:12.287658",
     "exception": false,
     "start_time": "2021-08-01T06:46:12.248867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class LitModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         config = AutoConfig.from_pretrained(CONFIG_PATH)\n",
    "#         config.update({\"output_hidden_states\":True, \n",
    "#                        \"hidden_dropout_prob\": 0.0,\n",
    "# #                        \"attention_probs_dropout_prob\":0.0,\n",
    "#                        \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "#         self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "#         self.cnn1 = nn.Conv1d(768, MAX_LEN, kernel_size=2, padding=1)\n",
    "#         self.cnn2 = nn.Conv1d(MAX_LEN, 1, kernel_size=2, padding=1)\n",
    "\n",
    "        \n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         roberta_output = self.roberta(input_ids=input_ids,\n",
    "#                                       attention_mask=attention_mask)        \n",
    "#         last_hidden_state = roberta_output[0]\n",
    "#         last_hidden_state = last_hidden_state.permute(0, 2, 1)\n",
    "#         cnn_embeddings = F.relu(self.cnn1(last_hidden_state))\n",
    "#         cnn_embeddings = self.cnn2(cnn_embeddings)\n",
    "#         logits, _ = torch.max(cnn_embeddings, 2)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "elegant-forth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:12.363301Z",
     "iopub.status.busy": "2021-08-01T06:46:12.362668Z",
     "iopub.status.idle": "2021-08-01T06:46:12.365709Z",
     "shell.execute_reply": "2021-08-01T06:46:12.366156Z",
     "shell.execute_reply.started": "2021-07-30T08:46:28.73768Z"
    },
    "papermill": {
     "duration": 0.047268,
     "end_time": "2021-08-01T06:46:12.366298",
     "exception": false,
     "start_time": "2021-08-01T06:46:12.319030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/2103.04083v1.pdf\n",
    "class LitModel(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CONFIG_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "#                        \"attention_probs_dropout_prob\":0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "#         self.cnn1 = nn.Conv1d(768, MAX_LEN, kernel_size=1)\n",
    "#         self.cnn2 = nn.Conv1d(MAX_LEN, 1, kernel_size=1)\n",
    "        self.cnn1 = nn.Conv1d(768, 512, kernel_size=1)\n",
    "        self.cnn2 = nn.Conv1d(512, MAX_LEN, kernel_size=1)\n",
    "         \n",
    "#         self.layernorm = nn.LayerNorm(MAX_LEN,MAX_LEN)    \n",
    "        self.layernorm = nn.LayerNorm(MAX_LEN)\n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(MAX_LEN, MAX_LEN),            \n",
    "            nn.Tanh(),  \n",
    "            nn.Linear(MAX_LEN, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(      \n",
    "#             nn.LayerNorm(768),\n",
    "            nn.Linear(MAX_LEN, 1),          \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)   \n",
    "        last_hidden_state = roberta_output.hidden_states[-1]\n",
    "#         print(last_hidden_state.shape)\n",
    "        last_hidden_state = last_hidden_state.permute(0, 2, 1)#16*768*MAX_LEN\n",
    "#         print(last_hidden_state.shape)\n",
    "        cnn_embeddings = F.relu(self.cnn1(last_hidden_state))#16*512*MAX_LEN\n",
    "#         print(cnn_embeddings.shape)\n",
    "        cnn_embeddings = self.cnn2(cnn_embeddings)#16*MAX_LEN(embedding)*MAX_LEN(tokens)\n",
    "#         print(cnn_embeddings.shape)\n",
    "        cnn_embeddings = cnn_embeddings.permute(0, 2, 1)\n",
    "#         cnn_embeddings = self.layernorm(cnn_embeddings)\n",
    "#         print(cnn_embeddings.shape)\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "#         last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "        \n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "#         print(cnn_embeddings.shape)\n",
    "        weights = self.attention(cnn_embeddings)#16*MAX_LEN*1\n",
    "#         print('weights.shape',weights.shape)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * cnn_embeddings, dim=1)#16*MAX_LEN   \n",
    "#         print('context_vector',context_vector.shape)\n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)#16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efficient-canvas",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:12.434237Z",
     "iopub.status.busy": "2021-08-01T06:46:12.432702Z",
     "iopub.status.idle": "2021-08-01T06:46:12.435225Z",
     "shell.execute_reply": "2021-08-01T06:46:12.435850Z",
     "shell.execute_reply.started": "2021-07-30T08:46:29.025604Z"
    },
    "papermill": {
     "duration": 0.039891,
     "end_time": "2021-08-01T06:46:12.436077",
     "exception": false,
     "start_time": "2021-08-01T06:46:12.396186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()            \n",
    "    mse_sum = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)                        \n",
    "            target = target.to(DEVICE)           \n",
    "            \n",
    "            pred = model(input_ids, attention_mask)                       \n",
    "\n",
    "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n",
    "                \n",
    "\n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "elder-flush",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:12.504268Z",
     "iopub.status.busy": "2021-08-01T06:46:12.502732Z",
     "iopub.status.idle": "2021-08-01T06:46:12.505200Z",
     "shell.execute_reply": "2021-08-01T06:46:12.505688Z",
     "shell.execute_reply.started": "2021-07-30T08:46:29.421724Z"
    },
    "papermill": {
     "duration": 0.039169,
     "end_time": "2021-08-01T06:46:12.505826",
     "exception": false,
     "start_time": "2021-08-01T06:46:12.466657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dynamic-function",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:12.576553Z",
     "iopub.status.busy": "2021-08-01T06:46:12.575775Z",
     "iopub.status.idle": "2021-08-01T06:46:12.579036Z",
     "shell.execute_reply": "2021-08-01T06:46:12.579417Z",
     "shell.execute_reply.started": "2021-07-30T08:46:29.57441Z"
    },
    "papermill": {
     "duration": 0.04431,
     "end_time": "2021-08-01T06:46:12.579572",
     "exception": false,
     "start_time": "2021-08-01T06:46:12.535262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, model_path, train_loader, val_loader,\n",
    "          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n",
    "    best_val_rmse = None\n",
    "    best_epoch = 0\n",
    "    step = 0\n",
    "    last_eval_step = 0\n",
    "    eval_period = EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):                           \n",
    "        val_rmse = None         \n",
    "\n",
    "        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)            \n",
    "            target = target.to(DEVICE)                        \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "            pred = model(input_ids, attention_mask)\n",
    "                                                        \n",
    "            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n",
    "                        \n",
    "            mse.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            if step >= last_eval_step + eval_period:\n",
    "                # Evaluate the model on val_loader.\n",
    "                elapsed_seconds = time.time() - start\n",
    "                num_steps = step - last_eval_step\n",
    "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                last_eval_step = step\n",
    "                \n",
    "                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n",
    "\n",
    "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
    "                      f\"val_rmse: {val_rmse:0.4}\")\n",
    "\n",
    "                for rmse, period in EVAL_SCHEDULE:\n",
    "                    if val_rmse >= rmse:\n",
    "                        eval_period = period\n",
    "                        break                               \n",
    "                \n",
    "                if not best_val_rmse or val_rmse < best_val_rmse:                    \n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_epoch = epoch\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "                else:       \n",
    "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "                          f\"(from epoch {best_epoch})\")                                    \n",
    "                    \n",
    "                start = time.time()\n",
    "                                            \n",
    "            step += 1\n",
    "                        \n",
    "    \n",
    "    return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "quantitative-advancement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:12.646413Z",
     "iopub.status.busy": "2021-08-01T06:46:12.645527Z",
     "iopub.status.idle": "2021-08-01T06:46:12.647899Z",
     "shell.execute_reply": "2021-08-01T06:46:12.648291Z",
     "shell.execute_reply.started": "2021-07-30T08:46:29.771045Z"
    },
    "papermill": {
     "duration": 0.039758,
     "end_time": "2021-08-01T06:46:12.648448",
     "exception": false,
     "start_time": "2021-08-01T06:46:12.608690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    \n",
    "    roberta_parameters = named_parameters[:197]    \n",
    "    attention_parameters = named_parameters[199:203]\n",
    "    regressor_parameters = named_parameters[203:]\n",
    "        \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "\n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group,\n",
    "                       \"weight_decay\": 0.001,\n",
    "                      \"lr\": 1e-3})\n",
    "    parameters.append({\"params\": regressor_group,\n",
    "                       \"weight_decay\": 0.001,\n",
    "                      \"lr\": 1e-3})\n",
    "\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "\n",
    "        lr = 2e-5\n",
    "\n",
    "        if layer_num >= 69:        \n",
    "            lr = 5e-5\n",
    "\n",
    "        if layer_num >= 133:\n",
    "            lr = 1e-4\n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "\n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "advance-australia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:46:12.887226Z",
     "iopub.status.busy": "2021-08-01T06:46:12.886197Z",
     "iopub.status.idle": "2021-08-01T07:45:00.817169Z",
     "shell.execute_reply": "2021-08-01T07:45:00.817657Z"
    },
    "papermill": {
     "duration": 3528.139676,
     "end_time": "2021-08-01T07:45:00.817823",
     "exception": false,
     "start_time": "2021-08-01T06:46:12.678147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.94 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9289\n",
      "New best_val_rmse: 0.9289\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7253\n",
      "New best_val_rmse: 0.7253\n",
      "\n",
      "16 steps took 8.31 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6896\n",
      "New best_val_rmse: 0.6896\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6827\n",
      "New best_val_rmse: 0.6827\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6979\n",
      "Still best_val_rmse: 0.6827 (from epoch 0)\n",
      "\n",
      "16 steps took 8.18 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5887\n",
      "New best_val_rmse: 0.5887\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5648\n",
      "New best_val_rmse: 0.5648\n",
      "\n",
      "16 steps took 8.25 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5512\n",
      "New best_val_rmse: 0.5512\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5338\n",
      "New best_val_rmse: 0.5338\n",
      "\n",
      "16 steps took 8.36 seconds\n",
      "Epoch: 1 batch_num: 13 val_rmse: 0.5995\n",
      "Still best_val_rmse: 0.5338 (from epoch 0)\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 1 batch_num: 29 val_rmse: 0.5501\n",
      "Still best_val_rmse: 0.5338 (from epoch 0)\n",
      "\n",
      "16 steps took 8.27 seconds\n",
      "Epoch: 1 batch_num: 45 val_rmse: 0.5193\n",
      "New best_val_rmse: 0.5193\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 1 batch_num: 61 val_rmse: 0.5133\n",
      "New best_val_rmse: 0.5133\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 1 batch_num: 77 val_rmse: 0.5494\n",
      "Still best_val_rmse: 0.5133 (from epoch 1)\n",
      "\n",
      "16 steps took 8.34 seconds\n",
      "Epoch: 1 batch_num: 93 val_rmse: 0.4988\n",
      "New best_val_rmse: 0.4988\n",
      "\n",
      "8 steps took 4.12 seconds\n",
      "Epoch: 1 batch_num: 101 val_rmse: 0.5754\n",
      "Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 1 batch_num: 117 val_rmse: 0.5224\n",
      "Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 1 batch_num: 133 val_rmse: 0.5239\n",
      "Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 8.34 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4965\n",
      "New best_val_rmse: 0.4965\n",
      "\n",
      "8 steps took 4.18 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.5122\n",
      "Still best_val_rmse: 0.4965 (from epoch 2)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.5161\n",
      "Still best_val_rmse: 0.4965 (from epoch 2)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.5231\n",
      "Still best_val_rmse: 0.4965 (from epoch 2)\n",
      "\n",
      "16 steps took 8.18 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.499\n",
      "Still best_val_rmse: 0.4965 (from epoch 2)\n",
      "\n",
      "8 steps took 4.11 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.5014\n",
      "Still best_val_rmse: 0.4965 (from epoch 2)\n",
      "\n",
      "16 steps took 8.3 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.5007\n",
      "Still best_val_rmse: 0.4965 (from epoch 2)\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.494\n",
      "New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 4.12 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.496\n",
      "Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 4.18 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4986\n",
      "Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 4.12 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.5012\n",
      "Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.5003\n",
      "Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.49403845250320266]\n",
      "Mean: 0.49403845250320266\n",
      "\n",
      "Fold 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.82 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9211\n",
      "New best_val_rmse: 0.9211\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6967\n",
      "New best_val_rmse: 0.6967\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6419\n",
      "New best_val_rmse: 0.6419\n",
      "\n",
      "16 steps took 8.27 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6455\n",
      "Still best_val_rmse: 0.6419 (from epoch 0)\n",
      "\n",
      "16 steps took 8.32 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6516\n",
      "Still best_val_rmse: 0.6419 (from epoch 0)\n",
      "\n",
      "16 steps took 8.16 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6302\n",
      "New best_val_rmse: 0.6302\n",
      "\n",
      "16 steps took 8.35 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5903\n",
      "New best_val_rmse: 0.5903\n",
      "\n",
      "16 steps took 8.26 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5599\n",
      "New best_val_rmse: 0.5599\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5026\n",
      "New best_val_rmse: 0.5026\n",
      "\n",
      "16 steps took 8.39 seconds\n",
      "Epoch: 1 batch_num: 13 val_rmse: 0.5309\n",
      "Still best_val_rmse: 0.5026 (from epoch 0)\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 1 batch_num: 29 val_rmse: 0.5059\n",
      "Still best_val_rmse: 0.5026 (from epoch 0)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 1 batch_num: 45 val_rmse: 0.4906\n",
      "New best_val_rmse: 0.4906\n",
      "\n",
      "8 steps took 4.13 seconds\n",
      "Epoch: 1 batch_num: 53 val_rmse: 0.4823\n",
      "New best_val_rmse: 0.4823\n",
      "\n",
      "4 steps took 2.06 seconds\n",
      "Epoch: 1 batch_num: 57 val_rmse: 0.4753\n",
      "New best_val_rmse: 0.4753\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 1 batch_num: 59 val_rmse: 0.5031\n",
      "Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 1 batch_num: 75 val_rmse: 0.4775\n",
      "Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 1 batch_num: 77 val_rmse: 0.4825\n",
      "Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "4 steps took 2.06 seconds\n",
      "Epoch: 1 batch_num: 81 val_rmse: 0.4825\n",
      "Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 1 batch_num: 85 val_rmse: 0.4942\n",
      "Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "8 steps took 4.11 seconds\n",
      "Epoch: 1 batch_num: 93 val_rmse: 0.4862\n",
      "Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 1 batch_num: 97 val_rmse: 0.5222\n",
      "Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 1 batch_num: 113 val_rmse: 0.4925\n",
      "Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "8 steps took 4.13 seconds\n",
      "Epoch: 1 batch_num: 121 val_rmse: 0.4739\n",
      "New best_val_rmse: 0.4739\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 1 batch_num: 123 val_rmse: 0.4667\n",
      "New best_val_rmse: 0.4667\n",
      "\n",
      "1 steps took 0.528 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4738\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "2 steps took 1.05 seconds\n",
      "Epoch: 1 batch_num: 126 val_rmse: 0.5138\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4839\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4727\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "2 steps took 1.17 seconds\n",
      "Epoch: 2 batch_num: 1 val_rmse: 0.4715\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "2 steps took 1.05 seconds\n",
      "Epoch: 2 batch_num: 3 val_rmse: 0.4705\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 5 val_rmse: 0.4686\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "1 steps took 0.517 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4699\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "1 steps took 0.514 seconds\n",
      "Epoch: 2 batch_num: 7 val_rmse: 0.4717\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 9 val_rmse: 0.4772\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "2 steps took 1.06 seconds\n",
      "Epoch: 2 batch_num: 11 val_rmse: 0.4847\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 15 val_rmse: 0.4978\n",
      "Still best_val_rmse: 0.4667 (from epoch 1)\n",
      "\n",
      "8 steps took 4.11 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4639\n",
      "New best_val_rmse: 0.4639\n",
      "\n",
      "1 steps took 0.542 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4639\n",
      "New best_val_rmse: 0.4639\n",
      "\n",
      "1 steps took 0.517 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4669\n",
      "Still best_val_rmse: 0.4639 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4736\n",
      "Still best_val_rmse: 0.4639 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4729\n",
      "Still best_val_rmse: 0.4639 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4673\n",
      "Still best_val_rmse: 0.4639 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4636\n",
      "New best_val_rmse: 0.4636\n",
      "\n",
      "1 steps took 0.516 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.462\n",
      "New best_val_rmse: 0.462\n",
      "\n",
      "1 steps took 0.563 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4623\n",
      "Still best_val_rmse: 0.462 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4628\n",
      "Still best_val_rmse: 0.462 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.461\n",
      "New best_val_rmse: 0.461\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4599\n",
      "New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.527 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4585\n",
      "New best_val_rmse: 0.4585\n",
      "\n",
      "1 steps took 0.519 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4576\n",
      "New best_val_rmse: 0.4576\n",
      "\n",
      "1 steps took 0.547 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4593\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.518 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4644\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.53 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4685\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.514 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4712\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "2 steps took 1.06 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4675\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4651\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.518 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4615\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.459\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4581\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4578\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.458\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4583\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4587\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4591\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.526 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4594\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.459\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4585\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.516 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4585\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.525 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4586\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.514 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4592\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.523 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4598\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4603\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4606\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4618\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4627\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4638\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4661\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4673\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.517 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4683\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.514 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4677\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.517 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.538 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4666\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4656\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4643\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4627\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4616\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4607\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.523 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4598\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4589\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.519 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4582\n",
      "Still best_val_rmse: 0.4576 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4575\n",
      "New best_val_rmse: 0.4575\n",
      "\n",
      "1 steps took 0.537 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4572\n",
      "New best_val_rmse: 0.4572\n",
      "\n",
      "1 steps took 0.542 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4569\n",
      "New best_val_rmse: 0.4569\n",
      "\n",
      "1 steps took 0.523 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4567\n",
      "New best_val_rmse: 0.4567\n",
      "\n",
      "1 steps took 0.521 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4566\n",
      "New best_val_rmse: 0.4566\n",
      "\n",
      "1 steps took 0.514 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4561\n",
      "New best_val_rmse: 0.4561\n",
      "\n",
      "1 steps took 0.534 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4561\n",
      "New best_val_rmse: 0.4561\n",
      "\n",
      "1 steps took 0.52 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4558\n",
      "New best_val_rmse: 0.4558\n",
      "\n",
      "1 steps took 0.522 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4556\n",
      "New best_val_rmse: 0.4556\n",
      "\n",
      "1 steps took 0.528 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4554\n",
      "New best_val_rmse: 0.4554\n",
      "\n",
      "1 steps took 0.531 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4553\n",
      "New best_val_rmse: 0.4553\n",
      "\n",
      "1 steps took 0.544 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4554\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4554\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4555\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4558\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.52 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4561\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4561\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.456\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4559\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4558\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4559\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.456\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.514 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4561\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4562\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4565\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.519 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4569\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4574\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.516 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.458\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.514 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4584\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4587\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4589\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4586\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4583\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4581\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.529 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4578\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4575\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.519 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4574\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.518 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4573\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.519 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4571\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.526 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.457\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4569\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4568\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4568\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4568\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.513 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4569\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4569\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.517 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4568\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.514 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4567\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4567\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4566\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4566\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4566\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4566\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.523 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4566\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4566\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.532 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4566\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.524 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4566\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4566\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.52 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4565\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.516 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4565\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4565\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.515 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4565\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.512 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4565\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.51 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4565\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.509 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4565\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "1 steps took 0.511 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4565\n",
      "Still best_val_rmse: 0.4553 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.49403845250320266, 0.455335878865602]\n",
      "Mean: 0.47468716568440233\n",
      "\n",
      "Fold 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.82 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.944\n",
      "New best_val_rmse: 0.944\n",
      "\n",
      "16 steps took 8.25 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7514\n",
      "New best_val_rmse: 0.7514\n",
      "\n",
      "16 steps took 8.26 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9206\n",
      "Still best_val_rmse: 0.7514 (from epoch 0)\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8612\n",
      "Still best_val_rmse: 0.7514 (from epoch 0)\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6576\n",
      "New best_val_rmse: 0.6576\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5836\n",
      "New best_val_rmse: 0.5836\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5441\n",
      "New best_val_rmse: 0.5441\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5801\n",
      "Still best_val_rmse: 0.5441 (from epoch 0)\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7879\n",
      "Still best_val_rmse: 0.5441 (from epoch 0)\n",
      "\n",
      "16 steps took 8.43 seconds\n",
      "Epoch: 1 batch_num: 13 val_rmse: 0.7212\n",
      "Still best_val_rmse: 0.5441 (from epoch 0)\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 1 batch_num: 29 val_rmse: 0.5733\n",
      "Still best_val_rmse: 0.5441 (from epoch 0)\n",
      "\n",
      "16 steps took 8.18 seconds\n",
      "Epoch: 1 batch_num: 45 val_rmse: 0.5321\n",
      "New best_val_rmse: 0.5321\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 1 batch_num: 61 val_rmse: 0.5476\n",
      "Still best_val_rmse: 0.5321 (from epoch 1)\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 1 batch_num: 77 val_rmse: 0.5555\n",
      "Still best_val_rmse: 0.5321 (from epoch 1)\n",
      "\n",
      "16 steps took 8.29 seconds\n",
      "Epoch: 1 batch_num: 93 val_rmse: 0.5057\n",
      "New best_val_rmse: 0.5057\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 1 batch_num: 109 val_rmse: 0.5145\n",
      "Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 1 batch_num: 125 val_rmse: 0.5119\n",
      "Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 1 batch_num: 141 val_rmse: 0.4833\n",
      "New best_val_rmse: 0.4833\n",
      "\n",
      "4 steps took 2.06 seconds\n",
      "Epoch: 1 batch_num: 145 val_rmse: 0.4842\n",
      "Still best_val_rmse: 0.4833 (from epoch 1)\n",
      "\n",
      "4 steps took 2.23 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.488\n",
      "Still best_val_rmse: 0.4833 (from epoch 1)\n",
      "\n",
      "4 steps took 2.08 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4878\n",
      "Still best_val_rmse: 0.4833 (from epoch 1)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4977\n",
      "Still best_val_rmse: 0.4833 (from epoch 1)\n",
      "\n",
      "8 steps took 4.09 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4929\n",
      "Still best_val_rmse: 0.4833 (from epoch 1)\n",
      "\n",
      "8 steps took 4.13 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4882\n",
      "Still best_val_rmse: 0.4833 (from epoch 1)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4796\n",
      "New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4772\n",
      "New best_val_rmse: 0.4772\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4779\n",
      "Still best_val_rmse: 0.4772 (from epoch 2)\n",
      "\n",
      "2 steps took 1.04 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4788\n",
      "Still best_val_rmse: 0.4772 (from epoch 2)\n",
      "\n",
      "2 steps took 1.06 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4771\n",
      "New best_val_rmse: 0.4771\n",
      "\n",
      "2 steps took 1.04 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4768\n",
      "New best_val_rmse: 0.4768\n",
      "\n",
      "2 steps took 1.05 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4771\n",
      "Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4772\n",
      "Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4762\n",
      "New best_val_rmse: 0.4762\n",
      "\n",
      "2 steps took 1.06 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4761\n",
      "New best_val_rmse: 0.4761\n",
      "\n",
      "2 steps took 1.04 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4767\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4773\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.05 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4829\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4886\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 2.06 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.05 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4773\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4769\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4767\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.477\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4778\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4797\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4821\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4822\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4791\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4793\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4798\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4806\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 2.11 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4832\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4814\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4792\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.04 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4785\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4777\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4772\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4767\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4763\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4761\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.09 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4761\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4761\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4761\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4761\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.06 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.03 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.02 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.49403845250320266, 0.455335878865602, 0.4760558258840567]\n",
      "Mean: 0.4751433857509538\n",
      "\n",
      "Fold 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.88 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9605\n",
      "New best_val_rmse: 0.9605\n",
      "\n",
      "16 steps took 8.17 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8249\n",
      "New best_val_rmse: 0.8249\n",
      "\n",
      "16 steps took 8.25 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6208\n",
      "New best_val_rmse: 0.6208\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.613\n",
      "New best_val_rmse: 0.613\n",
      "\n",
      "16 steps took 8.29 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.647\n",
      "Still best_val_rmse: 0.613 (from epoch 0)\n",
      "\n",
      "16 steps took 8.26 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.626\n",
      "Still best_val_rmse: 0.613 (from epoch 0)\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6757\n",
      "Still best_val_rmse: 0.613 (from epoch 0)\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5936\n",
      "New best_val_rmse: 0.5936\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6632\n",
      "Still best_val_rmse: 0.5936 (from epoch 0)\n",
      "\n",
      "16 steps took 8.45 seconds\n",
      "Epoch: 1 batch_num: 13 val_rmse: 0.5341\n",
      "New best_val_rmse: 0.5341\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 1 batch_num: 29 val_rmse: 0.569\n",
      "Still best_val_rmse: 0.5341 (from epoch 1)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 1 batch_num: 45 val_rmse: 0.5113\n",
      "New best_val_rmse: 0.5113\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 1 batch_num: 61 val_rmse: 0.533\n",
      "Still best_val_rmse: 0.5113 (from epoch 1)\n",
      "\n",
      "16 steps took 8.25 seconds\n",
      "Epoch: 1 batch_num: 77 val_rmse: 0.5178\n",
      "Still best_val_rmse: 0.5113 (from epoch 1)\n",
      "\n",
      "16 steps took 8.26 seconds\n",
      "Epoch: 1 batch_num: 93 val_rmse: 0.5427\n",
      "Still best_val_rmse: 0.5113 (from epoch 1)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 1 batch_num: 109 val_rmse: 0.4981\n",
      "New best_val_rmse: 0.4981\n",
      "\n",
      "8 steps took 4.14 seconds\n",
      "Epoch: 1 batch_num: 117 val_rmse: 0.5093\n",
      "Still best_val_rmse: 0.4981 (from epoch 1)\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 1 batch_num: 133 val_rmse: 0.5058\n",
      "Still best_val_rmse: 0.4981 (from epoch 1)\n",
      "\n",
      "16 steps took 8.38 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.5569\n",
      "Still best_val_rmse: 0.4981 (from epoch 1)\n",
      "\n",
      "16 steps took 8.28 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4923\n",
      "New best_val_rmse: 0.4923\n",
      "\n",
      "8 steps took 4.09 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.5014\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.5023\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4951\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 4.13 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4943\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 4.11 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4942\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 4.12 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4983\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 4.1 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.5002\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.493\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 4.11 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4927\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 4.12 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4929\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 4.1 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4932\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 4.19 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4933\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 4.1 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4933\n",
      "Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.49403845250320266, 0.455335878865602, 0.4760558258840567, 0.4923373777463265]\n",
      "Mean: 0.47944188374979696\n",
      "\n",
      "Fold 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.89 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9672\n",
      "New best_val_rmse: 0.9672\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6773\n",
      "New best_val_rmse: 0.6773\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6504\n",
      "New best_val_rmse: 0.6504\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6742\n",
      "Still best_val_rmse: 0.6504 (from epoch 0)\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7225\n",
      "Still best_val_rmse: 0.6504 (from epoch 0)\n",
      "\n",
      "16 steps took 8.32 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5714\n",
      "New best_val_rmse: 0.5714\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5638\n",
      "New best_val_rmse: 0.5638\n",
      "\n",
      "16 steps took 8.3 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6124\n",
      "Still best_val_rmse: 0.5638 (from epoch 0)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7455\n",
      "Still best_val_rmse: 0.5638 (from epoch 0)\n",
      "\n",
      "16 steps took 8.43 seconds\n",
      "Epoch: 1 batch_num: 13 val_rmse: 0.5442\n",
      "New best_val_rmse: 0.5442\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 1 batch_num: 29 val_rmse: 0.5132\n",
      "New best_val_rmse: 0.5132\n",
      "\n",
      "16 steps took 8.25 seconds\n",
      "Epoch: 1 batch_num: 45 val_rmse: 0.5171\n",
      "Still best_val_rmse: 0.5132 (from epoch 1)\n",
      "\n",
      "16 steps took 8.26 seconds\n",
      "Epoch: 1 batch_num: 61 val_rmse: 0.5871\n",
      "Still best_val_rmse: 0.5132 (from epoch 1)\n",
      "\n",
      "16 steps took 8.17 seconds\n",
      "Epoch: 1 batch_num: 77 val_rmse: 0.5064\n",
      "New best_val_rmse: 0.5064\n",
      "\n",
      "16 steps took 8.32 seconds\n",
      "Epoch: 1 batch_num: 93 val_rmse: 0.5407\n",
      "Still best_val_rmse: 0.5064 (from epoch 1)\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 1 batch_num: 109 val_rmse: 0.5253\n",
      "Still best_val_rmse: 0.5064 (from epoch 1)\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 1 batch_num: 125 val_rmse: 0.5113\n",
      "Still best_val_rmse: 0.5064 (from epoch 1)\n",
      "\n",
      "16 steps took 8.21 seconds\n",
      "Epoch: 1 batch_num: 141 val_rmse: 0.5765\n",
      "Still best_val_rmse: 0.5064 (from epoch 1)\n",
      "\n",
      "16 steps took 8.36 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.5165\n",
      "Still best_val_rmse: 0.5064 (from epoch 1)\n",
      "\n",
      "16 steps took 8.34 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4969\n",
      "New best_val_rmse: 0.4969\n",
      "\n",
      "8 steps took 4.14 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4901\n",
      "New best_val_rmse: 0.4901\n",
      "\n",
      "8 steps took 4.19 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4942\n",
      "Still best_val_rmse: 0.4901 (from epoch 2)\n",
      "\n",
      "8 steps took 4.16 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.5006\n",
      "Still best_val_rmse: 0.4901 (from epoch 2)\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4924\n",
      "Still best_val_rmse: 0.4901 (from epoch 2)\n",
      "\n",
      "8 steps took 4.13 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4925\n",
      "Still best_val_rmse: 0.4901 (from epoch 2)\n",
      "\n",
      "8 steps took 4.17 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4923\n",
      "Still best_val_rmse: 0.4901 (from epoch 2)\n",
      "\n",
      "8 steps took 4.1 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.5012\n",
      "Still best_val_rmse: 0.4901 (from epoch 2)\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.49\n",
      "New best_val_rmse: 0.49\n",
      "\n",
      "4 steps took 2.15 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.49\n",
      "Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.11 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4917\n",
      "Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.09 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.493\n",
      "Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.12 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4927\n",
      "Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.09 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4927\n",
      "Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.49403845250320266, 0.455335878865602, 0.4760558258840567, 0.4923373777463265, 0.4899736466218763]\n",
      "Mean: 0.48154823632421284\n",
      "\n",
      "Fold 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.85 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9248\n",
      "New best_val_rmse: 0.9248\n",
      "\n",
      "16 steps took 8.28 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7227\n",
      "New best_val_rmse: 0.7227\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8201\n",
      "Still best_val_rmse: 0.7227 (from epoch 0)\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8578\n",
      "Still best_val_rmse: 0.7227 (from epoch 0)\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6491\n",
      "New best_val_rmse: 0.6491\n",
      "\n",
      "16 steps took 8.29 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6223\n",
      "New best_val_rmse: 0.6223\n",
      "\n",
      "16 steps took 8.23 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.564\n",
      "New best_val_rmse: 0.564\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6135\n",
      "Still best_val_rmse: 0.564 (from epoch 0)\n",
      "\n",
      "16 steps took 8.18 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6076\n",
      "Still best_val_rmse: 0.564 (from epoch 0)\n",
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 1 batch_num: 13 val_rmse: 0.6053\n",
      "Still best_val_rmse: 0.564 (from epoch 0)\n",
      "\n",
      "16 steps took 8.32 seconds\n",
      "Epoch: 1 batch_num: 29 val_rmse: 0.5222\n",
      "New best_val_rmse: 0.5222\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 1 batch_num: 45 val_rmse: 0.5564\n",
      "Still best_val_rmse: 0.5222 (from epoch 1)\n",
      "\n",
      "16 steps took 8.35 seconds\n",
      "Epoch: 1 batch_num: 61 val_rmse: 0.5364\n",
      "Still best_val_rmse: 0.5222 (from epoch 1)\n",
      "\n",
      "16 steps took 8.27 seconds\n",
      "Epoch: 1 batch_num: 77 val_rmse: 0.5142\n",
      "New best_val_rmse: 0.5142\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 1 batch_num: 93 val_rmse: 0.4953\n",
      "New best_val_rmse: 0.4953\n",
      "\n",
      "8 steps took 4.1 seconds\n",
      "Epoch: 1 batch_num: 101 val_rmse: 0.4894\n",
      "New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 2.08 seconds\n",
      "Epoch: 1 batch_num: 105 val_rmse: 0.5001\n",
      "Still best_val_rmse: 0.4894 (from epoch 1)\n",
      "\n",
      "16 steps took 8.24 seconds\n",
      "Epoch: 1 batch_num: 121 val_rmse: 0.5074\n",
      "Still best_val_rmse: 0.4894 (from epoch 1)\n",
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 1 batch_num: 137 val_rmse: 0.4981\n",
      "Still best_val_rmse: 0.4894 (from epoch 1)\n",
      "\n",
      "8 steps took 4.09 seconds\n",
      "Epoch: 1 batch_num: 145 val_rmse: 0.5056\n",
      "Still best_val_rmse: 0.4894 (from epoch 1)\n",
      "\n",
      "16 steps took 8.42 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4979\n",
      "Still best_val_rmse: 0.4894 (from epoch 1)\n",
      "\n",
      "8 steps took 4.09 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4846\n",
      "New best_val_rmse: 0.4846\n",
      "\n",
      "4 steps took 2.08 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4877\n",
      "Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 2.06 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4889\n",
      "Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 2.11 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4932\n",
      "Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "8 steps took 4.11 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.5127\n",
      "Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "16 steps took 8.2 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4863\n",
      "Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.483\n",
      "New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 2.08 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.5154\n",
      "Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "16 steps took 8.19 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4868\n",
      "Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.06 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4853\n",
      "Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.13 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4857\n",
      "Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4871\n",
      "Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4848\n",
      "Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.07 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4831\n",
      "Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4827\n",
      "New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 2.06 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4832\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4833\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4831\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.08 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4833\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4833\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4838\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.484\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.05 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4841\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.08 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4842\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.04 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4842\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.49403845250320266, 0.455335878865602, 0.4760558258840567, 0.4923373777463265, 0.4899736466218763, 0.4827286498693235]\n",
      "Mean: 0.4817449719150646\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "SEED = 1000\n",
    "list_val_rmse = []\n",
    "\n",
    "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "    model_path = f\"model_{fold + 1}.pth\"\n",
    "        \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    train_dataset = LitDataset(train_df.loc[train_indices])    \n",
    "    val_dataset = LitDataset(train_df.loc[val_indices])    \n",
    "        \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              drop_last=True, shuffle=True, num_workers=2)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=2)    \n",
    "        \n",
    "    set_random_seed(SEED + fold)    \n",
    "    \n",
    "    model = LitModel().to(DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model)                        \n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_training_steps=NUM_EPOCHS * len(train_loader),\n",
    "        num_warmup_steps=50)    \n",
    "    \n",
    "    list_val_rmse.append(train(model, model_path, train_loader,\n",
    "                               val_loader, optimizer, scheduler=scheduler))\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\nPerformance estimates:\")\n",
    "    print(list_val_rmse)\n",
    "    print(\"Mean:\", np.array(list_val_rmse).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-examination",
   "metadata": {
    "papermill": {
     "duration": 0.239571,
     "end_time": "2021-08-01T07:45:01.297037",
     "exception": false,
     "start_time": "2021-08-01T07:45:01.057466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "recreational-brunei",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:45:01.790698Z",
     "iopub.status.busy": "2021-08-01T07:45:01.789868Z",
     "iopub.status.idle": "2021-08-01T07:45:01.814075Z",
     "shell.execute_reply": "2021-08-01T07:45:01.813599Z"
    },
    "papermill": {
     "duration": 0.278035,
     "end_time": "2021-08-01T07:45:01.814200",
     "exception": false,
     "start_time": "2021-08-01T07:45:01.536165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = LitDataset(test_df, inference_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "roman-cherry",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:45:02.308721Z",
     "iopub.status.busy": "2021-08-01T07:45:02.307892Z",
     "iopub.status.idle": "2021-08-01T07:45:32.492518Z",
     "shell.execute_reply": "2021-08-01T07:45:32.493481Z"
    },
    "papermill": {
     "duration": 30.436625,
     "end_time": "2021-08-01T07:45:32.493665",
     "exception": false,
     "start_time": "2021-08-01T07:45:02.057040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/commonlit-readability-prize-roberta-torch-itpt/output/pytorch_model.bin and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for index in range(len(list_val_rmse)):            \n",
    "    model_path = f\"model_{index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path))    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    all_predictions[index] = predict(model, test_loader)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "outside-living",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:45:33.226506Z",
     "iopub.status.busy": "2021-08-01T07:45:33.225852Z",
     "iopub.status.idle": "2021-08-01T07:45:33.663765Z",
     "shell.execute_reply": "2021-08-01T07:45:33.662794Z"
    },
    "papermill": {
     "duration": 0.718945,
     "end_time": "2021-08-01T07:45:33.663906",
     "exception": false,
     "start_time": "2021-08-01T07:45:32.944961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id    target\n",
      "0  c0f722661 -0.534043\n",
      "1  f0953f0a5 -0.554323\n",
      "2  0df072751 -0.468059\n",
      "3  04caf4e0c -2.560002\n",
      "4  0e63f8bea -1.818977\n",
      "5  12537fe78 -1.439899\n",
      "6  965e592c0  0.249321\n"
     ]
    }
   ],
   "source": [
    "predictions = all_predictions.mean(axis=0)\n",
    "submission_df.target = predictions\n",
    "print(submission_df)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-recipient",
   "metadata": {
    "papermill": {
     "duration": 0.258447,
     "end_time": "2021-08-01T07:45:34.187010",
     "exception": false,
     "start_time": "2021-08-01T07:45:33.928563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-principle",
   "metadata": {
    "papermill": {
     "duration": 0.258943,
     "end_time": "2021-08-01T07:45:34.709710",
     "exception": false,
     "start_time": "2021-08-01T07:45:34.450767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-message",
   "metadata": {
    "papermill": {
     "duration": 0.258407,
     "end_time": "2021-08-01T07:45:35.233295",
     "exception": false,
     "start_time": "2021-08-01T07:45:34.974888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3645.98386,
   "end_time": "2021-08-01T07:45:37.914968",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-01T06:44:51.931108",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
