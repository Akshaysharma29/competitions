{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "looking-wyoming",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T04:09:54.233745Z",
     "iopub.status.busy": "2021-03-31T04:09:54.232376Z",
     "iopub.status.idle": "2021-03-31T04:09:54.237619Z",
     "shell.execute_reply": "2021-03-31T04:09:54.236966Z"
    },
    "papermill": {
     "duration": 0.020113,
     "end_time": "2021-03-31T04:09:54.237788",
     "exception": false,
     "start_time": "2021-03-31T04:09:54.217675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resized with padding added\n",
    "VERSION = 'V9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "engaging-estimate",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-31T04:09:54.266422Z",
     "iopub.status.busy": "2021-03-31T04:09:54.265632Z",
     "iopub.status.idle": "2021-03-31T04:10:17.608619Z",
     "shell.execute_reply": "2021-03-31T04:10:17.607891Z"
    },
    "papermill": {
     "duration": 23.361696,
     "end_time": "2021-03-31T04:10:17.608764",
     "exception": false,
     "start_time": "2021-03-31T04:09:54.247068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.7/site-packages (0.12.1)\r\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow_addons) (2.11.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q efficientnet\n",
    "!pip install tensorflow_addons\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm.notebook import tqdm\n",
    "from kaggle_datasets import KaggleDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "genuine-player",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T04:10:17.701026Z",
     "iopub.status.busy": "2021-03-31T04:10:17.647269Z",
     "iopub.status.idle": "2021-03-31T04:10:23.376099Z",
     "shell.execute_reply": "2021-03-31T04:10:23.375440Z"
    },
    "papermill": {
     "duration": 5.757743,
     "end_time": "2021-03-31T04:10:23.376240",
     "exception": false,
     "start_time": "2021-03-31T04:10:17.618497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "maritime-machinery",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T04:10:23.415070Z",
     "iopub.status.busy": "2021-03-31T04:10:23.414141Z",
     "iopub.status.idle": "2021-03-31T04:10:23.936804Z",
     "shell.execute_reply": "2021-03-31T04:10:23.936116Z"
    },
    "papermill": {
     "duration": 0.550666,
     "end_time": "2021-03-31T04:10:23.936988",
     "exception": false,
     "start_time": "2021-03-31T04:10:23.386322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('shopee-tf-records-512-stratified')\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "IMAGE_SIZE = [512, 512]\n",
    "# Seed\n",
    "SEED = 42\n",
    "# Learning rate\n",
    "LR = 0.001\n",
    "# Verbosity\n",
    "VERBOSE = 2\n",
    "# Number of classes\n",
    "N_CLASSES = 11014#11011\n",
    "# Number of folds\n",
    "FOLDS = 5\n",
    "\n",
    "# Training filenames directory\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/*.tfrec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "roman-question",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T04:10:23.978315Z",
     "iopub.status.busy": "2021-03-31T04:10:23.976229Z",
     "iopub.status.idle": "2021-03-31T04:10:23.990490Z",
     "shell.execute_reply": "2021-03-31T04:10:23.991192Z"
    },
    "papermill": {
     "duration": 0.044046,
     "end_time": "2021-03-31T04:10:23.991373",
     "exception": false,
     "start_time": "2021-03-31T04:10:23.947327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 34250 training images\n"
     ]
    }
   ],
   "source": [
    "# Function to get our f1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1\n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "def arcface_format(posting_id, image, label_group, matches):\n",
    "    return posting_id, {'inp1': image, 'inp2': label_group}, label_group, matches\n",
    "\n",
    "# Data augmentation function\n",
    "def data_augment(posting_id, image, label_group, matches):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
    "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
    "    image = tf.image.random_brightness(image, 0.10)\n",
    "    return posting_id, image, label_group, matches\n",
    "\n",
    "# Function to decode our images\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
    "    image = tf.image.resize_with_pad(image, target_width = IMAGE_SIZE[0], target_height = IMAGE_SIZE[1])\n",
    "#     image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# This function parse our images and also get the target variable\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"posting_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label_group\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"matches\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    posting_id = example['posting_id']\n",
    "    image = decode_image(example['image'])\n",
    "#     label_group = tf.one_hot(tf.cast(example['label_group'], tf.int32), depth = N_CLASSES)\n",
    "    label_group = tf.cast(example['label_group'], tf.int32)\n",
    "    matches = example['matches']\n",
    "    return posting_id, image, label_group, matches\n",
    "\n",
    "# This function loads TF Records and parse them into tensors\n",
    "def load_dataset(filenames, ordered = False):\n",
    "    \n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False \n",
    "        \n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = AUTO) \n",
    "    return dataset\n",
    "\n",
    "# This function is to get our training tensors\n",
    "def get_training_dataset(filenames, ordered = False):\n",
    "    dataset = load_dataset(filenames, ordered = ordered)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "# This function is to get our validation tensors\n",
    "def get_validation_dataset(filenames, ordered = True):\n",
    "    dataset = load_dataset(filenames, ordered = ordered)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset\n",
    "\n",
    "# Function to count how many photos we have in\n",
    "def count_data_items(filenames):\n",
    "    # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "print(f'Dataset: {NUM_TRAINING_IMAGES} training images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "established-compression",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-31T04:10:24.049482Z",
     "iopub.status.busy": "2021-03-31T04:10:24.030147Z",
     "iopub.status.idle": "2021-03-31T04:39:00.559436Z",
     "shell.execute_reply": "2021-03-31T04:39:00.560125Z"
    },
    "papermill": {
     "duration": 1716.556235,
     "end_time": "2021-03-31T04:39:00.560327",
     "exception": false,
     "start_time": "2021-03-31T04:10:24.004092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "44113920/44107200 [==============================] - 1s 0us/step\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "98/98 - 166s - loss: 23.9664 - sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: loss improved from inf to 23.96642, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00025680000000000006.\n",
      "98/98 - 77s - loss: 22.1477 - sparse_categorical_accuracy: 3.9860e-05\n",
      "\n",
      "Epoch 00002: loss improved from 23.96642 to 22.14766, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0005126000000000001.\n",
      "98/98 - 77s - loss: 17.3668 - sparse_categorical_accuracy: 0.0150\n",
      "\n",
      "Epoch 00003: loss improved from 22.14766 to 17.36683, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0007684000000000001.\n",
      "98/98 - 78s - loss: 12.2298 - sparse_categorical_accuracy: 0.0902\n",
      "\n",
      "Epoch 00004: loss improved from 17.36683 to 12.22981, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010242.\n",
      "98/98 - 77s - loss: 8.1635 - sparse_categorical_accuracy: 0.2166\n",
      "\n",
      "Epoch 00005: loss improved from 12.22981 to 8.16352, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00128.\n",
      "98/98 - 77s - loss: 5.5469 - sparse_categorical_accuracy: 0.3520\n",
      "\n",
      "Epoch 00006: loss improved from 8.16352 to 5.54690, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010242.\n",
      "98/98 - 80s - loss: 3.4243 - sparse_categorical_accuracy: 0.5620\n",
      "\n",
      "Epoch 00007: loss improved from 5.54690 to 3.42427, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0008195600000000003.\n",
      "98/98 - 79s - loss: 2.1437 - sparse_categorical_accuracy: 0.7197\n",
      "\n",
      "Epoch 00008: loss improved from 3.42427 to 2.14366, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0006558480000000003.\n",
      "98/98 - 76s - loss: 1.4875 - sparse_categorical_accuracy: 0.8071\n",
      "\n",
      "Epoch 00009: loss improved from 2.14366 to 1.48752, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0005248784000000002.\n",
      "98/98 - 78s - loss: 1.0978 - sparse_categorical_accuracy: 0.8652\n",
      "\n",
      "Epoch 00010: loss improved from 1.48752 to 1.09777, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0004201027200000002.\n",
      "98/98 - 77s - loss: 0.8433 - sparse_categorical_accuracy: 0.9044\n",
      "\n",
      "Epoch 00011: loss improved from 1.09777 to 0.84334, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0003362821760000002.\n",
      "98/98 - 78s - loss: 0.6868 - sparse_categorical_accuracy: 0.9245\n",
      "\n",
      "Epoch 00012: loss improved from 0.84334 to 0.68681, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026922574080000017.\n",
      "98/98 - 78s - loss: 0.5734 - sparse_categorical_accuracy: 0.9445\n",
      "\n",
      "Epoch 00013: loss improved from 0.68681 to 0.57341, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00021558059264000014.\n",
      "98/98 - 77s - loss: 0.5035 - sparse_categorical_accuracy: 0.9539\n",
      "\n",
      "Epoch 00014: loss improved from 0.57341 to 0.50351, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001726644741120001.\n",
      "98/98 - 78s - loss: 0.4499 - sparse_categorical_accuracy: 0.9597\n",
      "\n",
      "Epoch 00015: loss improved from 0.50351 to 0.44986, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001383315792896001.\n",
      "98/98 - 78s - loss: 0.4061 - sparse_categorical_accuracy: 0.9646\n",
      "\n",
      "Epoch 00016: loss improved from 0.44986 to 0.40611, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00011086526343168008.\n",
      "98/98 - 76s - loss: 0.3892 - sparse_categorical_accuracy: 0.9671\n",
      "\n",
      "Epoch 00017: loss improved from 0.40611 to 0.38920, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 8.889221074534406e-05.\n",
      "98/98 - 76s - loss: 0.3519 - sparse_categorical_accuracy: 0.9702\n",
      "\n",
      "Epoch 00018: loss improved from 0.38920 to 0.35193, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 7.131376859627525e-05.\n",
      "98/98 - 79s - loss: 0.3364 - sparse_categorical_accuracy: 0.9718\n",
      "\n",
      "Epoch 00019: loss improved from 0.35193 to 0.33644, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 5.725101487702021e-05.\n",
      "98/98 - 77s - loss: 0.3243 - sparse_categorical_accuracy: 0.9723\n",
      "\n",
      "Epoch 00020: loss improved from 0.33644 to 0.32429, saving model to EfficientNetB3_512_42_V9.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp1 (InputLayer)               [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "efficientnet-b3 (Functional)    (None, None, None, 1 10783528    inp1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1536)         0           efficientnet-b3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "inp2 (InputLayer)               [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "head/arc_margin (ArcMarginProdu (None, 11014)        16917504    global_average_pooling2d[0][0]   \n",
      "                                                                 inp2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 11014)        0           head/arc_margin[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 27,701,032\n",
      "Trainable params: 27,613,736\n",
      "Non-trainable params: 87,296\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Function for a custom learning rate scheduler with warmup and decay\n",
    "def get_lr_callback():\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.000005 * BATCH_SIZE\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback\n",
    "\n",
    "# Arcmarginproduct class keras layer\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output\n",
    "\n",
    "\n",
    "# Function to create our EfficientNetB3 model\n",
    "def get_model():\n",
    "\n",
    "    with strategy.scope():\n",
    "\n",
    "        margin = ArcMarginProduct(\n",
    "            n_classes = N_CLASSES, \n",
    "            s = 30, \n",
    "            m = 0.5, \n",
    "            name='head/arc_margin', \n",
    "            dtype='float32'\n",
    "            )\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
    "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "        x = efn.EfficientNetB3(weights = 'imagenet', include_top = False)(inp)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = margin([x, label])\n",
    "        \n",
    "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer = opt,\n",
    "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "            ) \n",
    "        \n",
    "        return model\n",
    "\n",
    "def train_and_evaluate():\n",
    "\n",
    "    # Seed everything\n",
    "    seed_everything(SEED)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    train, valid = train_test_split(TRAINING_FILENAMES, shuffle = True, random_state = SEED)\n",
    "    train_dataset = get_training_dataset(train, ordered = False)\n",
    "    train_dataset = train_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
    "#     val_dataset = get_validation_dataset(valid, ordered = True)\n",
    "#     val_dataset = val_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
    "    STEPS_PER_EPOCH = count_data_items(train) // BATCH_SIZE\n",
    "    K.clear_session()\n",
    "    model = get_model()\n",
    "    # Model checkpoint\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'EfficientNetB3_{IMAGE_SIZE[0]}_{SEED}_{VERSION}.h5', \n",
    "#                                                     monitor = 'val_loss', \n",
    "                                                    monitor = 'loss', \n",
    "                                                    verbose = VERBOSE, \n",
    "                                                    save_best_only = True,\n",
    "                                                    save_weights_only = True, \n",
    "                                                    mode = 'min')\n",
    "\n",
    "    history = model.fit(train_dataset,\n",
    "                        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                        epochs = EPOCHS,\n",
    "                        callbacks = [checkpoint, get_lr_callback()], \n",
    "#                         validation_data = val_dataset,\n",
    "                        verbose = VERBOSE)\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    \n",
    "train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-liechtenstein",
   "metadata": {
    "papermill": {
     "duration": 0.03372,
     "end_time": "2021-03-31T04:39:00.628283",
     "exception": false,
     "start_time": "2021-03-31T04:39:00.594563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-nancy",
   "metadata": {
    "papermill": {
     "duration": 0.033957,
     "end_time": "2021-03-31T04:39:00.696205",
     "exception": false,
     "start_time": "2021-03-31T04:39:00.662248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-makeup",
   "metadata": {
    "papermill": {
     "duration": 0.034222,
     "end_time": "2021-03-31T04:39:00.764660",
     "exception": false,
     "start_time": "2021-03-31T04:39:00.730438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-stockholm",
   "metadata": {
    "papermill": {
     "duration": 0.033909,
     "end_time": "2021-03-31T04:39:00.832790",
     "exception": false,
     "start_time": "2021-03-31T04:39:00.798881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1756.942299,
   "end_time": "2021-03-31T04:39:04.690342",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-31T04:09:47.748043",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
